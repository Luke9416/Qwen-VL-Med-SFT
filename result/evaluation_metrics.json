{
  "metadata": {
    "base_model": "output_7B/stage1_Qwen7BInst_stage2",
    "inference_time": "2025-07-11T02:33:33.633812",
    "total_samples": 1061,
    "calculate_perplexity": true,
    "use_custom_prompt": true,
    "system_prompt": "You are a professional medical assistant with extensive medical knowledge and clinical experience.\nYour tasks include:\n1. Accurately answering medical-related questions\n2. Providing evidence-based medical advice\n3. Carefully analyzing symptoms and signs for diagnostic questions\n4. Always prioritizing patient safety\n5. If questions exceed your expertise, clearly state this and suggest consulting relevant specialists\n\nPlease answer questions using professional and accurate language, and make your answers as brief, clear, and precise as possible.",
    "language": "en",
    "evaluation_time": "2025-07-11T06:55:25.134040",
    "bertscore_available": true
  },
  "metrics": {
    "overall": {
      "eval_exact_match": 0.472196041470311,
      "eval_soft_match": 0.5791265611687512,
      "eval_word_overlap": 0.4953793994871491,
      "eval_bleu_4": 0.08990330084113371,
      "eval_rouge_l": 0.5124834834932342,
      "eval_char_precision": 0.6369889562907459,
      "eval_char_recall": 0.638298418522142,
      "eval_char_f1": 0.6246925464788426,
      "eval_word_precision": 0.5094527193540282,
      "eval_word_recall": 0.5154369193483236,
      "eval_word_f1": 0.5051688931997766,
      "eval_bertscore_f1": 0.0,
      "eval_sample_count": 1061
    },
    "by_type": {
      "open-ended": {
        "eval_exact_match": 0.3328611898016997,
        "eval_soft_match": 0.49344185278571034,
        "eval_word_overlap": 0.36770190206213205,
        "eval_bleu_4": 0.06810930578669619,
        "eval_rouge_l": 0.3934064815670276,
        "eval_char_precision": 0.581164028843643,
        "eval_char_recall": 0.5824853003569301,
        "eval_char_f1": 0.5626328495949745,
        "eval_word_precision": 0.38885174962411323,
        "eval_word_recall": 0.3978450020234722,
        "eval_word_f1": 0.38241387490787954,
        "eval_bertscore_f1": 0.0,
        "eval_sample_count": 706
      },
      "closed-set": {
        "eval_exact_match": 0.7492957746478873,
        "eval_soft_match": 0.7495305164319248,
        "eval_word_overlap": 0.7492957746478873,
        "eval_bleu_4": 0.1332457248085503,
        "eval_rouge_l": 0.7492957746478873,
        "eval_char_precision": 0.7480097979179424,
        "eval_char_recall": 0.7492957746478873,
        "eval_char_f1": 0.748112676056338,
        "eval_word_precision": 0.7492957746478873,
        "eval_word_recall": 0.7492957746478873,
        "eval_word_f1": 0.7492957746478873,
        "eval_bertscore_f1": 0.0,
        "eval_sample_count": 355
      }
    },
    "sample_counts": {
      "total": 1061,
      "open-ended_count": 706,
      "closed-set_count": 355
    },
    "perplexity": {
      "mean_perplexity": 173.8327491753063,
      "std_perplexity": 1582.0417371459498,
      "median_perplexity": 2.890625,
      "min_perplexity": 1.015625,
      "max_perplexity": 32000.0,
      "num_valid_samples": 1061,
      "valid_ratio": 1.0,
      "mean_nll": 1.7871517141847313,
      "std_nll": 1.83768941313941,
      "median_nll": 1.0625
    }
  },
  "detailed_metrics": [
    {
      "item_id": "test_0",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.46875,
      "nll": 0.3828125
    },
    {
      "item_id": "test_1",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.53125,
      "nll": 0.92578125
    },
    {
      "item_id": "test_2",
      "prediction": "Lung",
      "reference": "Lung, Spinal Cord",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.38095238095238093,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.008853531856477262,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.29411764705882354,
        "char_f1": 0.45454545454545453,
        "word_precision": 1.0,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.5
      },
      "perplexity": 18.875,
      "nll": 2.9375
    },
    {
      "item_id": "test_3",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0,
      "nll": 0.6953125
    },
    {
      "item_id": "test_4",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1015625,
      "nll": 0.10009765625
    },
    {
      "item_id": "test_5",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.1298828125
    },
    {
      "item_id": "test_6",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.03125,
      "nll": 0.7109375
    },
    {
      "item_id": "test_7",
      "prediction": "Pneumonia",
      "reference": "Lung Cancer",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.45454545454545453,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.96875,
      "nll": 1.7890625
    },
    {
      "item_id": "test_8",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.265625,
      "nll": 0.81640625
    },
    {
      "item_id": "test_9",
      "prediction": "Right lung",
      "reference": "Left Lung, Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.6875,
        "char_f1": 0.7795275590551182,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 11.625,
      "nll": 2.453125
    },
    {
      "item_id": "test_10",
      "prediction": "Heart",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.796875,
      "nll": 0.5859375
    },
    {
      "item_id": "test_11",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3203125,
      "nll": 0.275390625
    },
    {
      "item_id": "test_12",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.484375,
      "nll": 0.91015625
    },
    {
      "item_id": "test_13",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.546875,
      "nll": 0.93359375
    },
    {
      "item_id": "test_14",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.015625,
      "nll": 0.69921875
    },
    {
      "item_id": "test_15",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.07275390625
    },
    {
      "item_id": "test_16",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.53125,
      "nll": 1.7109375
    },
    {
      "item_id": "test_17",
      "prediction": "Pneumonia",
      "reference": "Lung Cancer",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.45454545454545453,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.875,
      "nll": 2.0625
    },
    {
      "item_id": "test_18",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.96875,
      "nll": 1.0859375
    },
    {
      "item_id": "test_19",
      "prediction": "Right lung",
      "reference": "Left Lung, Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.6875,
        "char_f1": 0.7795275590551182,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 17.75,
      "nll": 2.875
    },
    {
      "item_id": "test_20",
      "prediction": "Heart",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.65625,
      "nll": 1.296875
    },
    {
      "item_id": "test_21",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.671875,
      "nll": 0.515625
    },
    {
      "item_id": "test_22",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.203125,
      "nll": 0.79296875
    },
    {
      "item_id": "test_23",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7421875,
      "nll": 0.5546875
    },
    {
      "item_id": "test_24",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7265625,
      "nll": 0.546875
    },
    {
      "item_id": "test_25",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.08984375
    },
    {
      "item_id": "test_26",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4453125,
      "nll": 0.3671875
    },
    {
      "item_id": "test_27",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.5,
      "nll": 0.91796875
    },
    {
      "item_id": "test_28",
      "prediction": "Pneumonia",
      "reference": "Lung Cancer",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.45454545454545453,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.0625,
      "nll": 1.8046875
    },
    {
      "item_id": "test_29",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.15625,
      "nll": 0.76953125
    },
    {
      "item_id": "test_30",
      "prediction": "Right lung",
      "reference": "Right Lung, Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7692307692307693,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.11633369384516798,
        "rouge_l": 0.8,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.6875,
        "char_f1": 0.7795275590551182,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 21.375,
      "nll": 3.0625
    },
    {
      "item_id": "test_31",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.25,
      "nll": 1.4453125
    },
    {
      "item_id": "test_32",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.8359375,
      "nll": 0.609375
    },
    {
      "item_id": "test_33",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.328125,
      "nll": 0.84375
    },
    {
      "item_id": "test_34",
      "prediction": "Pneumonia",
      "reference": "Cardiomegaly",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09523809523809523,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.5,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.28125,
      "nll": 0.82421875
    },
    {
      "item_id": "test_35",
      "prediction": "Right lung",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.125,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 604.0,
      "nll": 6.40625
    },
    {
      "item_id": "test_36",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.125,
      "nll": 1.140625
    },
    {
      "item_id": "test_37",
      "prediction": "heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 6.21875,
      "nll": 1.828125
    },
    {
      "item_id": "test_38",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8046875,
      "nll": 0.58984375
    },
    {
      "item_id": "test_39",
      "prediction": "Liver",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.16666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.296875,
      "nll": 0.2578125
    },
    {
      "item_id": "test_40",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.40625,
      "nll": 0.33984375
    },
    {
      "item_id": "test_41",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.921875,
      "nll": 1.3671875
    },
    {
      "item_id": "test_42",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.46875,
      "nll": 1.8671875
    },
    {
      "item_id": "test_43",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.04150390625
    },
    {
      "item_id": "test_44",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.171875,
      "nll": 0.7734375
    },
    {
      "item_id": "test_45",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.34375,
      "nll": 0.294921875
    },
    {
      "item_id": "test_46",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 14.4375,
      "nll": 2.671875
    },
    {
      "item_id": "test_47",
      "prediction": "Chest",
      "reference": "Chest",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8984375,
      "nll": 0.640625
    },
    {
      "item_id": "test_48",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2176.0,
      "nll": 7.6875
    },
    {
      "item_id": "test_49",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.0625,
      "nll": 2.203125
    },
    {
      "item_id": "test_50",
      "prediction": "Pneumonia",
      "reference": "Pneumonia",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.28125,
      "nll": 1.1875
    },
    {
      "item_id": "test_51",
      "prediction": "Left upper lobe",
      "reference": "Left Lung, Lower Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5405405405405406,
        "word_overlap": 0.16666666666666666,
        "bleu_4": 0.05833544737207805,
        "rouge_l": 0.28571428571428575,
        "bertscore_f1": 0.0,
        "char_precision": 0.7333333333333333,
        "char_recall": 0.6363636363636364,
        "char_f1": 0.6814159292035398,
        "word_precision": 0.3333333333333333,
        "word_recall": 0.25,
        "word_f1": 0.28571428571428575
      },
      "perplexity": 153.0,
      "nll": 5.03125
    },
    {
      "item_id": "test_52",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 392.0,
      "nll": 5.96875
    },
    {
      "item_id": "test_53",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.9375,
      "nll": 1.59375
    },
    {
      "item_id": "test_54",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.53125,
      "nll": 0.42578125
    },
    {
      "item_id": "test_55",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.109375,
      "nll": 0.74609375
    },
    {
      "item_id": "test_56",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.265625,
      "nll": 0.81640625
    },
    {
      "item_id": "test_57",
      "prediction": "Pneumonia",
      "reference": "Cardiomegaly",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09523809523809523,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.5,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.140625,
      "nll": 0.76171875
    },
    {
      "item_id": "test_58",
      "prediction": "Right lung",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.125,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1128.0,
      "nll": 7.03125
    },
    {
      "item_id": "test_59",
      "prediction": "heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.921875,
      "nll": 1.3671875
    },
    {
      "item_id": "test_60",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.78125,
      "nll": 1.5625
    },
    {
      "item_id": "test_61",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.7109375,
      "nll": 0.5390625
    },
    {
      "item_id": "test_62",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.765625,
      "nll": 1.015625
    },
    {
      "item_id": "test_63",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.09375,
      "nll": 0.7421875
    },
    {
      "item_id": "test_64",
      "prediction": "Pneumonia",
      "reference": "Cardiomegaly",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09523809523809523,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.5,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.78125,
      "nll": 1.0234375
    },
    {
      "item_id": "test_65",
      "prediction": "Right lung",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.125,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2624.0,
      "nll": 7.875
    },
    {
      "item_id": "test_66",
      "prediction": "Lung",
      "reference": "Heart",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.125,
      "nll": 2.09375
    },
    {
      "item_id": "test_67",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.5,
      "nll": 1.703125
    },
    {
      "item_id": "test_68",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.6640625,
      "nll": 0.5078125
    },
    {
      "item_id": "test_69",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_70",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.3125,
      "nll": 0.83984375
    },
    {
      "item_id": "test_71",
      "prediction": "Pneumonia",
      "reference": "Atelectasis, Mass",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.07692307692307693,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.29411764705882354,
        "char_f1": 0.3125,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.5625,
      "nll": 2.359375
    },
    {
      "item_id": "test_72",
      "prediction": "Right upper lobe",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5384615384615384,
        "word_overlap": 0.25,
        "bleu_4": 0.11362193664674995,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.8,
        "char_f1": 0.6153846153846154,
        "word_precision": 0.3333333333333333,
        "word_recall": 0.5,
        "word_f1": 0.4
      },
      "perplexity": 25.0,
      "nll": 3.21875
    },
    {
      "item_id": "test_73",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 6.53125,
      "nll": 1.875
    },
    {
      "item_id": "test_74",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.75,
      "nll": 1.3203125
    },
    {
      "item_id": "test_75",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.71875,
      "nll": 1.5546875
    },
    {
      "item_id": "test_76",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.7265625,
      "nll": 0.546875
    },
    {
      "item_id": "test_77",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.46875,
      "nll": 0.90234375
    },
    {
      "item_id": "test_78",
      "prediction": "Liver",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.9375,
      "nll": 1.078125
    },
    {
      "item_id": "test_79",
      "prediction": "Pneumonia",
      "reference": "Cardiomegaly",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09523809523809523,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.5,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.359375,
      "nll": 0.859375
    },
    {
      "item_id": "test_80",
      "prediction": "Right lung",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.125,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1408.0,
      "nll": 7.25
    },
    {
      "item_id": "test_81",
      "prediction": "heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.03125,
      "nll": 1.390625
    },
    {
      "item_id": "test_82",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.0,
      "nll": 1.609375
    },
    {
      "item_id": "test_83",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.625,
      "nll": 0.484375
    },
    {
      "item_id": "test_84",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.40625,
      "nll": 0.87890625
    },
    {
      "item_id": "test_85",
      "prediction": "lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.75,
        "char_f1": 0.75,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.203125,
      "nll": 0.7890625
    },
    {
      "item_id": "test_86",
      "prediction": "Pneumonia",
      "reference": "Cardiomegaly",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09523809523809523,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.5,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.5,
      "nll": 0.91796875
    },
    {
      "item_id": "test_87",
      "prediction": "Right lung",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.125,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3824.0,
      "nll": 8.25
    },
    {
      "item_id": "test_88",
      "prediction": "heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.25,
      "nll": 1.4453125
    },
    {
      "item_id": "test_89",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.125,
      "nll": 1.6328125
    },
    {
      "item_id": "test_90",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.7578125,
      "nll": 0.5625
    },
    {
      "item_id": "test_91",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4921875,
      "nll": 0.3984375
    },
    {
      "item_id": "test_92",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.375,
      "nll": 0.8671875
    },
    {
      "item_id": "test_93",
      "prediction": "Pneumonia",
      "reference": "Nodule",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.5,
        "char_f1": 0.4,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 66.0,
      "nll": 4.1875
    },
    {
      "item_id": "test_94",
      "prediction": "Right lung",
      "reference": "Left Lung, Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.6875,
        "char_f1": 0.7795275590551182,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 23.125,
      "nll": 3.140625
    },
    {
      "item_id": "test_95",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.984375,
      "nll": 1.09375
    },
    {
      "item_id": "test_96",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5625,
      "nll": 0.447265625
    },
    {
      "item_id": "test_97",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 18.25,
      "nll": 2.90625
    },
    {
      "item_id": "test_98",
      "prediction": "Chest",
      "reference": "Chest",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8359375,
      "nll": 0.60546875
    },
    {
      "item_id": "test_99",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 6.71875,
      "nll": 1.90625
    },
    {
      "item_id": "test_100",
      "prediction": "Pneumonia",
      "reference": "Nodule",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.5,
        "char_f1": 0.4,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 102.0,
      "nll": 4.625
    },
    {
      "item_id": "test_101",
      "prediction": "Left upper lobe",
      "reference": "Right Lung, Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25806451612903225,
        "word_overlap": 0.2,
        "bleu_4": 0.0814136751754278,
        "rouge_l": 0.3333333333333333,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.5625,
        "char_f1": 0.5806451612903225,
        "word_precision": 0.3333333333333333,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.3333333333333333
      },
      "perplexity": 115.5,
      "nll": 4.75
    },
    {
      "item_id": "test_102",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 158.0,
      "nll": 5.0625
    },
    {
      "item_id": "test_103",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 776.0,
      "nll": 6.65625
    },
    {
      "item_id": "test_104",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 10.75,
      "nll": 2.375
    },
    {
      "item_id": "test_105",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 10.9375,
      "nll": 2.390625
    },
    {
      "item_id": "test_106",
      "prediction": "Chest",
      "reference": "Chest",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.546875,
      "nll": 0.435546875
    },
    {
      "item_id": "test_107",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 25.75,
      "nll": 3.25
    },
    {
      "item_id": "test_108",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.375,
      "nll": 1.4765625
    },
    {
      "item_id": "test_109",
      "prediction": "Answer: Pneumonia",
      "reference": "Atelectasis",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2857142857142857,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.35294117647058826,
        "char_recall": 0.6363636363636364,
        "char_f1": 0.4540540540540541,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.3125,
      "nll": 1.671875
    },
    {
      "item_id": "test_110",
      "prediction": "Right upper lobe",
      "reference": "Right Lung, Lower Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.631578947368421,
        "word_overlap": 0.16666666666666666,
        "bleu_4": 0.05833544737207805,
        "rouge_l": 0.28571428571428575,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.6818181818181818,
        "char_f1": 0.7142857142857143,
        "word_precision": 0.3333333333333333,
        "word_recall": 0.25,
        "word_f1": 0.28571428571428575
      },
      "perplexity": 72.5,
      "nll": 4.28125
    },
    {
      "item_id": "test_111",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 179.0,
      "nll": 5.1875
    },
    {
      "item_id": "test_112",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.625,
      "nll": 1.7265625
    },
    {
      "item_id": "test_113",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.59375,
      "nll": 1.5234375
    },
    {
      "item_id": "test_114",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.5625,
      "nll": 0.447265625
    },
    {
      "item_id": "test_115",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.484375,
      "nll": 0.91015625
    },
    {
      "item_id": "test_116",
      "prediction": "Pneumonia",
      "reference": "Pneumonia",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.5,
      "nll": 0.9140625
    },
    {
      "item_id": "test_117",
      "prediction": "Right lung",
      "reference": "Left Lung, Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.6875,
        "char_f1": 0.7795275590551182,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 28.75,
      "nll": 3.359375
    },
    {
      "item_id": "test_118",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 6.53125,
      "nll": 1.875
    },
    {
      "item_id": "test_119",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 7.75,
      "nll": 2.046875
    },
    {
      "item_id": "test_120",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.609375,
      "nll": 0.95703125
    },
    {
      "item_id": "test_121",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9453125,
      "nll": 0.6640625
    },
    {
      "item_id": "test_122",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.1044921875
    },
    {
      "item_id": "test_123",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4609375,
      "nll": 0.37890625
    },
    {
      "item_id": "test_124",
      "prediction": "2",
      "reference": "1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.9375,
      "nll": 2.390625
    },
    {
      "item_id": "test_125",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8046875,
      "nll": 0.58984375
    },
    {
      "item_id": "test_126",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.046875,
      "nll": 0.04638671875
    },
    {
      "item_id": "test_127",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.06298828125
    },
    {
      "item_id": "test_128",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.12451171875
    },
    {
      "item_id": "test_129",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.453125,
      "nll": 0.89453125
    },
    {
      "item_id": "test_130",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.1875,
      "nll": 0.78125
    },
    {
      "item_id": "test_131",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6796875,
      "nll": 0.51953125
    },
    {
      "item_id": "test_132",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.46875,
      "nll": 1.5
    },
    {
      "item_id": "test_133",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.75,
      "nll": 0.55859375
    },
    {
      "item_id": "test_134",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.265625,
      "nll": 0.236328125
    },
    {
      "item_id": "test_135",
      "prediction": "lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.75,
        "char_f1": 0.75,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9765625,
      "nll": 0.6796875
    },
    {
      "item_id": "test_136",
      "prediction": "Pneumothorax",
      "reference": "Pneumothorax",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3984375,
      "nll": 0.337890625
    },
    {
      "item_id": "test_137",
      "prediction": "Right lung",
      "reference": "Right Lung, Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7692307692307693,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.11633369384516798,
        "rouge_l": 0.8,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.6875,
        "char_f1": 0.7795275590551182,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 25.0,
      "nll": 3.21875
    },
    {
      "item_id": "test_138",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.3125,
      "nll": 0.8359375
    },
    {
      "item_id": "test_139",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.25,
      "nll": 0.80859375
    },
    {
      "item_id": "test_140",
      "prediction": "Heart",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.078125,
      "nll": 0.73046875
    },
    {
      "item_id": "test_141",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.390625,
      "nll": 0.330078125
    },
    {
      "item_id": "test_142",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.07421875
    },
    {
      "item_id": "test_143",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9609375,
      "nll": 0.671875
    },
    {
      "item_id": "test_144",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1015625,
      "nll": 0.09423828125
    },
    {
      "item_id": "test_145",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.10595703125
    },
    {
      "item_id": "test_146",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.96875,
      "nll": 1.6015625
    },
    {
      "item_id": "test_147",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0546875,
      "nll": 0.0546875
    },
    {
      "item_id": "test_148",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.03271484375
    },
    {
      "item_id": "test_149",
      "prediction": "Right",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.859375,
      "nll": 1.3515625
    },
    {
      "item_id": "test_150",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.78125,
      "nll": 0.578125
    },
    {
      "item_id": "test_151",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.453125,
      "nll": 0.373046875
    },
    {
      "item_id": "test_152",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9609375,
      "nll": 0.671875
    },
    {
      "item_id": "test_153",
      "prediction": "Kidney",
      "reference": "Left Kidney",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7058823529411765,
        "word_overlap": 0.5,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.6363636363636364,
        "char_f1": 0.7777777777777778,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 11.0625,
      "nll": 2.40625
    },
    {
      "item_id": "test_154",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0,
      "nll": 0.69140625
    },
    {
      "item_id": "test_155",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 105.0,
      "nll": 4.65625
    },
    {
      "item_id": "test_156",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4375,
      "nll": 0.361328125
    },
    {
      "item_id": "test_157",
      "prediction": "Thorax",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.15384615384615383,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.484375,
      "nll": 1.25
    },
    {
      "item_id": "test_158",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.0625,
      "nll": 1.8046875
    },
    {
      "item_id": "test_159",
      "prediction": "2",
      "reference": "4",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.1875,
      "nll": 2.21875
    },
    {
      "item_id": "test_160",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0,
      "nll": 0.69140625
    },
    {
      "item_id": "test_161",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_162",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2578125,
      "nll": 0.232421875
    },
    {
      "item_id": "test_163",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3359375,
      "nll": 0.2890625
    },
    {
      "item_id": "test_164",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.4375,
      "nll": 1.234375
    },
    {
      "item_id": "test_165",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0859375,
      "nll": 0.080078125
    },
    {
      "item_id": "test_166",
      "prediction": "Not visible",
      "reference": "Top",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14285714285714285,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.09090909090909091,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.14285714285714288,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 119.5,
      "nll": 4.78125
    },
    {
      "item_id": "test_167",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.765625,
      "nll": 0.56640625
    },
    {
      "item_id": "test_168",
      "prediction": "heart",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.171875,
      "nll": 1.15625
    },
    {
      "item_id": "test_169",
      "prediction": "Liver",
      "reference": "Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2857142857142857,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.3636363636363636,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 54.5,
      "nll": 4.0
    },
    {
      "item_id": "test_170",
      "prediction": "Left lung",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.631578947368421,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.6666666666666666,
        "char_recall": 0.7,
        "char_f1": 0.6829268292682926,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 37.5,
      "nll": 3.625
    },
    {
      "item_id": "test_171",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 112.0,
      "nll": 4.71875
    },
    {
      "item_id": "test_172",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7109375,
      "nll": 0.5390625
    },
    {
      "item_id": "test_173",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.25,
      "nll": 0.220703125
    },
    {
      "item_id": "test_174",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2421875,
      "nll": 0.21875
    },
    {
      "item_id": "test_175",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1171875,
      "nll": 0.1123046875
    },
    {
      "item_id": "test_176",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7734375,
      "nll": 0.57421875
    },
    {
      "item_id": "test_177",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3515625,
      "nll": 0.30078125
    },
    {
      "item_id": "test_178",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 13.8125,
      "nll": 2.625
    },
    {
      "item_id": "test_179",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2109375,
      "nll": 0.1943359375
    },
    {
      "item_id": "test_180",
      "prediction": "Liver",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4444444444444444,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.5,
        "char_f1": 0.4444444444444445,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 74.5,
      "nll": 4.3125
    },
    {
      "item_id": "test_181",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 66.0,
      "nll": 4.1875
    },
    {
      "item_id": "test_182",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.703125,
      "nll": 0.53125
    },
    {
      "item_id": "test_183",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.34375,
      "nll": 0.296875
    },
    {
      "item_id": "test_184",
      "prediction": "Kidney",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.5,
        "char_f1": 0.4,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.0625,
      "nll": 0.7265625
    },
    {
      "item_id": "test_185",
      "prediction": "Transverse",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7692307692307693,
        "word_overlap": 0.5,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.8125,
        "char_f1": 0.896551724137931,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 191.0,
      "nll": 5.25
    },
    {
      "item_id": "test_186",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4765625,
      "nll": 0.390625
    },
    {
      "item_id": "test_187",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2109375,
      "nll": 0.1904296875
    },
    {
      "item_id": "test_188",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4140625,
      "nll": 0.34375
    },
    {
      "item_id": "test_189",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.036865234375
    },
    {
      "item_id": "test_190",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.125,
      "nll": 1.4140625
    },
    {
      "item_id": "test_191",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.119140625
    },
    {
      "item_id": "test_192",
      "prediction": "Liver cirrhosis",
      "reference": "Liver Cancer",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5925925925925926,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.7333333333333333,
        "char_recall": 0.75,
        "char_f1": 0.7415730337078651,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 43.75,
      "nll": 3.78125
    },
    {
      "item_id": "test_193",
      "prediction": "Right upper quadrant",
      "reference": "Left and top",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3125,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.55,
        "char_recall": 0.75,
        "char_f1": 0.6346153846153847,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 72.5,
      "nll": 4.28125
    },
    {
      "item_id": "test_194",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.046875,
      "nll": 0.71875
    },
    {
      "item_id": "test_195",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 90.0,
      "nll": 4.5
    },
    {
      "item_id": "test_196",
      "prediction": "Kidney",
      "reference": "Liver",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.36363636363636365,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.4,
        "char_f1": 0.3636363636363636,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2048.0,
      "nll": 7.625
    },
    {
      "item_id": "test_197",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.90625,
      "nll": 0.64453125
    },
    {
      "item_id": "test_198",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.171875,
      "nll": 0.158203125
    },
    {
      "item_id": "test_199",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.12353515625
    },
    {
      "item_id": "test_200",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.6171875,
      "nll": 0.48046875
    },
    {
      "item_id": "test_201",
      "prediction": "Liver",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.36363636363636365,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.34375,
      "nll": 0.8515625
    },
    {
      "item_id": "test_202",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8203125,
      "nll": 0.59765625
    },
    {
      "item_id": "test_203",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 144.0,
      "nll": 4.96875
    },
    {
      "item_id": "test_204",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5546875,
      "nll": 0.443359375
    },
    {
      "item_id": "test_205",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.126953125
    },
    {
      "item_id": "test_206",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2578125,
      "nll": 0.232421875
    },
    {
      "item_id": "test_207",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.02783203125
    },
    {
      "item_id": "test_208",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.15625,
      "nll": 1.1484375
    },
    {
      "item_id": "test_209",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.0908203125
    },
    {
      "item_id": "test_210",
      "prediction": "Liver",
      "reference": "Left and top",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23529411764705882,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.16666666666666666,
        "char_f1": 0.23529411764705882,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 112.0,
      "nll": 4.71875
    },
    {
      "item_id": "test_211",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.671875,
      "nll": 0.515625
    },
    {
      "item_id": "test_212",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.984375,
      "nll": 0.68359375
    },
    {
      "item_id": "test_213",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1796875,
      "nll": 0.1650390625
    },
    {
      "item_id": "test_214",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 16.875,
      "nll": 2.828125
    },
    {
      "item_id": "test_215",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1796875,
      "nll": 0.16796875
    },
    {
      "item_id": "test_216",
      "prediction": "Liver",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.36363636363636365,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.578125,
      "nll": 0.455078125
    },
    {
      "item_id": "test_217",
      "prediction": "Kidney",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.5,
        "char_f1": 0.4,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.859375,
      "nll": 0.62109375
    },
    {
      "item_id": "test_218",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.34375,
      "nll": 0.29296875
    },
    {
      "item_id": "test_219",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 158.0,
      "nll": 5.0625
    },
    {
      "item_id": "test_220",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9296875,
      "nll": 0.65625
    },
    {
      "item_id": "test_221",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4296875,
      "nll": 0.35546875
    },
    {
      "item_id": "test_222",
      "prediction": "T2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.8515625,
      "nll": 0.6171875
    },
    {
      "item_id": "test_223",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 139.0,
      "nll": 4.9375
    },
    {
      "item_id": "test_224",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.046875,
      "nll": 0.048828125
    },
    {
      "item_id": "test_225",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.0,
      "nll": 1.609375
    },
    {
      "item_id": "test_226",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.091796875
    },
    {
      "item_id": "test_227",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8671875,
      "nll": 0.625
    },
    {
      "item_id": "test_228",
      "prediction": "Thorax",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.15384615384615383,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.15625,
      "nll": 1.421875
    },
    {
      "item_id": "test_229",
      "prediction": "T2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.78125,
      "nll": 0.578125
    },
    {
      "item_id": "test_230",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2578125,
      "nll": 0.2294921875
    },
    {
      "item_id": "test_231",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3125,
      "nll": 0.271484375
    },
    {
      "item_id": "test_232",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.359375,
      "nll": 0.3046875
    },
    {
      "item_id": "test_233",
      "prediction": "The liver is located in the right upper quadrant of the abdomen.",
      "reference": "Not seen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.46875,
        "char_recall": 0.875,
        "char_f1": 0.6104651162790697,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 35.25,
      "nll": 3.5625
    },
    {
      "item_id": "test_234",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_235",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9375,
      "nll": 0.66015625
    },
    {
      "item_id": "test_236",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.40625,
      "nll": 0.33984375
    },
    {
      "item_id": "test_237",
      "prediction": "Thorax",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.15384615384615383,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.984375,
      "nll": 1.09375
    },
    {
      "item_id": "test_238",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3671875,
      "nll": 0.3125
    },
    {
      "item_id": "test_239",
      "prediction": "Liver",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.5,
      "nll": 0.9140625
    },
    {
      "item_id": "test_240",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0703125,
      "nll": 0.068359375
    },
    {
      "item_id": "test_241",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.421875,
      "nll": 0.353515625
    },
    {
      "item_id": "test_242",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.75,
      "nll": 1.3203125
    },
    {
      "item_id": "test_243",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.890625,
      "nll": 1.0625
    },
    {
      "item_id": "test_244",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.07861328125
    },
    {
      "item_id": "test_245",
      "prediction": "Pneumonia",
      "reference": "Liver Cancer",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.4166666666666667,
        "char_f1": 0.4761904761904762,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 139.0,
      "nll": 4.9375
    },
    {
      "item_id": "test_246",
      "prediction": "The liver is located in the upper right quadrant of the abdomen.",
      "reference": "Upper Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1891891891891892,
        "word_overlap": 0.09090909090909091,
        "bleu_4": 0.01553712569276035,
        "rouge_l": 0.14285714285714285,
        "bertscore_f1": 0.0,
        "char_precision": 0.46875,
        "char_recall": 0.8,
        "char_f1": 0.5911330049261083,
        "word_precision": 0.1,
        "word_recall": 0.5,
        "word_f1": 0.16666666666666669
      },
      "perplexity": 58.0,
      "nll": 4.0625
    },
    {
      "item_id": "test_247",
      "prediction": "Right lung",
      "reference": "Liver",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.1,
        "char_recall": 0.2,
        "char_f1": 0.13333333333333333,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 18.875,
      "nll": 2.9375
    },
    {
      "item_id": "test_248",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 18.25,
      "nll": 2.90625
    },
    {
      "item_id": "test_249",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2734375,
      "nll": 0.2412109375
    },
    {
      "item_id": "test_250",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5859375,
      "nll": 0.462890625
    },
    {
      "item_id": "test_251",
      "prediction": "Liver",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.2,
        "char_f1": 0.26666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 34.75,
      "nll": 3.546875
    },
    {
      "item_id": "test_252",
      "prediction": "Lung",
      "reference": "Heart and Liver",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.10526315789473684,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2105263157894737,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 230.0,
      "nll": 5.4375
    },
    {
      "item_id": "test_253",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 102.0,
      "nll": 4.625
    },
    {
      "item_id": "test_254",
      "prediction": "Lungs",
      "reference": "Pulmonary bronchus",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2608695652173913,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.2777777777777778,
        "char_f1": 0.37974683544303794,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 17.75,
      "nll": 2.875
    },
    {
      "item_id": "test_255",
      "prediction": "Right",
      "reference": "Symmetrical to the bone marrow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.17142857142857143,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.16666666666666666,
        "char_f1": 0.2608695652173913,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 378.0,
      "nll": 5.9375
    },
    {
      "item_id": "test_256",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.546875,
      "nll": 0.435546875
    },
    {
      "item_id": "test_257",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0859375,
      "nll": 0.0849609375
    },
    {
      "item_id": "test_258",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.515625,
      "nll": 0.416015625
    },
    {
      "item_id": "test_259",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.890625,
      "nll": 0.63671875
    },
    {
      "item_id": "test_260",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.06298828125
    },
    {
      "item_id": "test_261",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0859375,
      "nll": 0.08056640625
    },
    {
      "item_id": "test_262",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.12109375
    },
    {
      "item_id": "test_263",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0546875,
      "nll": 0.052734375
    },
    {
      "item_id": "test_264",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0703125,
      "nll": 0.06787109375
    },
    {
      "item_id": "test_265",
      "prediction": "Renal cell carcinoma",
      "reference": "Liver Cancer",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.375,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7,
        "char_recall": 0.75,
        "char_f1": 0.7241379310344827,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 72.5,
      "nll": 4.28125
    },
    {
      "item_id": "test_266",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.515625,
      "nll": 1.2578125
    },
    {
      "item_id": "test_267",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.890625,
      "nll": 0.63671875
    },
    {
      "item_id": "test_268",
      "prediction": "Kidney",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.5,
        "char_f1": 0.4,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.5234375,
      "nll": 0.419921875
    },
    {
      "item_id": "test_269",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.015625,
      "nll": 0.69921875
    },
    {
      "item_id": "test_270",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 158.0,
      "nll": 5.0625
    },
    {
      "item_id": "test_271",
      "prediction": "The kidney is normal in shape.",
      "reference": "Irregular",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.23333333333333334,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.34567901234567905,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 11.625,
      "nll": 2.453125
    },
    {
      "item_id": "test_272",
      "prediction": "Right",
      "reference": "Symmetrical to the bone marrow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.17142857142857143,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.16666666666666666,
        "char_f1": 0.2608695652173913,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 520.0,
      "nll": 6.25
    },
    {
      "item_id": "test_273",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.515625,
      "nll": 0.416015625
    },
    {
      "item_id": "test_274",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3046875,
      "nll": 0.267578125
    },
    {
      "item_id": "test_275",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.375,
      "nll": 2.515625
    },
    {
      "item_id": "test_276",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.359375,
      "nll": 0.3046875
    },
    {
      "item_id": "test_277",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.046875,
      "nll": 0.04296875
    },
    {
      "item_id": "test_278",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.671875,
      "nll": 0.98046875
    },
    {
      "item_id": "test_279",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_280",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.125,
      "nll": 1.8125
    },
    {
      "item_id": "test_281",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.8984375,
      "nll": 0.640625
    },
    {
      "item_id": "test_282",
      "prediction": "Right upper quadrant",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.15,
        "char_recall": 0.5,
        "char_f1": 0.23076923076923075,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.9375,
      "nll": 2.390625
    },
    {
      "item_id": "test_283",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3046875,
      "nll": 0.265625
    },
    {
      "item_id": "test_284",
      "prediction": "Liver",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.2,
        "char_f1": 0.26666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 115.5,
      "nll": 4.75
    },
    {
      "item_id": "test_285",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 131.0,
      "nll": 4.875
    },
    {
      "item_id": "test_286",
      "prediction": "Kidneys",
      "reference": "Pulmonary bronchus",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.24,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.42857142857142855,
        "char_recall": 0.2222222222222222,
        "char_f1": 0.29268292682926833,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 29.25,
      "nll": 3.375
    },
    {
      "item_id": "test_287",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3828125,
      "nll": 0.326171875
    },
    {
      "item_id": "test_288",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.057373046875
    },
    {
      "item_id": "test_289",
      "prediction": "Liver",
      "reference": "Small Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.09090909090909091,
        "char_f1": 0.12500000000000003,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.3125,
      "nll": 2.234375
    },
    {
      "item_id": "test_290",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8984375,
      "nll": 0.640625
    },
    {
      "item_id": "test_291",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.12255859375
    },
    {
      "item_id": "test_292",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.890625,
      "nll": 1.0625
    },
    {
      "item_id": "test_293",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.03369140625
    },
    {
      "item_id": "test_294",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.0390625
    },
    {
      "item_id": "test_295",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.609375,
      "nll": 0.4765625
    },
    {
      "item_id": "test_296",
      "prediction": "Liver",
      "reference": "Large Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.375,
      "nll": 2.515625
    },
    {
      "item_id": "test_297",
      "prediction": "Liver",
      "reference": "Large Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 16.625,
      "nll": 2.8125
    },
    {
      "item_id": "test_298",
      "prediction": "Liver",
      "reference": "Small Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.09090909090909091,
        "char_f1": 0.12500000000000003,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 30.125,
      "nll": 3.40625
    },
    {
      "item_id": "test_299",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 90.0,
      "nll": 4.5
    },
    {
      "item_id": "test_300",
      "prediction": "Right",
      "reference": "Symmetrical to the bone marrow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.17142857142857143,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.16666666666666666,
        "char_f1": 0.2608695652173913,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 444.0,
      "nll": 6.09375
    },
    {
      "item_id": "test_301",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.484375,
      "nll": 0.392578125
    },
    {
      "item_id": "test_302",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2421875,
      "nll": 0.2177734375
    },
    {
      "item_id": "test_303",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3046875,
      "nll": 0.265625
    },
    {
      "item_id": "test_304",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.0306396484375
    },
    {
      "item_id": "test_305",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.40625,
      "nll": 1.2265625
    },
    {
      "item_id": "test_306",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.11865234375
    },
    {
      "item_id": "test_307",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.53125,
      "nll": 1.7109375
    },
    {
      "item_id": "test_308",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.015625,
      "nll": 0.69921875
    },
    {
      "item_id": "test_309",
      "prediction": "Liver",
      "reference": "Large Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 21.0,
      "nll": 3.046875
    },
    {
      "item_id": "test_310",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.25,
      "nll": 0.2216796875
    },
    {
      "item_id": "test_311",
      "prediction": "Liver",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.2,
        "char_f1": 0.26666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 72.5,
      "nll": 4.28125
    },
    {
      "item_id": "test_312",
      "prediction": "Liver",
      "reference": "Stomach",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.09375,
      "nll": 1.40625
    },
    {
      "item_id": "test_313",
      "prediction": "Transverse",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7692307692307693,
        "word_overlap": 0.5,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.8125,
        "char_f1": 0.896551724137931,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 127.0,
      "nll": 4.84375
    },
    {
      "item_id": "test_314",
      "prediction": "Liver",
      "reference": "Large Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 39.25,
      "nll": 3.671875
    },
    {
      "item_id": "test_315",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5078125,
      "nll": 0.408203125
    },
    {
      "item_id": "test_316",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1015625,
      "nll": 0.09716796875
    },
    {
      "item_id": "test_317",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.359375,
      "nll": 0.3046875
    },
    {
      "item_id": "test_318",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.029296875
    },
    {
      "item_id": "test_319",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.53125,
      "nll": 1.7109375
    },
    {
      "item_id": "test_320",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.1064453125
    },
    {
      "item_id": "test_321",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 18.875,
      "nll": 2.9375
    },
    {
      "item_id": "test_322",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.10498046875
    },
    {
      "item_id": "test_323",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 185.0,
      "nll": 5.21875
    },
    {
      "item_id": "test_324",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5234375,
      "nll": 0.419921875
    },
    {
      "item_id": "test_325",
      "prediction": "Liver",
      "reference": "Large Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 21.75,
      "nll": 3.078125
    },
    {
      "item_id": "test_326",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3125,
      "nll": 0.26953125
    },
    {
      "item_id": "test_327",
      "prediction": "Liver",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.2,
        "char_f1": 0.26666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 260.0,
      "nll": 5.5625
    },
    {
      "item_id": "test_328",
      "prediction": "Axial",
      "reference": "Transverse  Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 131.0,
      "nll": 4.875
    },
    {
      "item_id": "test_329",
      "prediction": "Liver",
      "reference": "Large Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 38.0,
      "nll": 3.640625
    },
    {
      "item_id": "test_330",
      "prediction": "Chest X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.625,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.8,
        "bertscore_f1": 0.0,
        "char_precision": 0.36363636363636365,
        "char_recall": 0.8,
        "char_f1": 0.5000000000000001,
        "word_precision": 0.6666666666666666,
        "word_recall": 1.0,
        "word_f1": 0.8
      },
      "perplexity": 7.875,
      "nll": 2.0625
    },
    {
      "item_id": "test_331",
      "prediction": "AP",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.07692307692307693,
        "char_f1": 0.13333333333333336,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.75,
      "nll": 2.546875
    },
    {
      "item_id": "test_332",
      "prediction": "left",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4444444444444444,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.4,
        "char_f1": 0.4444444444444445,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.15625,
      "nll": 0.765625
    },
    {
      "item_id": "test_333",
      "prediction": "Right",
      "reference": "Right",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.28125,
      "nll": 0.828125
    },
    {
      "item_id": "test_334",
      "prediction": "Right",
      "reference": "Upper Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.47619047619047616,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.024066394763145416,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.375,
        "char_f1": 0.5454545454545454,
        "word_precision": 1.0,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.5
      },
      "perplexity": 29.25,
      "nll": 3.375
    },
    {
      "item_id": "test_335",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.71875,
      "nll": 1.5546875
    },
    {
      "item_id": "test_336",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.03125,
      "nll": 1.390625
    },
    {
      "item_id": "test_337",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9609375,
      "nll": 0.671875
    },
    {
      "item_id": "test_338",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6171875,
      "nll": 0.478515625
    },
    {
      "item_id": "test_339",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4296875,
      "nll": 0.357421875
    },
    {
      "item_id": "test_340",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 82.0,
      "nll": 4.40625
    },
    {
      "item_id": "test_341",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.671875,
      "nll": 0.98046875
    },
    {
      "item_id": "test_342",
      "prediction": "Pneumonia",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2222222222222222,
        "char_recall": 0.42857142857142855,
        "char_f1": 0.29268292682926833,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 158.0,
      "nll": 5.0625
    },
    {
      "item_id": "test_343",
      "prediction": "Pain",
      "reference": "Chest pain, dyspnea",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.34782608695652173,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.008853531856477262,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.2631578947368421,
        "char_f1": 0.38961038961038963,
        "word_precision": 1.0,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.5
      },
      "perplexity": 5.625,
      "nll": 1.7265625
    },
    {
      "item_id": "test_344",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.9375,
      "nll": 1.078125
    },
    {
      "item_id": "test_345",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 9.8125,
      "nll": 2.28125
    },
    {
      "item_id": "test_346",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.109375,
      "nll": 0.75
    },
    {
      "item_id": "test_347",
      "prediction": "Heart",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.6,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.4,
        "char_f1": 0.4000000000000001,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.546875,
      "nll": 1.265625
    },
    {
      "item_id": "test_348",
      "prediction": "Right lung",
      "reference": "Lower Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7692307692307693,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.19180183554164504,
        "rouge_l": 0.8,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.625,
        "char_f1": 0.7377049180327869,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 53.0,
      "nll": 3.96875
    },
    {
      "item_id": "test_349",
      "prediction": "Right upper lobe",
      "reference": "Lower Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4375,
        "word_overlap": 0.2,
        "bleu_4": 0.11362193664674995,
        "rouge_l": 0.3333333333333333,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.75,
        "char_f1": 0.75,
        "word_precision": 0.3333333333333333,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.3333333333333333
      },
      "perplexity": 58.0,
      "nll": 4.0625
    },
    {
      "item_id": "test_350",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.625,
      "nll": 1.2890625
    },
    {
      "item_id": "test_351",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2578125,
      "nll": 0.2265625
    },
    {
      "item_id": "test_352",
      "prediction": "Pneumonia",
      "reference": "Atelectasis",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.34782608695652173,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.765625,
      "nll": 1.015625
    },
    {
      "item_id": "test_353",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.109375,
      "nll": 0.74609375
    },
    {
      "item_id": "test_354",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.578125,
      "nll": 1.2734375
    },
    {
      "item_id": "test_355",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 43.75,
      "nll": 3.78125
    },
    {
      "item_id": "test_356",
      "prediction": "1",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.734375,
      "nll": 1.0078125
    },
    {
      "item_id": "test_357",
      "prediction": "Lungs",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 30.125,
      "nll": 3.40625
    },
    {
      "item_id": "test_358",
      "prediction": "Pneumonia",
      "reference": "Bronchial obstruction",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6666666666666666,
        "char_recall": 0.42857142857142855,
        "char_f1": 0.5217391304347826,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.125,
      "nll": 2.09375
    },
    {
      "item_id": "test_359",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 8.375,
      "nll": 2.125
    },
    {
      "item_id": "test_360",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.10205078125
    },
    {
      "item_id": "test_361",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.640625,
      "nll": 0.96875
    },
    {
      "item_id": "test_362",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.59375,
      "nll": 0.466796875
    },
    {
      "item_id": "test_363",
      "prediction": "Right",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.828125,
      "nll": 0.6015625
    },
    {
      "item_id": "test_364",
      "prediction": "Right upper lobe",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3225806451612903,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5625,
        "char_recall": 0.6,
        "char_f1": 0.5806451612903225,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 38.75,
      "nll": 3.65625
    },
    {
      "item_id": "test_365",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.15625,
      "nll": 0.1435546875
    },
    {
      "item_id": "test_366",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.12890625
    },
    {
      "item_id": "test_367",
      "prediction": "Pneumonia",
      "reference": "Pulmonary Mass",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5217391304347826,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7777777777777778,
        "char_recall": 0.5,
        "char_f1": 0.6086956521739131,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 26.25,
      "nll": 3.265625
    },
    {
      "item_id": "test_368",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.3125,
      "nll": 1.4609375
    },
    {
      "item_id": "test_369",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.8125,
      "nll": 1.5703125
    },
    {
      "item_id": "test_370",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 20.375,
      "nll": 3.015625
    },
    {
      "item_id": "test_371",
      "prediction": "1",
      "reference": "1",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.46875,
      "nll": 0.90234375
    },
    {
      "item_id": "test_372",
      "prediction": "No damage",
      "reference": "A Little",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23529411764705882,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2222222222222222,
        "char_recall": 0.25,
        "char_f1": 0.23529411764705882,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1928.0,
      "nll": 7.5625
    },
    {
      "item_id": "test_373",
      "prediction": "Answer: Avoid smoking",
      "reference": "Keep healthy",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.06060606060606061,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.14285714285714285,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 940.0,
      "nll": 6.84375
    },
    {
      "item_id": "test_374",
      "prediction": "Lungs",
      "reference": "Heart",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 11.4375,
      "nll": 2.4375
    },
    {
      "item_id": "test_375",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 8.9375,
      "nll": 2.1875
    },
    {
      "item_id": "test_376",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.4375,
      "nll": 2.34375
    },
    {
      "item_id": "test_377",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.625,
      "nll": 0.484375
    },
    {
      "item_id": "test_378",
      "prediction": "Right lung",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.56,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.4,
        "char_f1": 0.48,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 63.75,
      "nll": 4.15625
    },
    {
      "item_id": "test_379",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 49.75,
      "nll": 3.90625
    },
    {
      "item_id": "test_380",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5703125,
      "nll": 0.44921875
    },
    {
      "item_id": "test_381",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.875,
      "nll": 0.62890625
    },
    {
      "item_id": "test_382",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.65625,
      "nll": 1.296875
    },
    {
      "item_id": "test_383",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.125,
      "nll": 0.75390625
    },
    {
      "item_id": "test_384",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.359375,
      "nll": 1.2109375
    },
    {
      "item_id": "test_385",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 29.25,
      "nll": 3.375
    },
    {
      "item_id": "test_386",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.6875,
      "nll": 0.98828125
    },
    {
      "item_id": "test_387",
      "prediction": "The chest X-ray is a low-dose exposure to radiation. The radiation dose is very low and is not harmful.",
      "reference": "A Little",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.12612612612612611,
        "word_overlap": 0.0625,
        "bleu_4": 0.00913442366656447,
        "rouge_l": 0.08695652173913042,
        "bertscore_f1": 0.0,
        "char_precision": 0.39805825242718446,
        "char_recall": 0.75,
        "char_f1": 0.5200845665961944,
        "word_precision": 0.06666666666666667,
        "word_recall": 0.5,
        "word_f1": 0.11764705882352941
      },
      "perplexity": 3168.0,
      "nll": 8.0625
    },
    {
      "item_id": "test_388",
      "prediction": "Answer: antibiotics",
      "reference": "Physical therapy, surgical treatment",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14545454545454545,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7368421052631579,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.7,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 127.0,
      "nll": 4.84375
    },
    {
      "item_id": "test_389",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.125,
      "nll": 1.140625
    },
    {
      "item_id": "test_390",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1640625,
      "nll": 0.1494140625
    },
    {
      "item_id": "test_391",
      "prediction": "AP",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.07692307692307693,
        "char_f1": 0.13333333333333336,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 18.25,
      "nll": 2.90625
    },
    {
      "item_id": "test_392",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.7421875,
      "nll": 0.5546875
    },
    {
      "item_id": "test_393",
      "prediction": "Right",
      "reference": "Right",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.765625,
      "nll": 1.015625
    },
    {
      "item_id": "test_394",
      "prediction": "Right",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.6666666666666666,
        "word_overlap": 0.5,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.6,
        "char_f1": 0.7499999999999999,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 25.0,
      "nll": 3.21875
    },
    {
      "item_id": "test_395",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.1875,
      "nll": 1.6484375
    },
    {
      "item_id": "test_396",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.4375,
      "nll": 1.234375
    },
    {
      "item_id": "test_397",
      "prediction": "Pneumonia",
      "reference": "Pneumonia",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.796875,
      "nll": 0.5859375
    },
    {
      "item_id": "test_398",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7109375,
      "nll": 0.5390625
    },
    {
      "item_id": "test_399",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.65625,
      "nll": 1.296875
    },
    {
      "item_id": "test_400",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 209.0,
      "nll": 5.34375
    },
    {
      "item_id": "test_401",
      "prediction": "1",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.59375,
      "nll": 0.953125
    },
    {
      "item_id": "test_402",
      "prediction": "Lung",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 27.0,
      "nll": 3.296875
    },
    {
      "item_id": "test_403",
      "prediction": "Answer: antibiotics",
      "reference": "Medical therapy, supportive therapy",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14814814814814814,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6842105263157895,
        "char_recall": 0.6,
        "char_f1": 0.639344262295082,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 54.5,
      "nll": 4.0
    },
    {
      "item_id": "test_404",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.625,
      "nll": 1.7265625
    },
    {
      "item_id": "test_405",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 8.375,
      "nll": 2.125
    },
    {
      "item_id": "test_406",
      "prediction": "Transverse plane",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5517241379310345,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.6875,
        "char_recall": 0.6923076923076923,
        "char_f1": 0.6898954703832753,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 3.15625,
      "nll": 1.1484375
    },
    {
      "item_id": "test_407",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.015625,
      "nll": 1.1015625
    },
    {
      "item_id": "test_408",
      "prediction": "Right lung",
      "reference": "Heart",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.1,
        "char_recall": 0.2,
        "char_f1": 0.13333333333333333,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 416.0,
      "nll": 6.03125
    },
    {
      "item_id": "test_409",
      "prediction": "cardiac silhouette",
      "reference": "Heart",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2608695652173913,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3888888888888889,
        "char_recall": 0.8,
        "char_f1": 0.5233644859813085,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 139.0,
      "nll": 4.9375
    },
    {
      "item_id": "test_410",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.25,
      "nll": 1.4453125
    },
    {
      "item_id": "test_411",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.59375,
      "nll": 1.5234375
    },
    {
      "item_id": "test_412",
      "prediction": "Pneumonia",
      "reference": "Cardiomegaly",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09523809523809523,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.5,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.328125,
      "nll": 0.84765625
    },
    {
      "item_id": "test_413",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8671875,
      "nll": 0.625
    },
    {
      "item_id": "test_414",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6484375,
      "nll": 0.498046875
    },
    {
      "item_id": "test_415",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 26.25,
      "nll": 3.265625
    },
    {
      "item_id": "test_416",
      "prediction": "5",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.21875,
      "nll": 1.4375
    },
    {
      "item_id": "test_417",
      "prediction": "Lung",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 24.625,
      "nll": 3.203125
    },
    {
      "item_id": "test_418",
      "prediction": "The patient will have symptoms of heart failure",
      "reference": "Chest tightness, arrhythmia",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2702702702702703,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.723404255319149,
        "char_recall": 0.8888888888888888,
        "char_f1": 0.7976539589442816,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 15.625,
      "nll": 2.75
    },
    {
      "item_id": "test_419",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.59375,
      "nll": 1.28125
    },
    {
      "item_id": "test_420",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 7.0625,
      "nll": 1.953125
    },
    {
      "item_id": "test_421",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9921875,
      "nll": 0.6875
    },
    {
      "item_id": "test_422",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.625,
      "nll": 0.484375
    },
    {
      "item_id": "test_423",
      "prediction": "Right upper lobe",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.1875,
        "char_recall": 0.5,
        "char_f1": 0.2727272727272727,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.40625,
      "nll": 1.484375
    },
    {
      "item_id": "test_424",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 30.625,
      "nll": 3.421875
    },
    {
      "item_id": "test_425",
      "prediction": "Cardiovascular",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21052631578947367,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.14285714285714285,
        "char_recall": 0.4,
        "char_f1": 0.21052631578947364,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.53125,
      "nll": 1.5078125
    },
    {
      "item_id": "test_426",
      "prediction": "Cardiovascular",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21052631578947367,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.14285714285714285,
        "char_recall": 0.4,
        "char_f1": 0.21052631578947364,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.0,
      "nll": 1.609375
    },
    {
      "item_id": "test_427",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.78125,
      "nll": 1.0234375
    },
    {
      "item_id": "test_428",
      "prediction": "Pneumonia",
      "reference": "Pulmonary Infiltration",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3870967741935484,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8888888888888888,
        "char_recall": 0.5454545454545454,
        "char_f1": 0.676056338028169,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.375,
      "nll": 2.515625
    },
    {
      "item_id": "test_429",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.734375,
      "nll": 1.0078125
    },
    {
      "item_id": "test_430",
      "prediction": "white",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 29.625,
      "nll": 3.390625
    },
    {
      "item_id": "test_431",
      "prediction": "1",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.8125,
      "nll": 1.7578125
    },
    {
      "item_id": "test_432",
      "prediction": "The heart is enlarged.",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.20689655172413793,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.8571428571428571,
        "char_f1": 0.631578947368421,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 45.25,
      "nll": 3.8125
    },
    {
      "item_id": "test_433",
      "prediction": "Pneumonia",
      "reference": "Live healthy, enhance physical fitness",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1276595744680851,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.3684210526315789,
        "char_f1": 0.4430379746835443,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 536.0,
      "nll": 6.28125
    },
    {
      "item_id": "test_434",
      "prediction": "lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.75,
        "char_f1": 0.75,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.421875,
      "nll": 0.8828125
    },
    {
      "item_id": "test_435",
      "prediction": "chest x-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.625,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.8,
        "bertscore_f1": 0.0,
        "char_precision": 0.2727272727272727,
        "char_recall": 0.6,
        "char_f1": 0.37499999999999994,
        "word_precision": 0.6666666666666666,
        "word_recall": 1.0,
        "word_f1": 0.8
      },
      "perplexity": 9.8125,
      "nll": 2.28125
    },
    {
      "item_id": "test_436",
      "prediction": "Transverse plane",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5517241379310345,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.6875,
        "char_recall": 0.6923076923076923,
        "char_f1": 0.6898954703832753,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 3.109375,
      "nll": 1.1328125
    },
    {
      "item_id": "test_437",
      "prediction": "left",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4444444444444444,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.4,
        "char_f1": 0.4444444444444445,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.984375,
      "nll": 0.68359375
    },
    {
      "item_id": "test_438",
      "prediction": "Right upper lobe",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3225806451612903,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5625,
        "char_recall": 0.6,
        "char_f1": 0.5806451612903225,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 28.375,
      "nll": 3.34375
    },
    {
      "item_id": "test_439",
      "prediction": "Right upper lobe",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3225806451612903,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5625,
        "char_recall": 0.6,
        "char_f1": 0.5806451612903225,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 19.125,
      "nll": 2.953125
    },
    {
      "item_id": "test_440",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.953125,
      "nll": 1.375
    },
    {
      "item_id": "test_441",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.25,
      "nll": 1.4453125
    },
    {
      "item_id": "test_442",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6640625,
      "nll": 0.5078125
    },
    {
      "item_id": "test_443",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.734375,
      "nll": 0.55078125
    },
    {
      "item_id": "test_444",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.625,
      "nll": 1.2890625
    },
    {
      "item_id": "test_445",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 96.0,
      "nll": 4.5625
    },
    {
      "item_id": "test_446",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.578125,
      "nll": 0.94921875
    },
    {
      "item_id": "test_447",
      "prediction": "Lung",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 14.25,
      "nll": 2.65625
    },
    {
      "item_id": "test_448",
      "prediction": "Antituberculosis therapy",
      "reference": "Medical treatment, surgical treatment",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19672131147540983,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.8378378378378378,
        "char_f1": 0.7914893617021277,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 23.125,
      "nll": 3.140625
    },
    {
      "item_id": "test_449",
      "prediction": "heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.640625,
      "nll": 0.97265625
    },
    {
      "item_id": "test_450",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 9.9375,
      "nll": 2.296875
    },
    {
      "item_id": "test_451",
      "prediction": "Transverse plane",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5517241379310345,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.6875,
        "char_recall": 0.6923076923076923,
        "char_f1": 0.6898954703832753,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 2.921875,
      "nll": 1.0703125
    },
    {
      "item_id": "test_452",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.78125,
      "nll": 1.0234375
    },
    {
      "item_id": "test_453",
      "prediction": "Right lung",
      "reference": "Upper Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7692307692307693,
        "word_overlap": 0.6666666666666666,
        "bleu_4": 0.19180183554164504,
        "rouge_l": 0.8,
        "bertscore_f1": 0.0,
        "char_precision": 0.9,
        "char_recall": 0.625,
        "char_f1": 0.7377049180327869,
        "word_precision": 1.0,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.8
      },
      "perplexity": 18.25,
      "nll": 2.90625
    },
    {
      "item_id": "test_454",
      "prediction": "Right upper lobe",
      "reference": "Upper Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4375,
        "word_overlap": 0.5,
        "bleu_4": 0.13512001548070346,
        "rouge_l": 0.3333333333333333,
        "bertscore_f1": 0.0,
        "char_precision": 0.8125,
        "char_recall": 0.8125,
        "char_f1": 0.8125,
        "word_precision": 0.6666666666666666,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 17.125,
      "nll": 2.84375
    },
    {
      "item_id": "test_455",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.28125,
      "nll": 1.453125
    },
    {
      "item_id": "test_456",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.09375,
      "nll": 1.625
    },
    {
      "item_id": "test_457",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.1875,
      "nll": 0.78515625
    },
    {
      "item_id": "test_458",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.734375,
      "nll": 1.0078125
    },
    {
      "item_id": "test_459",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.125,
      "nll": 1.6328125
    },
    {
      "item_id": "test_460",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 66.0,
      "nll": 4.1875
    },
    {
      "item_id": "test_461",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.421875,
      "nll": 0.8828125
    },
    {
      "item_id": "test_462",
      "prediction": "Lung",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 25.0,
      "nll": 3.21875
    },
    {
      "item_id": "test_463",
      "prediction": "Pain",
      "reference": "Chest tightness, fatigue",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14285714285714285,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.16666666666666666,
        "char_f1": 0.27272727272727276,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 21.0,
      "nll": 3.046875
    },
    {
      "item_id": "test_464",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.734375,
      "nll": 1.0078125
    },
    {
      "item_id": "test_465",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.28125,
      "nll": 0.25
    },
    {
      "item_id": "test_466",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.15625,
      "nll": 1.421875
    },
    {
      "item_id": "test_467",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.25,
      "nll": 0.2216796875
    },
    {
      "item_id": "test_468",
      "prediction": "Right lung",
      "reference": "Upper Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.56,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.4,
        "char_f1": 0.48,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 63.75,
      "nll": 4.15625
    },
    {
      "item_id": "test_469",
      "prediction": "Right",
      "reference": "Upper Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 72.5,
      "nll": 4.28125
    },
    {
      "item_id": "test_470",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.53125,
      "nll": 1.5078125
    },
    {
      "item_id": "test_471",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.21875,
      "nll": 1.4375
    },
    {
      "item_id": "test_472",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.078125,
      "nll": 0.73046875
    },
    {
      "item_id": "test_473",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.421875,
      "nll": 0.88671875
    },
    {
      "item_id": "test_474",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9140625,
      "nll": 0.6484375
    },
    {
      "item_id": "test_475",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 11.625,
      "nll": 2.453125
    },
    {
      "item_id": "test_476",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.984375,
      "nll": 1.3828125
    },
    {
      "item_id": "test_477",
      "prediction": "The radiation dose from this chest X-ray is approximately 0.1 mSv.",
      "reference": "A Little",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.08108108108108109,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.36363636363636365,
        "char_recall": 0.75,
        "char_f1": 0.48979591836734687,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1696.0,
      "nll": 7.4375
    },
    {
      "item_id": "test_478",
      "prediction": "Answer: antibiotics",
      "reference": "Medical therapy, supportive therapy",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14814814814814814,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6842105263157895,
        "char_recall": 0.6,
        "char_f1": 0.639344262295082,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 46.75,
      "nll": 3.84375
    },
    {
      "item_id": "test_479",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 11.0625,
      "nll": 2.40625
    },
    {
      "item_id": "test_480",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1875,
      "nll": 0.1708984375
    },
    {
      "item_id": "test_481",
      "prediction": "Frontal",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7142857142857143,
        "char_recall": 0.6923076923076923,
        "char_f1": 0.7031250000000001,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.15625,
      "nll": 1.96875
    },
    {
      "item_id": "test_482",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.203125,
      "nll": 0.79296875
    },
    {
      "item_id": "test_483",
      "prediction": "Right lung",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.56,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.4,
        "char_f1": 0.48,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 31.125,
      "nll": 3.4375
    },
    {
      "item_id": "test_484",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 90.0,
      "nll": 4.5
    },
    {
      "item_id": "test_485",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.09375,
      "nll": 1.625
    },
    {
      "item_id": "test_486",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.78125,
      "nll": 1.5625
    },
    {
      "item_id": "test_487",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.25,
      "nll": 0.8125
    },
    {
      "item_id": "test_488",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.453125,
      "nll": 0.8984375
    },
    {
      "item_id": "test_489",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.796875,
      "nll": 1.3359375
    },
    {
      "item_id": "test_490",
      "prediction": "white",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.3125,
      "nll": 2.234375
    },
    {
      "item_id": "test_491",
      "prediction": "2",
      "reference": "1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.265625,
      "nll": 0.81640625
    },
    {
      "item_id": "test_492",
      "prediction": "Pneumothorax",
      "reference": "Promote blood flow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5833333333333334,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.5691056910569107,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 268.0,
      "nll": 5.59375
    },
    {
      "item_id": "test_493",
      "prediction": "Pneumonia",
      "reference": "Chest injury, lung disease, bullae",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13953488372093023,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6666666666666666,
        "char_recall": 0.38235294117647056,
        "char_f1": 0.485981308411215,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 58.0,
      "nll": 4.0625
    },
    {
      "item_id": "test_494",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 6.40625,
      "nll": 1.859375
    },
    {
      "item_id": "test_495",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.134765625
    },
    {
      "item_id": "test_496",
      "prediction": "Coronal",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.7,
        "word_overlap": 0.5,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.7692307692307693,
        "char_f1": 0.8695652173913044,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 5.0,
      "nll": 1.609375
    },
    {
      "item_id": "test_497",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.65625,
      "nll": 1.296875
    },
    {
      "item_id": "test_498",
      "prediction": "Right lung",
      "reference": "Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.631578947368421,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.576923076923077,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 53.0,
      "nll": 3.96875
    },
    {
      "item_id": "test_499",
      "prediction": "Right upper lobe",
      "reference": "Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.24,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4375,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.48951048951048953,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.75,
      "nll": 2.546875
    },
    {
      "item_id": "test_500",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.171875,
      "nll": 0.77734375
    },
    {
      "item_id": "test_501",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.53125,
      "nll": 1.5078125
    },
    {
      "item_id": "test_502",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.203125,
      "nll": 0.79296875
    },
    {
      "item_id": "test_503",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.5,
      "nll": 0.9140625
    },
    {
      "item_id": "test_504",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.953125,
      "nll": 0.66796875
    },
    {
      "item_id": "test_505",
      "prediction": "white",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.625,
      "nll": 2.15625
    },
    {
      "item_id": "test_506",
      "prediction": "1",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.796875,
      "nll": 1.3359375
    },
    {
      "item_id": "test_507",
      "prediction": "Lung",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.0,
      "nll": 2.484375
    },
    {
      "item_id": "test_508",
      "prediction": "Pneumonectomy",
      "reference": "Quit smoking, enhance physical fitness",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19607843137254902,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.9230769230769231,
        "char_recall": 0.39473684210526316,
        "char_f1": 0.5529953917050692,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 87.0,
      "nll": 4.46875
    },
    {
      "item_id": "test_509",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.4375,
      "nll": 0.890625
    },
    {
      "item_id": "test_510",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.126953125
    },
    {
      "item_id": "test_511",
      "prediction": "Frontal",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7142857142857143,
        "char_recall": 0.6923076923076923,
        "char_f1": 0.7031250000000001,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.1875,
      "nll": 2.21875
    },
    {
      "item_id": "test_512",
      "prediction": "chest",
      "reference": "Chest",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9296875,
      "nll": 0.65625
    },
    {
      "item_id": "test_513",
      "prediction": "Right",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.234375,
      "nll": 0.80078125
    },
    {
      "item_id": "test_514",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 32.5,
      "nll": 3.484375
    },
    {
      "item_id": "test_515",
      "prediction": "Cardiovascular system",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23076923076923078,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2857142857142857,
        "char_recall": 0.8,
        "char_f1": 0.4210526315789473,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.84375,
      "nll": 1.046875
    },
    {
      "item_id": "test_516",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.1875,
      "nll": 1.4296875
    },
    {
      "item_id": "test_517",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.046875,
      "nll": 0.71875
    },
    {
      "item_id": "test_518",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7578125,
      "nll": 0.5625
    },
    {
      "item_id": "test_519",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.0625,
      "nll": 1.1171875
    },
    {
      "item_id": "test_520",
      "prediction": "white",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 90.0,
      "nll": 4.5
    },
    {
      "item_id": "test_521",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.84375,
      "nll": 1.046875
    },
    {
      "item_id": "test_522",
      "prediction": "The heart",
      "reference": "Promote blood flow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.2777777777777778,
        "char_f1": 0.3703703703703704,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 115.5,
      "nll": 4.75
    },
    {
      "item_id": "test_523",
      "prediction": "Pneumonia",
      "reference": "Medical therapy, supportive therapy",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13636363636363635,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5555555555555556,
        "char_recall": 0.3142857142857143,
        "char_f1": 0.40145985401459855,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 38.75,
      "nll": 3.65625
    },
    {
      "item_id": "test_524",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.40625,
      "nll": 0.875
    },
    {
      "item_id": "test_525",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 7.5,
      "nll": 2.015625
    },
    {
      "item_id": "test_526",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.28125,
      "nll": 1.453125
    },
    {
      "item_id": "test_527",
      "prediction": "left",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4444444444444444,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.4,
        "char_f1": 0.4444444444444445,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.390625,
      "nll": 0.87109375
    },
    {
      "item_id": "test_528",
      "prediction": "Right upper lobe",
      "reference": "Upper Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4375,
        "word_overlap": 0.5,
        "bleu_4": 0.13512001548070346,
        "rouge_l": 0.3333333333333333,
        "bertscore_f1": 0.0,
        "char_precision": 0.8125,
        "char_recall": 0.8125,
        "char_f1": 0.8125,
        "word_precision": 0.6666666666666666,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 21.375,
      "nll": 3.0625
    },
    {
      "item_id": "test_529",
      "prediction": "Right upper lobe",
      "reference": "Upper Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4375,
        "word_overlap": 0.5,
        "bleu_4": 0.13512001548070346,
        "rouge_l": 0.3333333333333333,
        "bertscore_f1": 0.0,
        "char_precision": 0.8125,
        "char_recall": 0.8125,
        "char_f1": 0.8125,
        "word_precision": 0.6666666666666666,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 25.375,
      "nll": 3.234375
    },
    {
      "item_id": "test_530",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.125,
      "nll": 1.6328125
    },
    {
      "item_id": "test_531",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1953125,
      "nll": 0.177734375
    },
    {
      "item_id": "test_532",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.921875,
      "nll": 1.0703125
    },
    {
      "item_id": "test_533",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.53125,
      "nll": 0.9296875
    },
    {
      "item_id": "test_534",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.078125,
      "nll": 1.125
    },
    {
      "item_id": "test_535",
      "prediction": "white",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 15.875,
      "nll": 2.765625
    },
    {
      "item_id": "test_536",
      "prediction": "1",
      "reference": "1",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0,
      "nll": 0.6953125
    },
    {
      "item_id": "test_537",
      "prediction": "Lungs",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 13.8125,
      "nll": 2.625
    },
    {
      "item_id": "test_538",
      "prediction": "Answer: Avoid smoking",
      "reference": "Pay attention to prevent cold and keep warm, enhance physical fitness",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15555555555555556,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8095238095238095,
        "char_recall": 0.6086956521739131,
        "char_f1": 0.6948905109489051,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 28.75,
      "nll": 3.359375
    },
    {
      "item_id": "test_539",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.40625,
      "nll": 1.2265625
    },
    {
      "item_id": "test_540",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 7.21875,
      "nll": 1.9765625
    },
    {
      "item_id": "test_541",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.40625,
      "nll": 0.341796875
    },
    {
      "item_id": "test_542",
      "prediction": "Lung",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.5625,
      "nll": 0.94140625
    },
    {
      "item_id": "test_543",
      "prediction": "Right",
      "reference": "Right",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.890625,
      "nll": 0.63671875
    },
    {
      "item_id": "test_544",
      "prediction": "Right",
      "reference": "Lower Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.47619047619047616,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.024066394763145416,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.375,
        "char_f1": 0.5454545454545454,
        "word_precision": 1.0,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.5
      },
      "perplexity": 31.625,
      "nll": 3.453125
    },
    {
      "item_id": "test_545",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.130859375
    },
    {
      "item_id": "test_546",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.84375,
      "nll": 1.765625
    },
    {
      "item_id": "test_547",
      "prediction": "Pneumonia",
      "reference": "Pneumonia",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7109375,
      "nll": 0.53515625
    },
    {
      "item_id": "test_548",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8828125,
      "nll": 0.6328125
    },
    {
      "item_id": "test_549",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.0,
      "nll": 1.609375
    },
    {
      "item_id": "test_550",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 54.5,
      "nll": 4.0
    },
    {
      "item_id": "test_551",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.125,
      "nll": 0.75390625
    },
    {
      "item_id": "test_552",
      "prediction": "Pneumothorax",
      "reference": "Promote blood flow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5833333333333334,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.5691056910569107,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 191.0,
      "nll": 5.25
    },
    {
      "item_id": "test_553",
      "prediction": "Answer: Avoid smoking",
      "reference": "Pay attention to prevent cold and keep warm, enhance physical fitness",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15555555555555556,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8095238095238095,
        "char_recall": 0.6086956521739131,
        "char_f1": 0.6948905109489051,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 30.625,
      "nll": 3.421875
    },
    {
      "item_id": "test_554",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.375,
      "nll": 0.86328125
    },
    {
      "item_id": "test_555",
      "prediction": "no",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.203125,
      "nll": 0.1826171875
    },
    {
      "item_id": "test_556",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.1875,
      "nll": 0.78515625
    },
    {
      "item_id": "test_557",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.734375,
      "nll": 1.0078125
    },
    {
      "item_id": "test_558",
      "prediction": "Left",
      "reference": "Left",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.046875,
      "nll": 0.71484375
    },
    {
      "item_id": "test_559",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 53.0,
      "nll": 3.96875
    },
    {
      "item_id": "test_560",
      "prediction": "Lungs",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8888888888888888,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 1.0,
        "char_f1": 0.888888888888889,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.40625,
      "nll": 1.484375
    },
    {
      "item_id": "test_561",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.015625,
      "nll": 0.703125
    },
    {
      "item_id": "test_562",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6328125,
      "nll": 0.4921875
    },
    {
      "item_id": "test_563",
      "prediction": "Pneumonia",
      "reference": "Pulmonary Infiltration",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3870967741935484,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8888888888888888,
        "char_recall": 0.5454545454545454,
        "char_f1": 0.676056338028169,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.3125,
      "nll": 2.234375
    },
    {
      "item_id": "test_564",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.125,
      "nll": 0.75390625
    },
    {
      "item_id": "test_565",
      "prediction": "black",
      "reference": "Black",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 7.5,
      "nll": 2.015625
    },
    {
      "item_id": "test_566",
      "prediction": "1",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.09375,
      "nll": 0.73828125
    },
    {
      "item_id": "test_567",
      "prediction": "Gas exchange",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3157894736842105,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4166666666666667,
        "char_recall": 0.5714285714285714,
        "char_f1": 0.48192771084337344,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.84375,
      "nll": 1.578125
    },
    {
      "item_id": "test_568",
      "prediction": "Pneumonia",
      "reference": "Chest pain, cough, expectoration",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1951219512195122,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7777777777777778,
        "char_recall": 0.40625,
        "char_f1": 0.533724340175953,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.21875,
      "nll": 1.9765625
    },
    {
      "item_id": "test_569",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.71875,
      "nll": 1.0
    },
    {
      "item_id": "test_570",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.1181640625
    },
    {
      "item_id": "test_571",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 29.25,
      "nll": 3.375
    },
    {
      "item_id": "test_572",
      "prediction": "thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.4,
        "char_f1": 0.3636363636363636,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.09375,
      "nll": 0.7421875
    },
    {
      "item_id": "test_573",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 43.25,
      "nll": 3.765625
    },
    {
      "item_id": "test_574",
      "prediction": "Right pleural",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.35714285714285715,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.46153846153846156,
        "char_recall": 0.5333333333333333,
        "char_f1": 0.4948453608247423,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 115.5,
      "nll": 4.75
    },
    {
      "item_id": "test_575",
      "prediction": "Lungs",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.2,
        "char_f1": 0.20000000000000004,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.0625,
      "nll": 1.8046875
    },
    {
      "item_id": "test_576",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1953125,
      "nll": 0.1787109375
    },
    {
      "item_id": "test_577",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.3125,
      "nll": 0.83984375
    },
    {
      "item_id": "test_578",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0,
      "nll": 0.6953125
    },
    {
      "item_id": "test_579",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.15625,
      "nll": 1.1484375
    },
    {
      "item_id": "test_580",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 27.5,
      "nll": 3.3125
    },
    {
      "item_id": "test_581",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.796875,
      "nll": 1.03125
    },
    {
      "item_id": "test_582",
      "prediction": "Respiration",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4444444444444444,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.36363636363636365,
        "char_recall": 0.7142857142857143,
        "char_f1": 0.48192771084337355,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.65625,
      "nll": 1.734375
    },
    {
      "item_id": "test_583",
      "prediction": "Pneumonia",
      "reference": "Inflammation, malignant tumor, trauma, etc",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19607843137254902,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8888888888888888,
        "char_recall": 0.5238095238095238,
        "char_f1": 0.6591760299625469,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 23.875,
      "nll": 3.171875
    },
    {
      "item_id": "test_584",
      "prediction": "Heart",
      "reference": "Heart",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.015625,
      "nll": 1.1015625
    },
    {
      "item_id": "test_585",
      "prediction": "X-ray",
      "reference": "X-Ray",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 5.84375,
      "nll": 1.765625
    },
    {
      "item_id": "test_586",
      "prediction": "Frontal",
      "reference": "Coronal Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7142857142857143,
        "char_recall": 0.6923076923076923,
        "char_f1": 0.7031250000000001,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.125,
      "nll": 2.3125
    },
    {
      "item_id": "test_587",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.796875,
      "nll": 0.5859375
    },
    {
      "item_id": "test_588",
      "prediction": "The heart is not visible in this image",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2631578947368421,
        "char_recall": 0.8333333333333334,
        "char_f1": 0.39999999999999997,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 880.0,
      "nll": 6.78125
    },
    {
      "item_id": "test_589",
      "prediction": "Right",
      "reference": "Lower Left Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.13333333333333333,
        "char_f1": 0.2,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 237.0,
      "nll": 5.46875
    },
    {
      "item_id": "test_590",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4765625,
      "nll": 0.390625
    },
    {
      "item_id": "test_591",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.328125,
      "nll": 0.28515625
    },
    {
      "item_id": "test_592",
      "prediction": "Pneumonia",
      "reference": "Pleural Effusion",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8888888888888888,
        "char_recall": 0.5,
        "char_f1": 0.64,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.78125,
      "nll": 1.5625
    },
    {
      "item_id": "test_593",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.421875,
      "nll": 0.8828125
    },
    {
      "item_id": "test_594",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.125,
      "nll": 1.4140625
    },
    {
      "item_id": "test_595",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 70.0,
      "nll": 4.25
    },
    {
      "item_id": "test_596",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 8.375,
      "nll": 2.125
    },
    {
      "item_id": "test_597",
      "prediction": "Pneumothorax",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3157894736842105,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4166666666666667,
        "char_recall": 0.8571428571428571,
        "char_f1": 0.5607476635514019,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 174.0,
      "nll": 5.15625
    },
    {
      "item_id": "test_598",
      "prediction": "Pleuritic pain",
      "reference": "Chest tightness, dyspnea, chest pain",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.36,
        "word_overlap": 0.2,
        "bleu_4": 0.012274570264879268,
        "rouge_l": 0.28571428571428575,
        "bertscore_f1": 0.0,
        "char_precision": 0.7142857142857143,
        "char_recall": 0.6111111111111112,
        "char_f1": 0.6586826347305389,
        "word_precision": 0.5,
        "word_recall": 0.25,
        "word_f1": 0.3333333333333333
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_599",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.5,
      "nll": 0.9140625
    },
    {
      "item_id": "test_600",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.037841796875
    },
    {
      "item_id": "test_601",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.453125,
      "nll": 0.375
    },
    {
      "item_id": "test_602",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.12451171875
    },
    {
      "item_id": "test_603",
      "prediction": "Brain stem is located in the center of the image.",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21818181818181817,
        "word_overlap": 0.1111111111111111,
        "bleu_4": 0.018850319022747353,
        "rouge_l": 0.18181818181818182,
        "bertscore_f1": 0.0,
        "char_precision": 0.3469387755102041,
        "char_recall": 0.8333333333333334,
        "char_f1": 0.48991354466858783,
        "word_precision": 0.1111111111111111,
        "word_recall": 1.0,
        "word_f1": 0.19999999999999998
      },
      "perplexity": 174.0,
      "nll": 5.15625
    },
    {
      "item_id": "test_604",
      "prediction": "Brain",
      "reference": "Temporal Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.15384615384615385,
        "char_f1": 0.2222222222222222,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.0,
      "nll": 2.484375
    },
    {
      "item_id": "test_605",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0859375,
      "nll": 0.0830078125
    },
    {
      "item_id": "test_606",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.038330078125
    },
    {
      "item_id": "test_607",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9453125,
      "nll": 0.6640625
    },
    {
      "item_id": "test_608",
      "prediction": "White",
      "reference": "Gray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 20.125,
      "nll": 3.0
    },
    {
      "item_id": "test_609",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.46875,
      "nll": 0.384765625
    },
    {
      "item_id": "test_610",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 209.0,
      "nll": 5.34375
    },
    {
      "item_id": "test_611",
      "prediction": "No damage",
      "reference": "Much",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 15168.0,
      "nll": 9.625
    },
    {
      "item_id": "test_612",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8515625,
      "nll": 0.6171875
    },
    {
      "item_id": "test_613",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.421875,
      "nll": 0.8828125
    },
    {
      "item_id": "test_614",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3515625,
      "nll": 0.302734375
    },
    {
      "item_id": "test_615",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.115234375
    },
    {
      "item_id": "test_616",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6328125,
      "nll": 0.48828125
    },
    {
      "item_id": "test_617",
      "prediction": "Head",
      "reference": "Head",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.546875,
      "nll": 1.265625
    },
    {
      "item_id": "test_618",
      "prediction": "Brain stem is located in the center of the image.",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21818181818181817,
        "word_overlap": 0.1111111111111111,
        "bleu_4": 0.018850319022747353,
        "rouge_l": 0.18181818181818182,
        "bertscore_f1": 0.0,
        "char_precision": 0.3469387755102041,
        "char_recall": 0.8333333333333334,
        "char_f1": 0.48991354466858783,
        "word_precision": 0.1111111111111111,
        "word_recall": 1.0,
        "word_f1": 0.19999999999999998
      },
      "perplexity": 230.0,
      "nll": 5.4375
    },
    {
      "item_id": "test_619",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.8125,
      "nll": 1.5703125
    },
    {
      "item_id": "test_620",
      "prediction": "Head and neck",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.47058823529411764,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.11362193664674995,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.5384615384615384,
        "char_recall": 1.0,
        "char_f1": 0.7000000000000001,
        "word_precision": 0.3333333333333333,
        "word_recall": 1.0,
        "word_f1": 0.5
      },
      "perplexity": 3.71875,
      "nll": 1.3125
    },
    {
      "item_id": "test_621",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.609375,
      "nll": 0.95703125
    },
    {
      "item_id": "test_622",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0546875,
      "nll": 0.056396484375
    },
    {
      "item_id": "test_623",
      "prediction": "white",
      "reference": "White",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.8,
        "char_f1": 0.8000000000000002,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.15625,
      "nll": 1.1484375
    },
    {
      "item_id": "test_624",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.671875,
      "nll": 0.51171875
    },
    {
      "item_id": "test_625",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 139.0,
      "nll": 4.9375
    },
    {
      "item_id": "test_626",
      "prediction": "Brain",
      "reference": "Control heartbeat and breathing",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2777777777777778,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.3548387096774194,
        "char_f1": 0.4916201117318436,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2048.0,
      "nll": 7.625
    },
    {
      "item_id": "test_627",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1484375,
      "nll": 0.1357421875
    },
    {
      "item_id": "test_628",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.34375,
      "nll": 0.296875
    },
    {
      "item_id": "test_629",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4140625,
      "nll": 0.345703125
    },
    {
      "item_id": "test_630",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 13.8125,
      "nll": 2.625
    },
    {
      "item_id": "test_631",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1640625,
      "nll": 0.1494140625
    },
    {
      "item_id": "test_632",
      "prediction": "Tooth is in the maxillary sinus.",
      "reference": "Top",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11428571428571428,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.09375,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.16438356164383564,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 776.0,
      "nll": 6.65625
    },
    {
      "item_id": "test_633",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.12451171875
    },
    {
      "item_id": "test_634",
      "prediction": "Head and neck",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.47058823529411764,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.11362193664674995,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.5384615384615384,
        "char_recall": 1.0,
        "char_f1": 0.7000000000000001,
        "word_precision": 0.3333333333333333,
        "word_recall": 1.0,
        "word_f1": 0.5
      },
      "perplexity": 2.625,
      "nll": 0.96484375
    },
    {
      "item_id": "test_635",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.171875,
      "nll": 0.77734375
    },
    {
      "item_id": "test_636",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.25,
      "nll": 1.1796875
    },
    {
      "item_id": "test_637",
      "prediction": "White",
      "reference": "Gray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.125,
      "nll": 2.3125
    },
    {
      "item_id": "test_638",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.28125,
      "nll": 0.828125
    },
    {
      "item_id": "test_639",
      "prediction": "1",
      "reference": "2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.1875,
      "nll": 1.8203125
    },
    {
      "item_id": "test_640",
      "prediction": "Thyroid gland",
      "reference": "Cutting, chewing, maintaining facial contour and assisting pronunciation",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16470588235294117,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8461538461538461,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.7457627118644068,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 112.0,
      "nll": 4.71875
    },
    {
      "item_id": "test_641",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.171875,
      "nll": 1.15625
    },
    {
      "item_id": "test_642",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.375,
      "nll": 0.86328125
    },
    {
      "item_id": "test_643",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6328125,
      "nll": 0.4921875
    },
    {
      "item_id": "test_644",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4140625,
      "nll": 0.34765625
    },
    {
      "item_id": "test_645",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.8125,
      "nll": 2.28125
    },
    {
      "item_id": "test_646",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.984375,
      "nll": 1.09375
    },
    {
      "item_id": "test_647",
      "prediction": "Neck",
      "reference": "Neck",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.34375,
      "nll": 0.8515625
    },
    {
      "item_id": "test_648",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.6484375,
      "nll": 0.498046875
    },
    {
      "item_id": "test_649",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.46875,
      "nll": 0.90234375
    },
    {
      "item_id": "test_650",
      "prediction": "Round",
      "reference": "Oval",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.5625,
      "nll": 1.71875
    },
    {
      "item_id": "test_651",
      "prediction": "Larynx",
      "reference": "Larynx",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.78125,
      "nll": 1.0234375
    },
    {
      "item_id": "test_652",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.25,
      "nll": 0.80859375
    },
    {
      "item_id": "test_653",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.09375,
      "nll": 0.7421875
    },
    {
      "item_id": "test_654",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1796875,
      "nll": 0.1640625
    },
    {
      "item_id": "test_655",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.5625,
      "nll": 2.53125
    },
    {
      "item_id": "test_656",
      "prediction": "Cervical spine",
      "reference": "Neck",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.21428571428571427,
        "char_recall": 0.5,
        "char_f1": 0.3,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.46875,
      "nll": 1.2421875
    },
    {
      "item_id": "test_657",
      "prediction": "The esophagus is located in the midline of the neck, anterior to the trachea and posterior to the carotid sheath.",
      "reference": "Under the trachea",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26153846153846155,
        "word_overlap": 0.125,
        "bleu_4": 0.01837177448896075,
        "rouge_l": 0.1739130434782609,
        "bertscore_f1": 0.0,
        "char_precision": 0.7079646017699115,
        "char_recall": 0.9411764705882353,
        "char_f1": 0.8080808080808081,
        "word_precision": 0.13333333333333333,
        "word_recall": 0.6666666666666666,
        "word_f1": 0.2222222222222222
      },
      "perplexity": 15.375,
      "nll": 2.734375
    },
    {
      "item_id": "test_658",
      "prediction": "Neurological",
      "reference": "Neck",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.375,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.25,
        "char_recall": 0.75,
        "char_f1": 0.375,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.546875,
      "nll": 1.265625
    },
    {
      "item_id": "test_659",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.390625,
      "nll": 0.328125
    },
    {
      "item_id": "test_660",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.265625,
      "nll": 0.234375
    },
    {
      "item_id": "test_661",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 17.75,
      "nll": 2.875
    },
    {
      "item_id": "test_662",
      "prediction": "Slightly compressed",
      "reference": "Irregular",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21428571428571427,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3157894736842105,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.42857142857142855,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 21.0,
      "nll": 3.046875
    },
    {
      "item_id": "test_663",
      "prediction": "Trachea",
      "reference": "Gas delivery",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21052631578947367,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5714285714285714,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.4210526315789474,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 28288.0,
      "nll": 10.25
    },
    {
      "item_id": "test_664",
      "prediction": "Larynx",
      "reference": "Trachea",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.42857142857142855,
        "char_f1": 0.375,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.15625,
      "nll": 0.765625
    },
    {
      "item_id": "test_665",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.5,
      "nll": 0.91796875
    },
    {
      "item_id": "test_666",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.3125,
      "nll": 0.8359375
    },
    {
      "item_id": "test_667",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.130859375
    },
    {
      "item_id": "test_668",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2109375,
      "nll": 0.189453125
    },
    {
      "item_id": "test_669",
      "prediction": "Neck",
      "reference": "Neck",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.671875,
      "nll": 0.51171875
    },
    {
      "item_id": "test_670",
      "prediction": "White arrow",
      "reference": "Black Hollow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2608695652173913,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.36363636363636365,
        "char_recall": 0.4166666666666667,
        "char_f1": 0.38834951456310685,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1496.0,
      "nll": 7.3125
    },
    {
      "item_id": "test_671",
      "prediction": "Neck",
      "reference": "Neck",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.203125,
      "nll": 1.1640625
    },
    {
      "item_id": "test_672",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.65625,
      "nll": 1.734375
    },
    {
      "item_id": "test_673",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2421875,
      "nll": 0.216796875
    },
    {
      "item_id": "test_674",
      "prediction": "Normal",
      "reference": "Irregular",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.26666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.5263157894736842,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 16.625,
      "nll": 2.8125
    },
    {
      "item_id": "test_675",
      "prediction": "Trachea",
      "reference": "Gas delivery",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21052631578947367,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5714285714285714,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.4210526315789474,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 32000.0,
      "nll": 10.375
    },
    {
      "item_id": "test_676",
      "prediction": "Esophagus",
      "reference": "Esophagus",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.75,
      "nll": 0.55859375
    },
    {
      "item_id": "test_677",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.265625,
      "nll": 0.23828125
    },
    {
      "item_id": "test_678",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.96875,
      "nll": 1.0859375
    },
    {
      "item_id": "test_679",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3046875,
      "nll": 0.265625
    },
    {
      "item_id": "test_680",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.859375,
      "nll": 0.62109375
    },
    {
      "item_id": "test_681",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 38.0,
      "nll": 3.640625
    },
    {
      "item_id": "test_682",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.1279296875
    },
    {
      "item_id": "test_683",
      "prediction": "Right",
      "reference": "Right",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.84375,
      "nll": 1.046875
    },
    {
      "item_id": "test_684",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1280.0,
      "nll": 7.15625
    },
    {
      "item_id": "test_685",
      "prediction": "The image shows a brain MRI scan with a focus on the cerebellum. The cerebellum appears to have a smooth, regular texture, which is typical for a healthy cerebellum. There are no visible signs of abnormalities such as tumors, lesions, or atrophy. Therefore, based on this image alone, there are no diseases present in the cerebellum. However, it is important to note that a comprehensive evaluation of the brain requires a full MRI scan, including T1, T2, and FLAIR sequences, as well as a review of the patient's medical history and clinical symptoms.",
      "reference": "Brain Edema, Brain Enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.010238907849829351,
        "word_overlap": 0.014705882352941176,
        "bleu_4": 0.002157116667939802,
        "rouge_l": 0.04040404040404041,
        "bertscore_f1": 0.0,
        "char_precision": 0.7246376811594203,
        "char_recall": 0.8823529411764706,
        "char_f1": 0.7957559681697612,
        "word_precision": 0.015384615384615385,
        "word_recall": 0.25,
        "word_f1": 0.028985507246376812
      },
      "perplexity": 20.75,
      "nll": 3.03125
    },
    {
      "item_id": "test_686",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.859375,
      "nll": 1.3515625
    },
    {
      "item_id": "test_687",
      "prediction": "The main cause of the disease on the right lobe of the brain in this picture is a stroke.",
      "reference": "Brain embryonic tissue dysplasia, genetic factors, chemical factors",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1794871794871795,
        "word_overlap": 0.047619047619047616,
        "bleu_4": 0.009629943614188135,
        "rouge_l": 0.07407407407407407,
        "bertscore_f1": 0.0,
        "char_precision": 0.9662921348314607,
        "char_recall": 0.9253731343283582,
        "char_f1": 0.9453900709219858,
        "word_precision": 0.06666666666666667,
        "word_recall": 0.14285714285714285,
        "word_f1": 0.09090909090909091
      },
      "perplexity": 70.0,
      "nll": 4.25
    },
    {
      "item_id": "test_688",
      "prediction": "This image was taken using a magnetic resonance imaging (MRI) scan.",
      "reference": "MRI",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.08571428571428572,
        "word_overlap": 0.09090909090909091,
        "bleu_4": 0.01428363257865929,
        "rouge_l": 0.16666666666666669,
        "bertscore_f1": 0.0,
        "char_precision": 0.04477611940298507,
        "char_recall": 1.0,
        "char_f1": 0.08571428571428572,
        "word_precision": 0.09090909090909091,
        "word_recall": 1.0,
        "word_f1": 0.16666666666666669
      },
      "perplexity": 1.265625,
      "nll": 0.2353515625
    },
    {
      "item_id": "test_689",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.328125,
      "nll": 0.84765625
    },
    {
      "item_id": "test_690",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.21875,
      "nll": 1.9765625
    },
    {
      "item_id": "test_691",
      "prediction": "The image shows a sagittal view of the brain, specifically highlighting the corpus callosum. The corpus callosum is the thick band of nerve fibers that connects the two hemispheres of the brain.",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.04040404040404041,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.14948453608247422,
        "char_recall": 0.75,
        "char_f1": 0.24928366762177648,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 880.0,
      "nll": 6.78125
    },
    {
      "item_id": "test_692",
      "prediction": "The brain non-enhancing tumor is located in the left hemisphere.",
      "reference": "Upper Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.22784810126582278,
        "word_overlap": 0.08333333333333333,
        "bleu_4": 0.018850319022747353,
        "rouge_l": 0.14285714285714288,
        "bertscore_f1": 0.0,
        "char_precision": 0.46875,
        "char_recall": 0.8,
        "char_f1": 0.5911330049261083,
        "word_precision": 0.1,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.15384615384615383
      },
      "perplexity": 82.0,
      "nll": 4.40625
    },
    {
      "item_id": "test_693",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 286.0,
      "nll": 5.65625
    },
    {
      "item_id": "test_694",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0625,
      "nll": 0.72265625
    },
    {
      "item_id": "test_695",
      "prediction": "White",
      "reference": "White",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.984375,
      "nll": 1.09375
    },
    {
      "item_id": "test_696",
      "prediction": "The main cause of the disease on the upper left lobe of brain in this picture is a brain tumor.",
      "reference": "Brain embryonic tissue dysplasia, genetic factors, chemical factors",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18518518518518517,
        "word_overlap": 0.045454545454545456,
        "bleu_4": 0.00913442366656447,
        "rouge_l": 0.07142857142857144,
        "bertscore_f1": 0.0,
        "char_precision": 0.9789473684210527,
        "char_recall": 0.9104477611940298,
        "char_f1": 0.9434558456677199,
        "word_precision": 0.0625,
        "word_recall": 0.14285714285714285,
        "word_f1": 0.08695652173913043
      },
      "perplexity": 70.0,
      "nll": 4.25
    },
    {
      "item_id": "test_697",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.03125,
      "nll": 0.70703125
    },
    {
      "item_id": "test_698",
      "prediction": "t2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.140625,
      "nll": 0.7578125
    },
    {
      "item_id": "test_699",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.015625,
      "nll": 1.1015625
    },
    {
      "item_id": "test_700",
      "prediction": "The image shows a sagittal view of the brain, specifically highlighting the lateral ventricles, which are fluid-filled cavities located in the cerebral hemispheres.",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.047619047619047616,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2073170731707317,
        "char_recall": 0.75,
        "char_f1": 0.32484076433121023,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 209.0,
      "nll": 5.34375
    },
    {
      "item_id": "test_701",
      "prediction": "Right",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.0625,
      "nll": 0.72265625
    },
    {
      "item_id": "test_702",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 776.0,
      "nll": 6.65625
    },
    {
      "item_id": "test_703",
      "prediction": "The image shows a brain with a large, irregularly shaped mass on the left side, which appears to be a glioblastoma multiforme (GBM). GBM is a highly aggressive and malignant type of brain tumor that can spread to other parts of the brain. It is typically characterized by a rapid growth rate and a tendency to invade surrounding brain tissue. The presence of this mass suggests that the patient may have a GBM, which is a serious and potentially life-threatening condition.",
      "reference": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.018726591760299626,
        "word_overlap": 0.034482758620689655,
        "bleu_4": 0.003145061935385489,
        "rouge_l": 0.08791208791208792,
        "bertscore_f1": 0.0,
        "char_precision": 0.7230443974630021,
        "char_recall": 0.9508196721311475,
        "char_f1": 0.8214344873281432,
        "word_precision": 0.03636363636363636,
        "word_recall": 0.4,
        "word_f1": 0.06666666666666667
      },
      "perplexity": 9.5,
      "nll": 2.25
    },
    {
      "item_id": "test_704",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9453125,
      "nll": 0.6640625
    },
    {
      "item_id": "test_705",
      "prediction": "2",
      "reference": "3",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.5,
      "nll": 1.703125
    },
    {
      "item_id": "test_706",
      "prediction": "The patient may experience symptoms such as seizures, memory loss, and speech difficulties.",
      "reference": "Visual impairment, vomiting, tinnitus, increased intracranial pressure",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2857142857142857,
        "word_overlap": 0.0,
        "bleu_4": 0.014628063653657535,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8681318681318682,
        "char_recall": 0.9571428571428572,
        "char_f1": 0.9104670164272812,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.9375,
      "nll": 2.296875
    },
    {
      "item_id": "test_707",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.953125,
      "nll": 1.375
    },
    {
      "item_id": "test_708",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7734375,
      "nll": 0.57421875
    },
    {
      "item_id": "test_709",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 23.875,
      "nll": 3.171875
    },
    {
      "item_id": "test_710",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.1162109375
    },
    {
      "item_id": "test_711",
      "prediction": "Left hemisphere",
      "reference": "Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.5333333333333333,
        "char_recall": 0.7777777777777778,
        "char_f1": 0.6327683615819208,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 48.25,
      "nll": 3.875
    },
    {
      "item_id": "test_712",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0234375,
      "nll": 0.0244140625
    },
    {
      "item_id": "test_713",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.046875,
      "nll": 0.71875
    },
    {
      "item_id": "test_714",
      "prediction": "2",
      "reference": "1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.125,
      "nll": 1.6328125
    },
    {
      "item_id": "test_715",
      "prediction": "The most severe disease in this image is stroke. The treatment for stroke depends on the type of stroke and the severity of the symptoms. In general, the treatment for stroke includes:\n\n  * Immediate medical attention: If you suspect that you or someone else is having a stroke, call emergency services immediately.\n  * Medications: Medications may be given to reduce the risk of further damage to the brain and to improve the chances of recovery.\n  * Surgery: In some cases, surgery may be necessary to remove blood clots or to repair damaged blood vessels.\n  * Rehabilitation: After a stroke, rehabilitation may be necessary to help the person recover from the effects of the stroke and to improve their ability to perform daily activities.\n\nIt is important to note that the treatment for stroke should be provided by a healthcare professional, and the specific treatment plan will depend on the individual's medical history and the severity of the stroke.",
      "reference": "Pharmacotherapy, rehabilitation",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0040858018386108275,
        "word_overlap": 0.011494252873563218,
        "bleu_4": 0.002155117641214824,
        "rouge_l": 0.012738853503184712,
        "bertscore_f1": 0.0,
        "char_precision": 0.8154008438818565,
        "char_recall": 0.967741935483871,
        "char_f1": 0.8850638322233461,
        "word_precision": 0.011627906976744186,
        "word_recall": 0.5,
        "word_f1": 0.022727272727272724
      },
      "perplexity": 23.875,
      "nll": 3.171875
    },
    {
      "item_id": "test_716",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 4.0625,
      "nll": 1.3984375
    },
    {
      "item_id": "test_717",
      "prediction": "T2 weighted",
      "reference": "T2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3076923076923077,
        "word_overlap": 0.5,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 0.18181818181818182,
        "char_recall": 1.0,
        "char_f1": 0.3076923076923077,
        "word_precision": 0.5,
        "word_recall": 1.0,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 1.125,
      "nll": 0.1162109375
    },
    {
      "item_id": "test_718",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.65625,
      "nll": 1.5390625
    },
    {
      "item_id": "test_719",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.15625,
      "nll": 0.14453125
    },
    {
      "item_id": "test_720",
      "prediction": "left hemisphere",
      "reference": "Upper Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.631578947368421,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 70.0,
      "nll": 4.25
    },
    {
      "item_id": "test_721",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.0311279296875
    },
    {
      "item_id": "test_722",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.671875,
      "nll": 0.515625
    },
    {
      "item_id": "test_723",
      "prediction": "Hypodense",
      "reference": "Hyperdense",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.8421052631578947,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8888888888888888,
        "char_recall": 0.9,
        "char_f1": 0.8944099378881988,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.703125,
      "nll": 0.99609375
    },
    {
      "item_id": "test_724",
      "prediction": "2",
      "reference": "1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.71875,
      "nll": 1.5546875
    },
    {
      "item_id": "test_725",
      "prediction": "The most severe disease in this image is stroke. The treatment for stroke depends on the type of stroke and the severity of the symptoms. In general, the treatment for stroke includes:\n\n  * Immediate medical attention and evaluation by a healthcare provider\n  * Medications to reduce blood pressure and prevent blood clots\n  * Anticoagulants to prevent blood clots from forming\n  * Thrombolytic therapy to dissolve blood clots\n  * Endovascular treatment to remove blood clots from the brain\n  * Rehabilitation to help the patient recover from the stroke\n\nIt is important to consult with a healthcare provider for the most appropriate treatment plan for the patient.",
      "reference": "Pharmacotherapy, rehabilitation",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.008797653958944282,
        "word_overlap": 0.01818181818181818,
        "bleu_4": 0.001949174943170598,
        "rouge_l": 0.019801980198019806,
        "bertscore_f1": 0.0,
        "char_precision": 0.8341013824884793,
        "char_recall": 0.967741935483871,
        "char_f1": 0.8959656793993895,
        "word_precision": 0.018518518518518517,
        "word_recall": 0.5,
        "word_f1": 0.03571428571428571
      },
      "perplexity": 23.875,
      "nll": 3.171875
    },
    {
      "item_id": "test_726",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.078125,
      "nll": 0.734375
    },
    {
      "item_id": "test_727",
      "prediction": "T2 weighted",
      "reference": "T2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3076923076923077,
        "word_overlap": 0.5,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 0.18181818181818182,
        "char_recall": 1.0,
        "char_f1": 0.3076923076923077,
        "word_precision": 0.5,
        "word_recall": 1.0,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 1.5,
      "nll": 0.404296875
    },
    {
      "item_id": "test_728",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.6875,
      "nll": 1.546875
    },
    {
      "item_id": "test_729",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.03125,
      "nll": 1.6171875
    },
    {
      "item_id": "test_730",
      "prediction": "Left hemisphere",
      "reference": "Upper Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6666666666666666,
        "char_recall": 0.8,
        "char_f1": 0.7272727272727272,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 58.0,
      "nll": 4.0625
    },
    {
      "item_id": "test_731",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 174.0,
      "nll": 5.15625
    },
    {
      "item_id": "test_732",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.921875,
      "nll": 0.65234375
    },
    {
      "item_id": "test_733",
      "prediction": "The patient has a large left frontal lobe glioblastoma multiforme.",
      "reference": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.22666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.9696969696969697,
        "char_recall": 0.8690476190476191,
        "char_f1": 0.9166176182067883,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 28.375,
      "nll": 3.34375
    },
    {
      "item_id": "test_734",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.28125,
      "nll": 0.828125
    },
    {
      "item_id": "test_735",
      "prediction": "t2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9765625,
      "nll": 0.6796875
    },
    {
      "item_id": "test_736",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.11572265625
    },
    {
      "item_id": "test_737",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.1875,
      "nll": 1.6484375
    },
    {
      "item_id": "test_738",
      "prediction": "The brain tumor is located in the left hemisphere of the brain.",
      "reference": "Upper Right Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.20253164556962025,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6984126984126984,
        "char_recall": 0.75,
        "char_f1": 0.7232876712328766,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 54.5,
      "nll": 4.0
    },
    {
      "item_id": "test_739",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.015625,
      "nll": 0.0189208984375
    },
    {
      "item_id": "test_740",
      "prediction": "The image shows a hyperintense lesion in the right hemisphere of the brain, which is consistent with a lesion in the white matter. This could be indicative of a variety of conditions, including multiple sclerosis, stroke, or other white matter diseases.",
      "reference": "Brain Edema, Brain Non-enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.08247422680412371,
        "word_overlap": 0.029411764705882353,
        "bleu_4": 0.004649189828106597,
        "rouge_l": 0.0425531914893617,
        "bertscore_f1": 0.0,
        "char_precision": 0.7430830039525692,
        "char_recall": 0.868421052631579,
        "char_f1": 0.8008778157877752,
        "word_precision": 0.03333333333333333,
        "word_recall": 0.2,
        "word_f1": 0.05714285714285715
      },
      "perplexity": 22.375,
      "nll": 3.109375
    },
    {
      "item_id": "test_741",
      "prediction": "Hyperdense",
      "reference": "Hyperdense",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.109375,
      "nll": 0.74609375
    },
    {
      "item_id": "test_742",
      "prediction": "The patient will have symptoms of a stroke.",
      "reference": "Visual impairment, vomiting, tinnitus, increased intracranial pressure",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2831858407079646,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.813953488372093,
        "char_recall": 0.8428571428571429,
        "char_f1": 0.8281531983156206,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.1875,
      "nll": 2.21875
    },
    {
      "item_id": "test_743",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.21875,
      "nll": 0.1953125
    },
    {
      "item_id": "test_744",
      "prediction": "T2 weighted",
      "reference": "T2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3076923076923077,
        "word_overlap": 0.5,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 0.18181818181818182,
        "char_recall": 1.0,
        "char_f1": 0.3076923076923077,
        "word_precision": 0.5,
        "word_recall": 1.0,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 1.2421875,
      "nll": 0.2197265625
    },
    {
      "item_id": "test_745",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.65625,
      "nll": 1.734375
    },
    {
      "item_id": "test_746",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.091796875
    },
    {
      "item_id": "test_747",
      "prediction": "Right hemisphere",
      "reference": "Upper Right Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4375,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.875,
        "char_recall": 0.75,
        "char_f1": 0.8076923076923077,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 53.0,
      "nll": 3.96875
    },
    {
      "item_id": "test_748",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1128.0,
      "nll": 7.03125
    },
    {
      "item_id": "test_749",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2578125,
      "nll": 0.228515625
    },
    {
      "item_id": "test_750",
      "prediction": "The image shows a brain MRI with a hyperintense lesion in the upper right lobe, which is consistent with a brain abscess. To prevent the occurrence of a brain abscess, it is important to identify and treat the underlying cause of the infection. This may include antibiotics, surgery, or other treatments depending on the cause of the infection. It is also important to maintain good hygiene and avoid contact with infected individuals to prevent the spread of infection.",
      "reference": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.01444043321299639,
        "word_overlap": 0.09090909090909091,
        "bleu_4": 0.00585759207210904,
        "rouge_l": 0.08888888888888889,
        "bertscore_f1": 0.0,
        "char_precision": 0.9659574468085106,
        "char_recall": 0.9880952380952381,
        "char_f1": 0.9769009410727711,
        "word_precision": 0.10416666666666667,
        "word_recall": 0.4166666666666667,
        "word_f1": 0.16666666666666669
      },
      "perplexity": 28.75,
      "nll": 3.359375
    },
    {
      "item_id": "test_751",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2734375,
      "nll": 0.244140625
    },
    {
      "item_id": "test_752",
      "prediction": "T2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.9765625,
      "nll": 0.6796875
    },
    {
      "item_id": "test_753",
      "prediction": "axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.1875,
        "char_f1": 0.2857142857142857,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 19.5,
      "nll": 2.96875
    },
    {
      "item_id": "test_754",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 13.1875,
      "nll": 2.578125
    },
    {
      "item_id": "test_755",
      "prediction": "Left",
      "reference": "Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.25,
        "char_recall": 0.2,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.296875,
      "nll": 1.1953125
    },
    {
      "item_id": "test_756",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0859375,
      "nll": 0.07958984375
    },
    {
      "item_id": "test_757",
      "prediction": "Multiple sclerosis",
      "reference": "Brain Edema, Brain Non-enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.07142857142857142,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.42105263157894735,
        "char_f1": 0.45714285714285713,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 24.25,
      "nll": 3.1875
    },
    {
      "item_id": "test_758",
      "prediction": "No damage",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.75,
        "char_f1": 0.46153846153846156,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.46875,
      "nll": 1.8671875
    },
    {
      "item_id": "test_759",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.03125,
      "nll": 0.70703125
    },
    {
      "item_id": "test_760",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6171875,
      "nll": 0.482421875
    },
    {
      "item_id": "test_761",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.328125,
      "nll": 0.28515625
    },
    {
      "item_id": "test_762",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.07373046875
    },
    {
      "item_id": "test_763",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2421875,
      "nll": 0.2197265625
    },
    {
      "item_id": "test_764",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0234375,
      "nll": 0.0235595703125
    },
    {
      "item_id": "test_765",
      "prediction": "The image shows a brain MRI scan with a focus on the cerebellum. The cerebellum appears to have a large, well-defined, hyperintense lesion on the T2-weighted image, which is consistent with a cerebellar abscess. Cerebellar abscesses are typically caused by bacterial or fungal infections and can lead to symptoms such as headache, fever, and neurological deficits. The surrounding edema and mass effect may also be present.",
      "reference": "Brain Edema",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.03773584905660377,
        "bleu_4": 0.002877533669929391,
        "rouge_l": 0.05714285714285715,
        "bertscore_f1": 0.0,
        "char_precision": 0.5508274231678487,
        "char_recall": 0.8181818181818182,
        "char_f1": 0.6583987441130298,
        "word_precision": 0.03773584905660377,
        "word_recall": 1.0,
        "word_f1": 0.07272727272727272
      },
      "perplexity": 15.1875,
      "nll": 2.71875
    },
    {
      "item_id": "test_766",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.109375,
      "nll": 0.75
    },
    {
      "item_id": "test_767",
      "prediction": "Left hemisphere",
      "reference": "Lower Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23076923076923078,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.7333333333333333,
        "char_recall": 0.6363636363636364,
        "char_f1": 0.6814159292035398,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 776.0,
      "nll": 6.65625
    },
    {
      "item_id": "test_768",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2890625,
      "nll": 0.25390625
    },
    {
      "item_id": "test_769",
      "prediction": "t2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.546875,
      "nll": 1.265625
    },
    {
      "item_id": "test_770",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.3125,
      "nll": 1.4609375
    },
    {
      "item_id": "test_771",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.953125,
      "nll": 1.375
    },
    {
      "item_id": "test_772",
      "prediction": "Left hemisphere",
      "reference": "Upper Right Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25806451612903225,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.6875,
        "char_f1": 0.7394957983193278,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 61.75,
      "nll": 4.125
    },
    {
      "item_id": "test_773",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 278.0,
      "nll": 5.625
    },
    {
      "item_id": "test_774",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.40625,
      "nll": 1.2265625
    },
    {
      "item_id": "test_775",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.890625,
      "nll": 1.359375
    },
    {
      "item_id": "test_776",
      "prediction": "No damage",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.75,
        "char_f1": 0.46153846153846156,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.0,
      "nll": 1.9453125
    },
    {
      "item_id": "test_777",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.5625,
      "nll": 1.515625
    },
    {
      "item_id": "test_778",
      "prediction": "T2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.9375,
      "nll": 0.66015625
    },
    {
      "item_id": "test_779",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.75,
      "nll": 2.375
    },
    {
      "item_id": "test_780",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0546875,
      "nll": 0.05322265625
    },
    {
      "item_id": "test_781",
      "prediction": "left hemisphere",
      "reference": "Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.4666666666666667,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.5072463768115943,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 52.0,
      "nll": 3.953125
    },
    {
      "item_id": "test_782",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 908.0,
      "nll": 6.8125
    },
    {
      "item_id": "test_783",
      "prediction": "The image shows a brain MRI scan with a hyperintense lesion on T2-weighted imaging, which is consistent with a brain abscess.",
      "reference": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2903225806451613,
        "word_overlap": 0.045454545454545456,
        "bleu_4": 0.010900096978029118,
        "rouge_l": 0.1290322580645161,
        "bertscore_f1": 0.0,
        "char_precision": 0.736,
        "char_recall": 0.8688524590163934,
        "char_f1": 0.7969273514750347,
        "word_precision": 0.05555555555555555,
        "word_recall": 0.2,
        "word_f1": 0.08695652173913045
      },
      "perplexity": 9.5,
      "nll": 2.25
    },
    {
      "item_id": "test_784",
      "prediction": "Hyperdense",
      "reference": "Hyperdense",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7734375,
      "nll": 0.57421875
    },
    {
      "item_id": "test_785",
      "prediction": "The patient will have symptoms of a stroke",
      "reference": "Visual impairment, vomiting, tinnitus, increased intracranial pressure",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2857142857142857,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.8428571428571429,
        "char_f1": 0.8380681818181819,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.8125,
      "nll": 2.28125
    },
    {
      "item_id": "test_786",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.59375,
      "nll": 0.953125
    },
    {
      "item_id": "test_787",
      "prediction": "t2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.40625,
      "nll": 0.875
    },
    {
      "item_id": "test_788",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 23.125,
      "nll": 3.140625
    },
    {
      "item_id": "test_789",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 29.625,
      "nll": 3.390625
    },
    {
      "item_id": "test_790",
      "prediction": "Left hemisphere",
      "reference": "Lower Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.7333333333333333,
        "char_f1": 0.6599999999999999,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 93.0,
      "nll": 4.53125
    },
    {
      "item_id": "test_791",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0546875,
      "nll": 0.051025390625
    },
    {
      "item_id": "test_792",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.126953125
    },
    {
      "item_id": "test_793",
      "prediction": "Brain abscess",
      "reference": "Brain embryonic tissue dysplasia, genetic factors, chemical factors",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.225,
        "word_overlap": 0.125,
        "bleu_4": 0.002738826828955665,
        "rouge_l": 0.2,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.6716417910447762,
        "char_f1": 0.8035714285714287,
        "word_precision": 0.5,
        "word_recall": 0.14285714285714285,
        "word_f1": 0.22222222222222224
      },
      "perplexity": 72.5,
      "nll": 4.28125
    },
    {
      "item_id": "test_794",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4140625,
      "nll": 0.345703125
    },
    {
      "item_id": "test_795",
      "prediction": "T2",
      "reference": "T1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.453125,
      "nll": 0.8984375
    },
    {
      "item_id": "test_796",
      "prediction": "axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.1875,
        "char_f1": 0.2857142857142857,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 27.875,
      "nll": 3.328125
    },
    {
      "item_id": "test_797",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.5625,
      "nll": 2.359375
    },
    {
      "item_id": "test_798",
      "prediction": "left hemisphere",
      "reference": "Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.4666666666666667,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.5072463768115943,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 35.25,
      "nll": 3.5625
    },
    {
      "item_id": "test_799",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.0615234375
    },
    {
      "item_id": "test_800",
      "prediction": "The image shows a brain MRI scan with a large, irregularly shaped, hyperintense lesion in the left hemisphere, consistent with a glioblastoma multiforme.",
      "reference": "Brain Edema, Brain Enhancing Tumor, Brain Non-enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2616822429906542,
        "word_overlap": 0.043478260869565216,
        "bleu_4": 0.009186672262412125,
        "rouge_l": 0.0625,
        "bertscore_f1": 0.0,
        "char_precision": 0.7189542483660131,
        "char_recall": 0.8852459016393442,
        "char_f1": 0.7934811648410367,
        "word_precision": 0.05263157894736842,
        "word_recall": 0.2,
        "word_f1": 0.08333333333333333
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_801",
      "prediction": "Isodense",
      "reference": "Hyperdense",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5555555555555556,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.75,
        "char_recall": 0.6,
        "char_f1": 0.6666666666666665,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.28125,
      "nll": 1.984375
    },
    {
      "item_id": "test_802",
      "prediction": "The patient will have symptoms of a left hemisphere stroke.",
      "reference": "Visual impairment, vomiting, tinnitus, increased intracranial pressure",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.29457364341085274,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8135593220338984,
        "char_recall": 0.8428571428571429,
        "char_f1": 0.8279491302441163,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.125,
      "nll": 2.3125
    },
    {
      "item_id": "test_803",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.125,
      "nll": 2.3125
    },
    {
      "item_id": "test_804",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.3125,
      "nll": 1.671875
    },
    {
      "item_id": "test_805",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 24.625,
      "nll": 3.203125
    },
    {
      "item_id": "test_806",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.0771484375
    },
    {
      "item_id": "test_807",
      "prediction": "Left hemisphere",
      "reference": "Lower Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4,
        "word_overlap": 0.25,
        "bleu_4": 0.09069748827745895,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.7333333333333333,
        "char_f1": 0.6599999999999999,
        "word_precision": 0.5,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.4
      },
      "perplexity": 87.0,
      "nll": 4.46875
    },
    {
      "item_id": "test_808",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 260.0,
      "nll": 5.5625
    },
    {
      "item_id": "test_809",
      "prediction": "Cysticercosis",
      "reference": "Brain Edema, Brain Enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0851063829787234,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5384615384615384,
        "char_recall": 0.2647058823529412,
        "char_f1": 0.3549295774647887,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 23.5,
      "nll": 3.15625
    },
    {
      "item_id": "test_810",
      "prediction": "White",
      "reference": "White",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.703125,
      "nll": 0.53125
    },
    {
      "item_id": "test_811",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.78125,
      "nll": 1.0234375
    },
    {
      "item_id": "test_812",
      "prediction": "The patient should avoid smoking and excessive alcohol consumption",
      "reference": "Pay attention to dietary hygiene, strengthen physical fitness and avoid brain trauma",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.32,
        "word_overlap": 0.10526315789473684,
        "bleu_4": 0.018284887072297418,
        "rouge_l": 0.09523809523809525,
        "bertscore_f1": 0.0,
        "char_precision": 0.9545454545454546,
        "char_recall": 0.8571428571428571,
        "char_f1": 0.9032258064516128,
        "word_precision": 0.2222222222222222,
        "word_recall": 0.16666666666666666,
        "word_f1": 0.1904761904761905
      },
      "perplexity": 25.75,
      "nll": 3.25
    },
    {
      "item_id": "test_813",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6484375,
      "nll": 0.5
    },
    {
      "item_id": "test_814",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1953125,
      "nll": 0.17578125
    },
    {
      "item_id": "test_815",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 32.5,
      "nll": 3.484375
    },
    {
      "item_id": "test_816",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.0927734375
    },
    {
      "item_id": "test_817",
      "prediction": "Answer: left hemisphere",
      "reference": "Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.375,
        "word_overlap": 0.25,
        "bleu_4": 0.08034284189446518,
        "rouge_l": 0.4,
        "bertscore_f1": 0.0,
        "char_precision": 0.391304347826087,
        "char_recall": 0.5555555555555556,
        "char_f1": 0.4591836734693878,
        "word_precision": 0.3333333333333333,
        "word_recall": 0.5,
        "word_f1": 0.4
      },
      "perplexity": 53.75,
      "nll": 3.984375
    },
    {
      "item_id": "test_818",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.041015625
    },
    {
      "item_id": "test_819",
      "prediction": "The image shows a brain MRI scan with a large, well-defined, hyperintense lesion in the left hemisphere, consistent with a glioblastoma multiforme.",
      "reference": "Brain Edema, Brain Non-enhancing Tumor",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23783783783783785,
        "word_overlap": 0.043478260869565216,
        "bleu_4": 0.008640609739997756,
        "rouge_l": 0.06896551724137931,
        "bertscore_f1": 0.0,
        "char_precision": 0.7142857142857143,
        "char_recall": 0.8947368421052632,
        "char_f1": 0.794392523364486,
        "word_precision": 0.05263157894736842,
        "word_recall": 0.2,
        "word_f1": 0.08333333333333333
      },
      "perplexity": 24.625,
      "nll": 3.203125
    },
    {
      "item_id": "test_820",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.84375,
      "nll": 1.046875
    },
    {
      "item_id": "test_821",
      "prediction": "No damage",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.75,
        "char_f1": 0.46153846153846156,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.9375,
      "nll": 1.78125
    },
    {
      "item_id": "test_822",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.06298828125
    },
    {
      "item_id": "test_823",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.40625,
      "nll": 0.341796875
    },
    {
      "item_id": "test_824",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.65625,
      "nll": 1.5390625
    },
    {
      "item_id": "test_825",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1484375,
      "nll": 0.138671875
    },
    {
      "item_id": "test_826",
      "prediction": "Left hemisphere",
      "reference": "Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.5333333333333333,
        "char_recall": 0.7777777777777778,
        "char_f1": 0.6327683615819208,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 48.25,
      "nll": 3.875
    },
    {
      "item_id": "test_827",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 223.0,
      "nll": 5.40625
    },
    {
      "item_id": "test_828",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3125,
      "nll": 0.271484375
    },
    {
      "item_id": "test_829",
      "prediction": "No damage",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.75,
        "char_f1": 0.46153846153846156,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.15625,
      "nll": 1.640625
    },
    {
      "item_id": "test_830",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.109375,
      "nll": 0.74609375
    },
    {
      "item_id": "test_831",
      "prediction": "This image is a cross-sectional view of the human brain, specifically a T2-weighted MRI scan.",
      "reference": "Brain",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.10204081632653061,
        "word_overlap": 0.0625,
        "bleu_4": 0.011502783619900045,
        "rouge_l": 0.1111111111111111,
        "bertscore_f1": 0.0,
        "char_precision": 0.24731182795698925,
        "char_recall": 0.8,
        "char_f1": 0.37782340862423003,
        "word_precision": 0.0625,
        "word_recall": 1.0,
        "word_f1": 0.11764705882352941
      },
      "perplexity": 1.6171875,
      "nll": 0.48046875
    },
    {
      "item_id": "test_832",
      "prediction": "The image appears to be a T2-weighted MRI scan.",
      "reference": "T2",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.08163265306122448,
        "word_overlap": 0.1,
        "bleu_4": 0,
        "rouge_l": 0.18181818181818182,
        "bertscore_f1": 0.0,
        "char_precision": 0.06382978723404255,
        "char_recall": 1.0,
        "char_f1": 0.12,
        "word_precision": 0.1,
        "word_recall": 1.0,
        "word_f1": 0.18181818181818182
      },
      "perplexity": 1.59375,
      "nll": 0.46484375
    },
    {
      "item_id": "test_833",
      "prediction": "The main organ in the image is the brain.",
      "reference": "Brain",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.21739130434782608,
        "word_overlap": 0.14285714285714285,
        "bleu_4": 0.021105340631872645,
        "rouge_l": 0.19999999999999998,
        "bertscore_f1": 0.0,
        "char_precision": 0.36585365853658536,
        "char_recall": 0.8,
        "char_f1": 0.5020920502092049,
        "word_precision": 0.14285714285714285,
        "word_recall": 1.0,
        "word_f1": 0.25
      },
      "perplexity": 1.109375,
      "nll": 0.1044921875
    },
    {
      "item_id": "test_834",
      "prediction": "The largest organ in the picture is the brain.",
      "reference": "Brain",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19607843137254902,
        "word_overlap": 0.14285714285714285,
        "bleu_4": 0.021105340631872645,
        "rouge_l": 0.19999999999999998,
        "bertscore_f1": 0.0,
        "char_precision": 0.30434782608695654,
        "char_recall": 0.8,
        "char_f1": 0.44094488188976383,
        "word_precision": 0.14285714285714285,
        "word_recall": 1.0,
        "word_f1": 0.25
      },
      "perplexity": 1.484375,
      "nll": 0.396484375
    },
    {
      "item_id": "test_835",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0546875,
      "nll": 0.0517578125
    },
    {
      "item_id": "test_836",
      "prediction": "The image provided is a brain MRI scan, specifically a T2-weighted image. This type of scan is often used to detect various conditions, including:\n\n1. **Multiple Sclerosis (MS)**: This disease is characterized by the formation of lesions in the brain and spinal cord, which can appear as hyperintense (bright) areas on T2-weighted images.\n2. **Brain Tumors**: Tumors can appear as abnormal masses on MRI scans, often showing hyperintense areas on T2-weighted images.\n3. **Stroke**: Acute strokes can show hyperintense areas on T2-weighted images, indicating edema or hemorrhage.\n4. **Infections**: Infections such as meningitis or encephalitis can cause hyperintense areas on T2-weighted images.\n5. **Vascular Diseases**: Conditions like cerebral venous sinus thrombosis can show hyperintense areas on T2-weighted images.\n\nWithout additional clinical context or a series of images, it's not possible to definitively diagnose any of these conditions from a single T2-weighted MRI scan. A comprehensive clinical evaluation and possibly additional imaging or tests would be necessary to determine the exact nature of any abnormalities seen in the image.",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.21602787456445993,
        "char_recall": 0.75,
        "char_f1": 0.33543733092876465,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 940.0,
      "nll": 6.84375
    },
    {
      "item_id": "test_837",
      "prediction": "Yes, the brain appears to have an abnormality.",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.08333333333333333,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.043478260869565216,
        "char_recall": 0.5,
        "char_f1": 0.08,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.9140625,
      "nll": 0.6484375
    },
    {
      "item_id": "test_838",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.1259765625
    },
    {
      "item_id": "test_839",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9609375,
      "nll": 0.671875
    },
    {
      "item_id": "test_840",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.09228515625
    },
    {
      "item_id": "test_841",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.625,
      "nll": 1.53125
    },
    {
      "item_id": "test_842",
      "prediction": "The abnormalities are in the left hemisphere of the brain.",
      "reference": "Lower Left Lobe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3013698630136986,
        "word_overlap": 0.1,
        "bleu_4": 0.018850319022747353,
        "rouge_l": 0.15384615384615383,
        "bertscore_f1": 0.0,
        "char_precision": 0.5517241379310345,
        "char_recall": 0.7333333333333333,
        "char_f1": 0.629695885509839,
        "word_precision": 0.125,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.18181818181818182
      },
      "perplexity": 158.0,
      "nll": 5.0625
    },
    {
      "item_id": "test_843",
      "prediction": "Brain",
      "reference": "Head",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1168.0,
      "nll": 7.0625
    },
    {
      "item_id": "test_844",
      "prediction": "The image shows a brain MRI scan with a focus on the left hemisphere. The left hemisphere appears to have a large area of abnormal signal intensity, which is indicative of a lesion or abnormality. This could be consistent with a variety of conditions, including stroke, multiple sclerosis, or other neurological disorders. However, without more context or additional imaging, it is not possible to definitively diagnose the condition. It is important to consult with a neurologist or radiologist for a proper evaluation and diagnosis.",
      "reference": "Brain Edema",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.022018348623853212,
        "word_overlap": 0.016129032258064516,
        "bleu_4": 0.001902039103801027,
        "rouge_l": 0.023255813953488372,
        "bertscore_f1": 0.0,
        "char_precision": 0.5393258426966292,
        "char_recall": 0.8181818181818182,
        "char_f1": 0.6501128668171557,
        "word_precision": 0.01639344262295082,
        "word_recall": 0.5,
        "word_f1": 0.031746031746031744
      },
      "perplexity": 19.75,
      "nll": 2.984375
    },
    {
      "item_id": "test_845",
      "prediction": "2",
      "reference": "1",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.4375,
      "nll": 1.6953125
    },
    {
      "item_id": "test_846",
      "prediction": "No damage",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.46153846153846156,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.75,
        "char_f1": 0.46153846153846156,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.21875,
      "nll": 1.828125
    },
    {
      "item_id": "test_847",
      "prediction": "Right kidney",
      "reference": "Symmetrical to the bottom spine",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.27906976744186046,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6666666666666666,
        "char_recall": 0.5483870967741935,
        "char_f1": 0.6017699115044248,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 708.0,
      "nll": 6.5625
    },
    {
      "item_id": "test_848",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 3.15625,
      "nll": 1.1484375
    },
    {
      "item_id": "test_849",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.046875,
      "nll": 0.71484375
    },
    {
      "item_id": "test_850",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.171875,
      "nll": 0.1591796875
    },
    {
      "item_id": "test_851",
      "prediction": "black",
      "reference": "Gray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.71875,
      "nll": 1.90625
    },
    {
      "item_id": "test_852",
      "prediction": "Liver",
      "reference": "Right Kidney",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11764705882352941,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.25,
        "char_f1": 0.3076923076923077,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.71875,
      "nll": 1.3125
    },
    {
      "item_id": "test_853",
      "prediction": "2",
      "reference": "6",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 19.125,
      "nll": 2.953125
    },
    {
      "item_id": "test_854",
      "prediction": "No damage",
      "reference": "Much",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12544.0,
      "nll": 9.4375
    },
    {
      "item_id": "test_855",
      "prediction": "Spleen",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7421875,
      "nll": 0.5546875
    },
    {
      "item_id": "test_856",
      "prediction": "Kidney",
      "reference": "Kidney",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8359375,
      "nll": 0.609375
    },
    {
      "item_id": "test_857",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2421875,
      "nll": 0.2158203125
    },
    {
      "item_id": "test_858",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4921875,
      "nll": 0.400390625
    },
    {
      "item_id": "test_859",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 14.25,
      "nll": 2.65625
    },
    {
      "item_id": "test_860",
      "prediction": "Right side of the image",
      "reference": "Upper Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.29411764705882354,
        "word_overlap": 0.16666666666666666,
        "bleu_4": 0.05372849659117709,
        "rouge_l": 0.28571428571428575,
        "bertscore_f1": 0.0,
        "char_precision": 0.7391304347826086,
        "char_recall": 0.6363636363636364,
        "char_f1": 0.6839080459770114,
        "word_precision": 0.2,
        "word_recall": 0.5,
        "word_f1": 0.28571428571428575
      },
      "perplexity": 168.0,
      "nll": 5.125
    },
    {
      "item_id": "test_861",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0390625,
      "nll": 0.041748046875
    },
    {
      "item_id": "test_862",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.375,
      "nll": 0.318359375
    },
    {
      "item_id": "test_863",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.10302734375
    },
    {
      "item_id": "test_864",
      "prediction": "Liver",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.875,
      "nll": 1.0546875
    },
    {
      "item_id": "test_865",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.875,
      "nll": 1.0546875
    },
    {
      "item_id": "test_866",
      "prediction": "Liver",
      "reference": "Biotransformation, detoxification",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.10526315789473684,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.24242424242424243,
        "char_f1": 0.3453237410071942,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 102.0,
      "nll": 4.625
    },
    {
      "item_id": "test_867",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 6.0625,
      "nll": 1.8046875
    },
    {
      "item_id": "test_868",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.78125,
      "nll": 1.328125
    },
    {
      "item_id": "test_869",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1953125,
      "nll": 0.1806640625
    },
    {
      "item_id": "test_870",
      "prediction": "Liver",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.16666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.875,
      "nll": 0.62890625
    },
    {
      "item_id": "test_871",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.09228515625
    },
    {
      "item_id": "test_872",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.625,
      "nll": 1.7265625
    },
    {
      "item_id": "test_873",
      "prediction": "Liver",
      "reference": "Colon",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.53125,
      "nll": 1.875
    },
    {
      "item_id": "test_874",
      "prediction": "1",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 37.0,
      "nll": 3.609375
    },
    {
      "item_id": "test_875",
      "prediction": "Liver",
      "reference": "Breathe",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.42857142857142855,
        "char_f1": 0.4137931034482759,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 66.0,
      "nll": 4.1875
    },
    {
      "item_id": "test_876",
      "prediction": "Liver",
      "reference": "Stomach, Colon",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.10526315789473684,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 10.5625,
      "nll": 2.359375
    },
    {
      "item_id": "test_877",
      "prediction": "Kidney",
      "reference": "Kidney",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.125,
      "nll": 0.75390625
    },
    {
      "item_id": "test_878",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1015625,
      "nll": 0.09716796875
    },
    {
      "item_id": "test_879",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5703125,
      "nll": 0.44921875
    },
    {
      "item_id": "test_880",
      "prediction": "Liver",
      "reference": "Small Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.09090909090909091,
        "char_f1": 0.12500000000000003,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.375,
      "nll": 2.0
    },
    {
      "item_id": "test_881",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.5,
      "nll": 2.140625
    },
    {
      "item_id": "test_882",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.06396484375
    },
    {
      "item_id": "test_883",
      "prediction": "Small bowel",
      "reference": "Kidney",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11764705882352941,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.09090909090909091,
        "char_recall": 0.16666666666666666,
        "char_f1": 0.11764705882352942,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.765625,
      "nll": 0.56640625
    },
    {
      "item_id": "test_884",
      "prediction": "1",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.125,
      "nll": 1.4140625
    },
    {
      "item_id": "test_885",
      "prediction": "Liver",
      "reference": "Absorb nutrients, secrete enzymes, digest food",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0392156862745098,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.2608695652173913,
        "char_f1": 0.36363636363636365,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 123.0,
      "nll": 4.8125
    },
    {
      "item_id": "test_886",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.515625,
      "nll": 1.2578125
    },
    {
      "item_id": "test_887",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6328125,
      "nll": 0.4921875
    },
    {
      "item_id": "test_888",
      "prediction": "Right",
      "reference": "Symmetrical to the bottom spine",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.25806451612903225,
        "char_f1": 0.3609022556390977,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1032.0,
      "nll": 6.9375
    },
    {
      "item_id": "test_889",
      "prediction": "Liver",
      "reference": "Small Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.09090909090909091,
        "char_f1": 0.12500000000000003,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 12.5625,
      "nll": 2.53125
    },
    {
      "item_id": "test_890",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.515625,
      "nll": 0.41796875
    },
    {
      "item_id": "test_891",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.03369140625
    },
    {
      "item_id": "test_892",
      "prediction": "black",
      "reference": "Gray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.5,
      "nll": 2.015625
    },
    {
      "item_id": "test_893",
      "prediction": "Right kidney",
      "reference": "Liver",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23529411764705882,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.25,
        "char_recall": 0.4,
        "char_f1": 0.3076923076923077,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.015625,
      "nll": 0.703125
    },
    {
      "item_id": "test_894",
      "prediction": "3",
      "reference": "5",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.5625,
      "nll": 1.8828125
    },
    {
      "item_id": "test_895",
      "prediction": "Round",
      "reference": "Circular",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3076923076923077,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.125,
        "char_f1": 0.15384615384615385,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 17.125,
      "nll": 2.84375
    },
    {
      "item_id": "test_896",
      "prediction": "Kidney",
      "reference": "Absorb nutrients, secrete enzymes, digest food",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11538461538461539,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.32608695652173914,
        "char_f1": 0.46875000000000006,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 144.0,
      "nll": 4.96875
    },
    {
      "item_id": "test_897",
      "prediction": "Spinal cord",
      "reference": "Spinal cord",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.316227766016838,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.515625,
      "nll": 0.416015625
    },
    {
      "item_id": "test_898",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.015625,
      "nll": 0.703125
    },
    {
      "item_id": "test_899",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4296875,
      "nll": 0.357421875
    },
    {
      "item_id": "test_900",
      "prediction": "The liver is located in the upper right quadrant of the abdomen.",
      "reference": "Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11764705882352941,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.203125,
        "char_recall": 0.75,
        "char_f1": 0.319672131147541,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 11.625,
      "nll": 2.453125
    },
    {
      "item_id": "test_901",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.028564453125
    },
    {
      "item_id": "test_902",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.5625,
      "nll": 1.515625
    },
    {
      "item_id": "test_903",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0703125,
      "nll": 0.0654296875
    },
    {
      "item_id": "test_904",
      "prediction": "White",
      "reference": "Gray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.28125,
      "nll": 1.984375
    },
    {
      "item_id": "test_905",
      "prediction": "Spleen",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2265625,
      "nll": 0.2041015625
    },
    {
      "item_id": "test_906",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.09375,
      "nll": 1.625
    },
    {
      "item_id": "test_907",
      "prediction": "Splenomegaly",
      "reference": "Oval",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.375,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.25,
        "char_recall": 0.5,
        "char_f1": 0.3333333333333333,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.875,
      "nll": 1.9296875
    },
    {
      "item_id": "test_908",
      "prediction": "Kidney",
      "reference": "Absorb nutrients, secrete enzymes, digest food",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11538461538461539,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.32608695652173914,
        "char_f1": 0.46875000000000006,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 25.75,
      "nll": 3.25
    },
    {
      "item_id": "test_909",
      "prediction": "Spinal cord",
      "reference": "Spinal cord",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.316227766016838,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3046875,
      "nll": 0.263671875
    },
    {
      "item_id": "test_910",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.130859375
    },
    {
      "item_id": "test_911",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.328125,
      "nll": 0.84765625
    },
    {
      "item_id": "test_912",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1015625,
      "nll": 0.09619140625
    },
    {
      "item_id": "test_913",
      "prediction": "The stomach is located in the upper left quadrant of the image.",
      "reference": "Upper Right",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.24324324324324326,
        "word_overlap": 0.09090909090909091,
        "bleu_4": 0.01553712569276035,
        "rouge_l": 0.14285714285714285,
        "bertscore_f1": 0.0,
        "char_precision": 0.5714285714285714,
        "char_recall": 0.8181818181818182,
        "char_f1": 0.6728971962616823,
        "word_precision": 0.1,
        "word_recall": 0.5,
        "word_f1": 0.16666666666666669
      },
      "perplexity": 123.0,
      "nll": 4.8125
    },
    {
      "item_id": "test_914",
      "prediction": "Abdomen",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.421875,
      "nll": 0.353515625
    },
    {
      "item_id": "test_915",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.0284423828125
    },
    {
      "item_id": "test_916",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.09228515625
    },
    {
      "item_id": "test_917",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5703125,
      "nll": 0.44921875
    },
    {
      "item_id": "test_918",
      "prediction": "1",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 37.5,
      "nll": 3.625
    },
    {
      "item_id": "test_919",
      "prediction": "Round",
      "reference": "Circular",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3076923076923077,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.125,
        "char_f1": 0.15384615384615385,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 26.25,
      "nll": 3.265625
    },
    {
      "item_id": "test_920",
      "prediction": "Liver",
      "reference": "Improve the body's immunity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1875,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8,
        "char_recall": 0.2222222222222222,
        "char_f1": 0.3478260869565218,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 108.5,
      "nll": 4.6875
    },
    {
      "item_id": "test_921",
      "prediction": "Spleen",
      "reference": "Spleen",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.15625,
      "nll": 0.765625
    },
    {
      "item_id": "test_922",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6328125,
      "nll": 0.48828125
    },
    {
      "item_id": "test_923",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.9140625,
      "nll": 0.6484375
    },
    {
      "item_id": "test_924",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 11.4375,
      "nll": 2.4375
    },
    {
      "item_id": "test_925",
      "prediction": "Esophagus is located in the left side of the image.",
      "reference": "Center",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.10526315789473684,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.21568627450980393,
        "char_recall": 0.6666666666666666,
        "char_f1": 0.3259259259259259,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 174.0,
      "nll": 5.15625
    },
    {
      "item_id": "test_926",
      "prediction": "Liver",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.16666666666666666,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.16666666666666666,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.734375,
      "nll": 0.55078125
    },
    {
      "item_id": "test_927",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.59375,
      "nll": 0.466796875
    },
    {
      "item_id": "test_928",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.15625,
      "nll": 1.1484375
    },
    {
      "item_id": "test_929",
      "prediction": "black",
      "reference": "Gray",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.25,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.0625,
      "nll": 1.8046875
    },
    {
      "item_id": "test_930",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5703125,
      "nll": 0.453125
    },
    {
      "item_id": "test_931",
      "prediction": "1",
      "reference": "1",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.234375,
      "nll": 0.2099609375
    },
    {
      "item_id": "test_932",
      "prediction": "No damage",
      "reference": "Much",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8640.0,
      "nll": 9.0625
    },
    {
      "item_id": "test_933",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.7421875,
      "nll": 0.5546875
    },
    {
      "item_id": "test_934",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.359375,
      "nll": 0.85546875
    },
    {
      "item_id": "test_935",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.53125,
      "nll": 0.42578125
    },
    {
      "item_id": "test_936",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3671875,
      "nll": 0.3125
    },
    {
      "item_id": "test_937",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.08935546875
    },
    {
      "item_id": "test_938",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2265625,
      "nll": 0.203125
    },
    {
      "item_id": "test_939",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.03125,
      "nll": 0.0299072265625
    },
    {
      "item_id": "test_940",
      "prediction": "The image shows a brain MRI scan with a large area of abnormal signal intensity in the left hemisphere, consistent with a stroke. The patient may have had a stroke, which is a medical emergency that requires immediate attention.",
      "reference": "Brain Tumor, Brain Edema",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.07142857142857142,
        "word_overlap": 0.030303030303030304,
        "bleu_4": 0.005097615802300061,
        "rouge_l": 0.04651162790697674,
        "bertscore_f1": 0.0,
        "char_precision": 0.6535087719298246,
        "char_recall": 0.875,
        "char_f1": 0.748206599713056,
        "word_precision": 0.03225806451612903,
        "word_recall": 0.3333333333333333,
        "word_f1": 0.0588235294117647
      },
      "perplexity": 10.5625,
      "nll": 2.359375
    },
    {
      "item_id": "test_941",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9453125,
      "nll": 0.6640625
    },
    {
      "item_id": "test_942",
      "prediction": "Left hemisphere",
      "reference": "Lower Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.32,
        "word_overlap": 0.3333333333333333,
        "bleu_4": 0.1495348781221221,
        "rouge_l": 0.5,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.8,
        "char_f1": 0.6857142857142857,
        "word_precision": 0.5,
        "word_recall": 0.5,
        "word_f1": 0.5
      },
      "perplexity": 568.0,
      "nll": 6.34375
    },
    {
      "item_id": "test_943",
      "prediction": "Thorax",
      "reference": "Abdomen",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.14285714285714285,
        "char_f1": 0.15384615384615383,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.40625,
      "nll": 0.87890625
    },
    {
      "item_id": "test_944",
      "prediction": "Upper left",
      "reference": "Top",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.3333333333333333,
        "char_f1": 0.25,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.625,
      "nll": 2.03125
    },
    {
      "item_id": "test_945",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.28125,
      "nll": 1.1875
    },
    {
      "item_id": "test_946",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8125,
      "nll": 0.59375
    },
    {
      "item_id": "test_947",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.5625,
      "nll": 1.515625
    },
    {
      "item_id": "test_948",
      "prediction": "Liver",
      "reference": "Esophagus",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14285714285714285,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.7578125,
      "nll": 0.5625
    },
    {
      "item_id": "test_949",
      "prediction": "2",
      "reference": "2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.65625,
      "nll": 0.50390625
    },
    {
      "item_id": "test_950",
      "prediction": "No damage",
      "reference": "Much",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.15384615384615385,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 14208.0,
      "nll": 9.5625
    },
    {
      "item_id": "test_951",
      "prediction": "Liver",
      "reference": "Esophagus",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.14285714285714285,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.25,
      "nll": 1.1796875
    },
    {
      "item_id": "test_952",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.8203125,
      "nll": 0.59765625
    },
    {
      "item_id": "test_953",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5546875,
      "nll": 0.443359375
    },
    {
      "item_id": "test_954",
      "prediction": "Axial",
      "reference": "Transverse Plane",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19047619047619047,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4,
        "char_recall": 0.1875,
        "char_f1": 0.25531914893617025,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 9.9375,
      "nll": 2.296875
    },
    {
      "item_id": "test_955",
      "prediction": "Lung",
      "reference": "Right Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5714285714285714,
        "word_overlap": 0.5,
        "bleu_4": 0.06541924356118012,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.5,
        "char_f1": 0.6666666666666666,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 51.25,
      "nll": 3.9375
    },
    {
      "item_id": "test_956",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 18.875,
      "nll": 2.9375
    },
    {
      "item_id": "test_957",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.10498046875
    },
    {
      "item_id": "test_958",
      "prediction": "White",
      "reference": "Black",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.53125,
      "nll": 1.875
    },
    {
      "item_id": "test_959",
      "prediction": "Heart",
      "reference": "Esophagus",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2857142857142857,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.1111111111111111,
        "char_f1": 0.14285714285714285,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.8359375,
      "nll": 0.60546875
    },
    {
      "item_id": "test_960",
      "prediction": "2",
      "reference": "4",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 19.125,
      "nll": 2.953125
    },
    {
      "item_id": "test_961",
      "prediction": "Lung",
      "reference": "Promote blood flow",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.09090909090909091,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 139.0,
      "nll": 4.9375
    },
    {
      "item_id": "test_962",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.71875,
      "nll": 0.54296875
    },
    {
      "item_id": "test_963",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.125,
      "nll": 0.12060546875
    },
    {
      "item_id": "test_964",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4296875,
      "nll": 0.35546875
    },
    {
      "item_id": "test_965",
      "prediction": "Pelvis",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5263157894736842,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.5384615384615384,
        "char_f1": 0.6542056074766356,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 41.25,
      "nll": 3.71875
    },
    {
      "item_id": "test_966",
      "prediction": "The rectum is existing in the center of the image.",
      "reference": "Bottom",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.10714285714285714,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16,
        "char_recall": 0.8333333333333334,
        "char_f1": 0.2684563758389262,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 404.0,
      "nll": 6.0
    },
    {
      "item_id": "test_967",
      "prediction": "Pelvis",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5263157894736842,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.5384615384615384,
        "char_f1": 0.6542056074766356,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 17.125,
      "nll": 2.84375
    },
    {
      "item_id": "test_968",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3984375,
      "nll": 0.333984375
    },
    {
      "item_id": "test_969",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1015625,
      "nll": 0.09619140625
    },
    {
      "item_id": "test_970",
      "prediction": "Rectum",
      "reference": "Rectum",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6171875,
      "nll": 0.48046875
    },
    {
      "item_id": "test_971",
      "prediction": "2",
      "reference": "0",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 237.0,
      "nll": 5.46875
    },
    {
      "item_id": "test_972",
      "prediction": "Rectum",
      "reference": "Store feces, excrete feces",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.5,
        "char_f1": 0.5,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 23.5,
      "nll": 3.15625
    },
    {
      "item_id": "test_973",
      "prediction": "Rectum",
      "reference": "Rectum",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.234375,
      "nll": 0.8046875
    },
    {
      "item_id": "test_974",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 6.1875,
      "nll": 1.8203125
    },
    {
      "item_id": "test_975",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.15625,
      "nll": 0.14453125
    },
    {
      "item_id": "test_976",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.375,
      "nll": 0.3203125
    },
    {
      "item_id": "test_977",
      "prediction": "Pelvis",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5263157894736842,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.5384615384615384,
        "char_f1": 0.6542056074766356,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 17.5,
      "nll": 2.859375
    },
    {
      "item_id": "test_978",
      "prediction": "Rectum is located in the center of the image.",
      "reference": "Bottom",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.19607843137254902,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.8333333333333334,
        "char_f1": 0.3225806451612903,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 486.0,
      "nll": 6.1875
    },
    {
      "item_id": "test_979",
      "prediction": "Pelvis",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5263157894736842,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.5384615384615384,
        "char_f1": 0.6542056074766356,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 33.0,
      "nll": 3.5
    },
    {
      "item_id": "test_980",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2890625,
      "nll": 0.25390625
    },
    {
      "item_id": "test_981",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2890625,
      "nll": 0.25390625
    },
    {
      "item_id": "test_982",
      "prediction": "Bladder",
      "reference": "Bladder",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.5,
      "nll": 0.9140625
    },
    {
      "item_id": "test_983",
      "prediction": "Urinary bladder",
      "reference": "Store urine",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.38461538461538464,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.4666666666666667,
        "char_recall": 0.6363636363636364,
        "char_f1": 0.5384615384615385,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 11072.0,
      "nll": 9.3125
    },
    {
      "item_id": "test_984",
      "prediction": "Rectum",
      "reference": "Rectum",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.6796875,
      "nll": 0.51953125
    },
    {
      "item_id": "test_985",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.140625,
      "nll": 0.1337890625
    },
    {
      "item_id": "test_986",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.515625,
      "nll": 1.2578125
    },
    {
      "item_id": "test_987",
      "prediction": "Abdomen",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.14285714285714285,
        "char_recall": 0.07692307692307693,
        "char_f1": 0.1,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 68.0,
      "nll": 4.21875
    },
    {
      "item_id": "test_988",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.09375,
      "nll": 0.09228515625
    },
    {
      "item_id": "test_989",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2265625,
      "nll": 0.2041015625
    },
    {
      "item_id": "test_990",
      "prediction": "Kidney",
      "reference": "Bladder",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.3076923076923077,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.42857142857142855,
        "char_f1": 0.375,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.984375,
      "nll": 1.3828125
    },
    {
      "item_id": "test_991",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.953125,
      "nll": 0.66796875
    },
    {
      "item_id": "test_992",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.875,
      "nll": 1.0546875
    },
    {
      "item_id": "test_993",
      "prediction": "Pelvis",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5263157894736842,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.8333333333333334,
        "char_recall": 0.5384615384615384,
        "char_f1": 0.6542056074766356,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 168.0,
      "nll": 5.125
    },
    {
      "item_id": "test_994",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.453125,
      "nll": 0.89453125
    },
    {
      "item_id": "test_995",
      "prediction": "Colon",
      "reference": "Small Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.25,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.4528301886792453,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.671875,
      "nll": 0.98046875
    },
    {
      "item_id": "test_996",
      "prediction": "Rectum",
      "reference": "Digest food, absorb water, excrete body waste",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11764705882352941,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.24444444444444444,
        "char_f1": 0.3283582089552239,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 93.0,
      "nll": 4.53125
    },
    {
      "item_id": "test_997",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.234375,
      "nll": 0.80078125
    },
    {
      "item_id": "test_998",
      "prediction": "Gastrointestinal",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13793103448275862,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5625,
        "char_recall": 0.46153846153846156,
        "char_f1": 0.5070422535211268,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 123.0,
      "nll": 4.8125
    },
    {
      "item_id": "test_999",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.171875,
      "nll": 0.15625
    },
    {
      "item_id": "test_1000",
      "prediction": "Colon",
      "reference": "Colon",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.140625,
      "nll": 0.76171875
    },
    {
      "item_id": "test_1001",
      "prediction": "Bladder",
      "reference": "Absorb nutrients, secrete enzymes, digest food",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.07547169811320754,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5714285714285714,
        "char_recall": 0.2608695652173913,
        "char_f1": 0.3582089552238805,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 58.0,
      "nll": 4.0625
    },
    {
      "item_id": "test_1002",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.0771484375
    },
    {
      "item_id": "test_1003",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1796875,
      "nll": 0.1630859375
    },
    {
      "item_id": "test_1004",
      "prediction": "Abdomen",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.1,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.14285714285714285,
        "char_recall": 0.07692307692307693,
        "char_f1": 0.1,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 29.625,
      "nll": 3.390625
    },
    {
      "item_id": "test_1005",
      "prediction": "Colon is in the left side of the image.",
      "reference": "Upper Left",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.24489795918367346,
        "word_overlap": 0.1111111111111111,
        "bleu_4": 0.021105340631872645,
        "rouge_l": 0.1818181818181818,
        "bertscore_f1": 0.0,
        "char_precision": 0.46153846153846156,
        "char_recall": 0.5,
        "char_f1": 0.48000000000000004,
        "word_precision": 0.125,
        "word_recall": 0.5,
        "word_f1": 0.2
      },
      "perplexity": 203.0,
      "nll": 5.3125
    },
    {
      "item_id": "test_1006",
      "prediction": "Gastrointestinal",
      "reference": "Pelvic Cavity",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.13793103448275862,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5625,
        "char_recall": 0.46153846153846156,
        "char_f1": 0.5070422535211268,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 197.0,
      "nll": 5.28125
    },
    {
      "item_id": "test_1007",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.75,
      "nll": 2.171875
    },
    {
      "item_id": "test_1008",
      "prediction": "Kidney",
      "reference": "Digest food, absorb water, excrete body waste",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.11764705882352941,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.6666666666666666,
        "char_recall": 0.2222222222222222,
        "char_f1": 0.3333333333333333,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 174.0,
      "nll": 5.15625
    },
    {
      "item_id": "test_1009",
      "prediction": "Liver",
      "reference": "Colon, Small Bowel",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.17391304347826086,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2,
        "char_recall": 0.05555555555555555,
        "char_f1": 0.08695652173913045,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 16.875,
      "nll": 2.828125
    },
    {
      "item_id": "test_1010",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.75,
      "nll": 2.171875
    },
    {
      "item_id": "test_1011",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9921875,
      "nll": 0.6875
    },
    {
      "item_id": "test_1012",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.46875,
      "nll": 0.38671875
    },
    {
      "item_id": "test_1013",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.3828125,
      "nll": 0.32421875
    },
    {
      "item_id": "test_1014",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.07177734375
    },
    {
      "item_id": "test_1015",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.2421875,
      "nll": 0.21875
    },
    {
      "item_id": "test_1016",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0234375,
      "nll": 0.0263671875
    },
    {
      "item_id": "test_1017",
      "prediction": "The image shows a brain MRI scan with a focus on the cerebellum. The cerebellum appears to have a normal structure and size, with no visible signs of atrophy or mass lesions. However, without additional clinical information and a full MRI examination, it is not possible to definitively diagnose any specific disease. The cerebellum is part of the brain responsible for coordinating voluntary movements, maintaining posture and balance, and regulating muscle tone and movement. Any abnormalities in the cerebellum could be indicative of conditions such as cerebellar ataxia, cerebellar tumors, or other neurological disorders. For a precise diagnosis, a comprehensive evaluation by a neurologist or neurosurgeon is recommended.",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.23383768913342504,
        "char_recall": 0.75,
        "char_f1": 0.3565186997553303,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1544.0,
      "nll": 7.34375
    },
    {
      "item_id": "test_1018",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.578125,
      "nll": 1.2734375
    },
    {
      "item_id": "test_1019",
      "prediction": "MRI",
      "reference": "MRI",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.609375,
      "nll": 0.95703125
    },
    {
      "item_id": "test_1020",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.453125,
      "nll": 0.375
    },
    {
      "item_id": "test_1021",
      "prediction": "T2",
      "reference": "T2",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.390625,
      "nll": 0.328125
    },
    {
      "item_id": "test_1022",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.109375,
      "nll": 0.10498046875
    },
    {
      "item_id": "test_1023",
      "prediction": "Brain",
      "reference": "Brain",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.34375,
      "nll": 0.294921875
    },
    {
      "item_id": "test_1024",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0234375,
      "nll": 0.024169921875
    },
    {
      "item_id": "test_1025",
      "prediction": "The picture shows a brain with a large area of abnormal signal intensity in the left hemisphere, consistent with a large area of infarction.",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.05555555555555555,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.20714285714285716,
        "char_recall": 0.75,
        "char_f1": 0.3246268656716418,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 502.0,
      "nll": 6.21875
    },
    {
      "item_id": "test_1026",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.234375,
      "nll": 0.8046875
    },
    {
      "item_id": "test_1027",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5390625,
      "nll": 0.4296875
    },
    {
      "item_id": "test_1028",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.359375,
      "nll": 0.85546875
    },
    {
      "item_id": "test_1029",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.5390625,
      "nll": 0.4296875
    },
    {
      "item_id": "test_1030",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.890625,
      "nll": 0.63671875
    },
    {
      "item_id": "test_1031",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.078125,
      "nll": 0.07763671875
    },
    {
      "item_id": "test_1032",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.057861328125
    },
    {
      "item_id": "test_1033",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.484375,
      "nll": 0.91015625
    },
    {
      "item_id": "test_1034",
      "prediction": "Lung cancer",
      "reference": "None",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.4,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.2727272727272727,
        "char_recall": 0.5,
        "char_f1": 0.3529411764705882,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 828.0,
      "nll": 6.71875
    },
    {
      "item_id": "test_1035",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.890625,
      "nll": 1.0625
    },
    {
      "item_id": "test_1036",
      "prediction": "Heart",
      "reference": "Lung",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 1.796875,
      "nll": 0.5859375
    },
    {
      "item_id": "test_1037",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.4375,
      "nll": 0.365234375
    },
    {
      "item_id": "test_1038",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.359375,
      "nll": 0.85546875
    },
    {
      "item_id": "test_1039",
      "prediction": "Lung",
      "reference": "Liver",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.25,
        "char_recall": 0.2,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.125,
      "nll": 1.6328125
    },
    {
      "item_id": "test_1040",
      "prediction": "Lung",
      "reference": "Liver",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.2222222222222222,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.25,
        "char_recall": 0.2,
        "char_f1": 0.22222222222222224,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 4.5625,
      "nll": 1.515625
    },
    {
      "item_id": "test_1041",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.4375,
      "nll": 1.234375
    },
    {
      "item_id": "test_1042",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.03125,
      "nll": 0.7109375
    },
    {
      "item_id": "test_1043",
      "prediction": "Yes",
      "reference": "No",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.328125,
      "nll": 0.84375
    },
    {
      "item_id": "test_1044",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.71875,
      "nll": 1.0
    },
    {
      "item_id": "test_1045",
      "prediction": "Liver",
      "reference": "Liver",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.84375,
      "nll": 0.61328125
    },
    {
      "item_id": "test_1046",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 2.0,
      "nll": 0.6953125
    },
    {
      "item_id": "test_1047",
      "prediction": "Thorax",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.18181818181818182,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.16666666666666666,
        "char_recall": 0.2,
        "char_f1": 0.1818181818181818,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 2.828125,
      "nll": 1.0390625
    },
    {
      "item_id": "test_1048",
      "prediction": "Lung",
      "reference": "Lung, Heart",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.5333333333333333,
        "word_overlap": 0.5,
        "bleu_4": 0.024066394763145416,
        "rouge_l": 0.6666666666666666,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 0.36363636363636365,
        "char_f1": 0.5333333333333333,
        "word_precision": 1.0,
        "word_recall": 0.5,
        "word_f1": 0.6666666666666666
      },
      "perplexity": 51.25,
      "nll": 3.9375
    },
    {
      "item_id": "test_1049",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.9609375,
      "nll": 0.671875
    },
    {
      "item_id": "test_1050",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.15625,
      "nll": 0.142578125
    },
    {
      "item_id": "test_1051",
      "prediction": "Yes",
      "reference": "Yes",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1328125,
      "nll": 0.126953125
    },
    {
      "item_id": "test_1052",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 3.578125,
      "nll": 1.2734375
    },
    {
      "item_id": "test_1053",
      "prediction": "No",
      "reference": "Yes",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.0,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.0,
        "char_recall": 0.0,
        "char_f1": 0.0,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 8.0,
      "nll": 2.078125
    },
    {
      "item_id": "test_1054",
      "prediction": "Lung",
      "reference": "Lung",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.8515625,
      "nll": 0.6171875
    },
    {
      "item_id": "test_1055",
      "prediction": "CT",
      "reference": "CT",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.46875,
      "nll": 0.3828125
    },
    {
      "item_id": "test_1056",
      "prediction": "Pelvis",
      "reference": "Chest",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.36363636363636365,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.3333333333333333,
        "char_recall": 0.4,
        "char_f1": 0.3636363636363636,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 5.5,
      "nll": 1.703125
    },
    {
      "item_id": "test_1057",
      "prediction": "Kidney",
      "reference": "Spinal Cord",
      "metrics": {
        "exact_match": 0.0,
        "soft_match": 0.23529411764705882,
        "word_overlap": 0.0,
        "bleu_4": 0,
        "rouge_l": 0.0,
        "bertscore_f1": 0.0,
        "char_precision": 0.5,
        "char_recall": 0.2727272727272727,
        "char_f1": 0.3529411764705882,
        "word_precision": 0.0,
        "word_recall": 0.0,
        "word_f1": 0.0
      },
      "perplexity": 7.75,
      "nll": 2.046875
    },
    {
      "item_id": "test_1058",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.1953125,
      "nll": 0.1787109375
    },
    {
      "item_id": "test_1059",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0625,
      "nll": 0.0634765625
    },
    {
      "item_id": "test_1060",
      "prediction": "No",
      "reference": "No",
      "metrics": {
        "exact_match": 1.0,
        "soft_match": 1.0,
        "word_overlap": 1.0,
        "bleu_4": 0.1778279410038923,
        "rouge_l": 1.0,
        "bertscore_f1": 0.0,
        "char_precision": 1.0,
        "char_recall": 1.0,
        "char_f1": 1.0,
        "word_precision": 1.0,
        "word_recall": 1.0,
        "word_f1": 1.0
      },
      "perplexity": 1.0859375,
      "nll": 0.0791015625
    }
  ]
}