from tensorboardX import SummaryWriter
import datetime
import logging
import os
import sys
from transformers import TrainerCallback
import psutil
import torch
import math

def init_tensorboard(training_args, model_args, data_args):
    """åˆå§‹åŒ–TensorBoard"""
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—ç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"{training_args.output_dir}/tensorboard_{timestamp}"
    
    writer = SummaryWriter(log_dir=log_dir)
    
    # è®°å½•è¶…å‚æ•°
    hparams = {
        "model_name": model_args.model_id.split('/')[-1],
        "lora_rank": training_args.lora_rank,
        "lora_alpha": training_args.lora_alpha,
        "learning_rate": training_args.learning_rate,
        "batch_size": training_args.per_device_train_batch_size,
        "gradient_accumulation_steps": training_args.gradient_accumulation_steps,
        "max_steps": training_args.max_steps,
        "freeze_vision_tower": training_args.freeze_vision_tower,
        "freeze_llm": training_args.freeze_llm,
    }
    
    # è®°å½•è¶…å‚æ•°åˆ°TensorBoard
    writer.add_hparams(hparams, {})
    
    print(f"ğŸ“Š TensorBoardæ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“Š å¯åŠ¨TensorBoard: tensorboard --logdir {log_dir}")
    
    return writer


def rank0_print(*args):
    """æ‰“å°å‡½æ•°"""
    print(*args)

class TensorBoardCallback(TrainerCallback):
    """å¢å¼ºçš„TensorBoardå›è°ƒ - åŒ…å«æ–‡ä»¶æ—¥å¿—åŠŸèƒ½"""
    
    def __init__(self, tb_writer=None, output_dir=None):
        self.tb_writer = tb_writer
        self.output_dir = output_dir
        self.start_time = None
        self.last_log_step = 0
        
        # GPUä¿¡æ¯
        self.gpu_available = torch.cuda.is_available()
        if self.gpu_available:
            self.gpu_count = torch.cuda.device_count()
            self.current_device = torch.cuda.current_device()
        
        # è®¾ç½®æ–‡ä»¶æ—¥å¿—
        self.file_logger = None
        if output_dir:
            self._setup_file_logger(output_dir)
    
    def _setup_file_logger(self, output_dir):
        """è®¾ç½®æ–‡ä»¶æ—¥å¿—è®°å½•å™¨"""
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºlogger
        self.file_logger = logging.getLogger("QwenTraining")
        self.file_logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        self.file_logger.handlers = []
        
        # æ–‡ä»¶handler
        log_file = os.path.join(output_dir, "training.log")
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter(
            '[%(asctime)s] [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ handler
        self.file_logger.addHandler(file_handler)
        
        # è®°å½•åˆå§‹ä¿¡æ¯
        self.file_logger.info("=" * 60)
        self.file_logger.info("QWEN2-VL TRAINING LOG")
        self.file_logger.info("=" * 60)
        self.file_logger.info(f"Log file: {log_file}")
        self.file_logger.info(f"Output directory: {output_dir}")
    
    def log_print(self, *args, **kwargs):
        """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
        message = " ".join(str(arg) for arg in args)
        
        # æ§åˆ¶å°è¾“å‡º
        print(message, **kwargs)
        
        # æ–‡ä»¶æ—¥å¿—è¾“å‡º
        if self.file_logger:
            self.file_logger.info(message)
    
    def get_memory_info(self):
        """è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        memory_info = {}
        
        # CPUå†…å­˜
        try:
            cpu_memory = psutil.virtual_memory()
            memory_info.update({
                'cpu_memory_used_gb': cpu_memory.used / (1024**3),
                'cpu_memory_total_gb': cpu_memory.total / (1024**3),
                'cpu_memory_percent': cpu_memory.percent,
                'cpu_memory_available_gb': cpu_memory.available / (1024**3)
            })
        except Exception as e:
            self.log_print(f"âš ï¸  CPUå†…å­˜è·å–å¤±è´¥: {e}")
        
        # GPUå†…å­˜
        if self.gpu_available:
            try:
                gpu_memory = torch.cuda.memory_stats(self.current_device)
                memory_info.update({
                    'gpu_memory_allocated_gb': torch.cuda.memory_allocated(self.current_device) / (1024**3),
                    'gpu_memory_reserved_gb': torch.cuda.memory_reserved(self.current_device) / (1024**3),
                    'gpu_memory_max_allocated_gb': torch.cuda.max_memory_allocated(self.current_device) / (1024**3),
                    'gpu_memory_max_reserved_gb': torch.cuda.max_memory_reserved(self.current_device) / (1024**3),
                })
                
                # GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœæœ‰nvidia-ml-pyåº“ï¼‰
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(self.current_device)
                    gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    memory_info['gpu_utilization_percent'] = gpu_util.gpu
                    memory_info['gpu_memory_utilization_percent'] = gpu_util.memory
                except ImportError:
                    pass  # pynvml not available
                except Exception as e:
                    pass  # GPUç›‘æ§å¤±è´¥
                    
            except Exception as e:
                self.log_print(f"âš ï¸  GPUå†…å­˜è·å–å¤±è´¥: {e}")
        
        return memory_info
    
    def _extract_loss_from_logs(self, logs):
        """ä»logsä¸­æå–losså€¼ï¼Œå¤„ç†å¤šç§å¯èƒ½çš„é”®å"""
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„lossé”®å
        loss_keys = [
            'train_loss',  # æ ‡å‡†è®­ç»ƒloss
            'loss',        # é€šç”¨loss
            'train/loss',  # å¯èƒ½çš„å‘½åç©ºé—´æ ¼å¼
            'training_loss', # å¦ä¸€ç§å¯èƒ½çš„å‘½å
        ]
        
        for key in loss_keys:
            if key in logs:
                value = logs[key]
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    return value
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†çš„lossé”®ï¼Œå°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«"loss"çš„é”®
        for key, value in logs.items():
            if 'loss' in key.lower() and not key.startswith('eval_'):
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    return value
        
        return None
    
    def _extract_lr_from_logs(self, logs):
        """ä»logsä¸­æå–å­¦ä¹ ç‡"""
        lr_keys = [
            'learning_rate',
            'lr',
            'train/learning_rate',
            'training/learning_rate'
        ]
        
        for key in lr_keys:
            if key in logs:
                value = logs[key]
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    return value
        
        return None
    
    def _extract_grad_norm_from_logs(self, logs):
        """ä»logsä¸­æå–æ¢¯åº¦èŒƒæ•°"""
        grad_norm_keys = [
            'grad_norm',
            'gradient_norm',
            'train/grad_norm',
            'training/grad_norm'
        ]
        
        for key in grad_norm_keys:
            if key in logs:
                value = logs[key]
                if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                    return value
        
        return None
    
    def on_init_end(self, args, state, control, **kwargs):
        """åˆå§‹åŒ–ç»“æŸæ—¶è®°å½•è®­ç»ƒé…ç½®"""
        if self.file_logger:
            self.file_logger.info("\nTraining Configuration:")
            self.file_logger.info(f"  Model: {kwargs.get('model', 'N/A')}")
            self.file_logger.info(f"  Learning Rate: {args.learning_rate}")
            self.file_logger.info(f"  Batch Size: {args.per_device_train_batch_size}")
            self.file_logger.info(f"  Gradient Accumulation: {args.gradient_accumulation_steps}")
            self.file_logger.info(f"  Max Steps: {args.max_steps}")
            self.file_logger.info(f"  Evaluation Strategy: {args.evaluation_strategy}")
            if args.evaluation_strategy != "no":
                self.file_logger.info(f"  Eval Steps: {args.eval_steps}")
            self.file_logger.info("=" * 60)
    
    def on_train_begin(self, args, state, control, **kwargs):
        import time
        self.start_time = time.time()
        self.log_print("ğŸš€ Training started!")
        
        if self.tb_writer:
            self.tb_writer.add_text("training/status", "Training started", 0)
            
            # è®°å½•åˆå§‹å†…å­˜çŠ¶æ€
            memory_info = self.get_memory_info()
            for key, value in memory_info.items():
                self.tb_writer.add_scalar(f"memory/{key}", value, 0)
        
        # æ›´æ–°output_dirï¼ˆå¦‚æœtrainerä¸­è®¾ç½®äº†ï¼‰
        if hasattr(args, 'output_dir') and args.output_dir and not self.file_logger:
            self._setup_file_logger(args.output_dir)
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        
        current_step = state.global_step
        
        # åˆ¤æ–­è¿™æ˜¯è®­ç»ƒæ—¥å¿—è¿˜æ˜¯è¯„ä¼°æ—¥å¿—
        is_eval_log = any(key.startswith('eval_') for key in logs.keys())
        is_train_log = any(key in ['train_loss', 'loss', 'learning_rate', 'grad_norm'] for key in logs.keys())
        
        # ä¸ºè®­ç»ƒæ—¥å¿—å’Œè¯„ä¼°æ—¥å¿—åˆ†åˆ«è¿½è¸ªæœ€åè®°å½•çš„æ­¥éª¤
        if not hasattr(self, 'last_train_log_step'):
            self.last_train_log_step = 0
        if not hasattr(self, 'last_eval_log_step'):
            self.last_eval_log_step = 0
        
        # æ ¹æ®æ—¥å¿—ç±»å‹è¿›è¡Œé‡å¤æ£€æŸ¥
        if is_eval_log:
            if current_step <= self.last_eval_log_step:
                return
            self.last_eval_log_step = current_step
            log_type = "eval"
        elif is_train_log:
            if current_step <= self.last_train_log_step:
                return
            self.last_train_log_step = current_step
            log_type = "train"
        else:
            # æœªçŸ¥ç±»å‹çš„æ—¥å¿—ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
            if current_step <= self.last_log_step:
                return
            self.last_log_step = current_step
            log_type = "unknown"
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ‰“å°æ‰€æœ‰å¯ç”¨çš„æ—¥å¿—é”®ï¼ˆä»…åœ¨å‰å‡ æ­¥ï¼‰
        if current_step <= 10:
            self.log_print(f"ğŸ” Debug - Step {current_step} ({log_type}): {list(logs.keys())}")
        
        # è®¡ç®—è®­ç»ƒé€Ÿåº¦
        import time
        current_time = time.time()
        elapsed = current_time - self.start_time if self.start_time else 0
        steps_per_sec = current_step / elapsed if elapsed > 0 else 0
        
        # è·å–å†…å­˜ä¿¡æ¯
        memory_info = self.get_memory_info()
        epoch = state.epoch if state.epoch else 0
        
        # è·å–GPUå†…å­˜
        if int(os.environ.get("LOCAL_RANK", -1)) == -1:
            max_gpu_memory = torch.cuda.max_memory_allocated() / 1024 ** 2 if self.gpu_available else 0
        else:
            local_rank = int(os.environ["LOCAL_RANK"]) 
            device = torch.device(f"cuda:{local_rank}")
            max_gpu_memory = torch.cuda.max_memory_allocated(device) / 1024 ** 2
        
        # å¤„ç†ä¸åŒç±»å‹çš„æ—¥å¿—
        if is_eval_log:
            # å¤„ç†è¯„ä¼°æ—¥å¿—
            self._handle_eval_logs(logs, current_step)
        elif is_train_log:
            # å¤„ç†è®­ç»ƒæ—¥å¿—
            self._handle_train_logs(logs, current_step, steps_per_sec, epoch, max_gpu_memory, memory_info)
        
        # TensorBoardè®°å½•ï¼ˆæ‰€æœ‰ç±»å‹çš„æ—¥å¿—éƒ½è®°å½•ï¼‰
        if self.tb_writer:
            try:
                # è®°å½•æ‰€æœ‰å¯ç”¨çš„è®­ç»ƒæŒ‡æ ‡
                for key, value in logs.items():
                    if isinstance(value, (int, float)) and not (math.isnan(value) or math.isinf(value)):
                        self.tb_writer.add_scalar(f"training/{key}", value, current_step)
                
                # åªåœ¨è®­ç»ƒæ—¥å¿—æ—¶è®°å½•è¿™äº›æŒ‡æ ‡
                if is_train_log:
                    # è®­ç»ƒé€Ÿåº¦
                    self.tb_writer.add_scalar("training/steps_per_second", steps_per_sec, current_step)
                    self.tb_writer.add_scalar("training/elapsed_minutes", elapsed / 60, current_step)
                    
                    # å†…å­˜ä½¿ç”¨æƒ…å†µ
                    for key, value in memory_info.items():
                        if isinstance(value, (int, float)):
                            self.tb_writer.add_scalar(f"memory/{key}", value, current_step)
                
                # æ¯50æ­¥åˆ·æ–°ä¸€æ¬¡
                if current_step % 50 == 0:
                    self.tb_writer.flush()
                    
            except Exception as e:
                if current_step % 100 == 0:
                    self.log_print(f"âš ï¸  TensorBoardè®°å½•å‡ºé”™: {e}")
    
    def _handle_train_logs(self, logs, current_step, steps_per_sec, epoch, max_gpu_memory, memory_info):
        """å¤„ç†è®­ç»ƒæ—¥å¿—"""
        # æå–è®­ç»ƒæŒ‡æ ‡
        loss = self._extract_loss_from_logs(logs)
        lr = self._extract_lr_from_logs(logs)
        grad_norm = self._extract_grad_norm_from_logs(logs)
        
        # æ„å»ºæ—¥å¿—æ¶ˆæ¯
        log_parts = [f"ğŸ“ˆ Step {current_step}: "]
        
        if loss is not None:
            log_parts.append(f"Loss={loss:.4f}")
        else:
            log_parts.append("Loss=N/A")
            
        if lr is not None:
            log_parts.append(f"LR={lr:.2e}")
        else:
            log_parts.append("LR=N/A")
            
        if grad_norm is not None:
            log_parts.append(f"GradNorm={grad_norm:.2f}")
        else:
            log_parts.append("GradNorm=N/A")
            
        log_parts.extend([
            f"Speed={steps_per_sec:.2f} steps/s",
            f"Epoch={epoch:.2f}",
            f"GPU MaxMemory={max_gpu_memory:.2f} MB"
        ])
        
        # æ·»åŠ å†…å­˜ä¿¡æ¯
        cpu_mem = memory_info.get('cpu_memory_percent', 0)
        gpu_mem = memory_info.get('gpu_memory_allocated_gb', 0)
        log_parts.extend([
            f"CPU={cpu_mem:.1f}%",
            f"GPU={gpu_mem:.2f}GB"
        ])
        
        # å®šæœŸè¾“å‡ºæ—¥å¿—ï¼ˆæ¯20æ­¥ï¼‰
        if current_step % 20 == 0:
            self.log_print(", ".join(log_parts))
        
        # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°lossï¼Œåœ¨å‰å‡ æ­¥æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        if loss is None and current_step <= 10:
            self.log_print(f"âš ï¸  Warning: No loss found in training logs at step {current_step}")
            self.log_print(f"   Available keys: {list(logs.keys())}")
            self.log_print(f"   Log values: {logs}")
    
    def _handle_eval_logs(self, logs, current_step):
        """å¤„ç†è¯„ä¼°æ—¥å¿—"""
        eval_logs = {k: v for k, v in logs.items() if k.startswith('eval_')}
        if eval_logs:
            self.log_print(f"\nğŸ“Š Evaluation at step {current_step}:")
            for key, value in eval_logs.items():
                if isinstance(value, (int, float)):
                    self.log_print(f"  {key}: {value:.4f}")
                else:
                    self.log_print(f"  {key}: {value}")
            self.log_print("")  # ç©ºè¡Œ
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """è¯„ä¼°å®Œæˆæ—¶è®°å½•"""
        if metrics:
            self.log_print(f"\nâœ… Evaluation completed at step {state.global_step}")
            # è¯¦ç»†æŒ‡æ ‡å·²åœ¨on_logä¸­è®°å½•
    
    def on_save(self, args, state, control, **kwargs):
        """ä¿å­˜æ£€æŸ¥ç‚¹æ—¶è®°å½•"""
        self.log_print(f"\nğŸ’¾ Model checkpoint saved at step {state.global_step}")
        if hasattr(state, 'best_metric') and state.best_metric is not None:
            self.log_print(f"  Best metric so far: {state.best_metric:.4f}")
    
    def on_train_end(self, args, state, control, **kwargs):
        self.log_print("\n" + "=" * 60)
        self.log_print("âœ… Training completed!")
        self.log_print(f"Total steps: {state.global_step}")
        
        if self.start_time:
            import time
            elapsed = time.time() - self.start_time
            hours = int(elapsed // 3600)
            minutes = int((elapsed % 3600) // 60)
            seconds = int(elapsed % 60)
            self.log_print(f"Total time: {hours}h {minutes}m {seconds}s")
        
        if hasattr(state, 'best_metric') and state.best_metric is not None:
            self.log_print(f"Best metric: {state.best_metric:.4f}")
        if hasattr(state, 'best_model_checkpoint') and state.best_model_checkpoint:
            self.log_print(f"Best checkpoint: {state.best_model_checkpoint}")
        
        self.log_print("=" * 60)
        
        if self.tb_writer:
            # è®°å½•æœ€ç»ˆå†…å­˜çŠ¶æ€
            memory_info = self.get_memory_info()
            for key, value in memory_info.items():
                self.tb_writer.add_scalar(f"memory_final/{key}", value, state.global_step)
            
            self.tb_writer.add_text("training/status", "Training completed", state.global_step)
            self.tb_writer.close()
            self.log_print("ğŸ“Š TensorBoardæ—¥å¿—å·²å…³é—­")
        
        # å…³é—­æ–‡ä»¶æ—¥å¿—
        if self.file_logger:
            self.file_logger.info("\nTraining log closed.")
            for handler in self.file_logger.handlers:
                handler.close()