"""
ç®€åŒ–çš„è¯„ä¼°æ¨¡å—ï¼Œç”¨äºQwen2.5-VLè®­ç»ƒéªŒè¯
æ”¯æŒåŸºäºtype_mappingçš„åˆ†ç±»è¯„ä¼° (close-set vs open-set)
"""

import os
import re
import json
import torch
import numpy as np
import time
from typing import Dict, List, Any, Optional
from collections import defaultdict
from difflib import SequenceMatcher

# å°è¯•å¯¼å…¥NLTKï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
try:
    import nltk
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from rouge_score import rouge_scorer
    FULL_EVAL = True
except ImportError:
    FULL_EVAL = False
    print("âš ï¸ NLTK/rouge_score not available, using simplified evaluation")

# å…¨å±€å˜é‡å­˜å‚¨type_mapping
TYPE_MAPPING = {}
TYPE_MAPPING_LOADED = True


def load_type_mapping(mapping_file_path: str) -> bool:
    """åŠ è½½type_mappingæ–‡ä»¶ä½œä¸ºå…¨å±€å˜é‡"""
    global TYPE_MAPPING, TYPE_MAPPING_LOADED
    
    if not os.path.exists(mapping_file_path):
        print(f"âš ï¸ Type mapping file not found: {mapping_file_path}")
        return False
    
    try:
        with open(mapping_file_path, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # æå–type_mappingéƒ¨åˆ†
        if "type_mapping" in mapping_data:
            TYPE_MAPPING = mapping_data["type_mapping"]
        elif "id_type_mapping" in mapping_data:  # å…¼å®¹æ—§æ ¼å¼
            TYPE_MAPPING = mapping_data["id_type_mapping"]
        else:
            TYPE_MAPPING = mapping_data
        
        TYPE_MAPPING_LOADED = True
        
        # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
        type_stats = {}
        for item_type in TYPE_MAPPING.values():
            type_stats[item_type] = type_stats.get(item_type, 0) + 1
        
        print(f"âœ… Type mapping loaded: {len(TYPE_MAPPING)} items")
        print(f"ğŸ“Š Type distribution: {type_stats}")
        return True
        
    except Exception as e:
        print(f"âŒ Failed to load type mapping: {e}")
        return False


def get_item_type(item_id: str) -> str:
    """è·å–itemçš„ç±»å‹"""
    if not TYPE_MAPPING_LOADED:
        return "unknown"
    return TYPE_MAPPING.get(item_id, "unknown")

class SimpleTextEvaluator:
    """ç®€åŒ–çš„æ–‡æœ¬è¯„ä¼°å™¨ï¼Œä¸“æ³¨äºæ ¸å¿ƒæŒ‡æ ‡"""
    
    def __init__(self, use_full_metrics: bool = None):
        self.use_full_metrics = FULL_EVAL if use_full_metrics is None else use_full_metrics
        
        if self.use_full_metrics:
            try:
                # è®¾ç½®è‡ªå®šä¹‰NLTKè·¯å¾„
                custom_path = None
                if os.path.exists(custom_path):
                    nltk.data.path.insert(0, custom_path)
                
                self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
                self.smoothing = SmoothingFunction().method1
                print("âœ… Full evaluation metrics loaded")
            except Exception as e:
                print(f"âš ï¸ Failed to load full metrics: {e}, using simplified version")
                self.use_full_metrics = False
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not isinstance(text, str):
            text = str(text)
        text = text.strip()
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        return text
    
    def simple_tokenize(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯ï¼ˆä¸ä¾èµ–NLTKï¼‰"""
        text = self.preprocess_text(text).lower()
        # ç®€å•çš„å•è¯åˆ†å‰²
        words = re.findall(r'\b\w+\b', text)
        return words
    
    def calculate_exact_match(self, reference: str, candidate: str) -> float:
        """ç²¾ç¡®åŒ¹é…"""
        ref = self.preprocess_text(reference).lower()
        cand = self.preprocess_text(candidate).lower()
        return 1.0 if ref == cand else 0.0
    
    def calculate_soft_match(self, reference: str, candidate: str, threshold: float = 0.6) -> float:
        """è½¯åŒ¹é…ï¼ˆåŸºäºå­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼‰"""
        ref = self.preprocess_text(reference).lower()
        cand = self.preprocess_text(candidate).lower()
        
        if not cand.strip():
            return 0.0
        
        if ref == cand:
            return 1.0
        
        similarity = SequenceMatcher(None, ref, cand).ratio()
        return similarity
    
    def calculate_word_overlap(self, reference: str, candidate: str) -> float:
        """è¯æ±‡é‡å åº¦"""
        ref_words = set(self.simple_tokenize(reference))
        cand_words = set(self.simple_tokenize(candidate))
        
        if not ref_words or not cand_words:
            return 0.0
        
        overlap = len(ref_words.intersection(cand_words))
        union = len(ref_words.union(cand_words))
        return overlap / union if union > 0 else 0.0
    
    def calculate_bleu_4(self, reference: str, candidate: str) -> float:
        """BLEU-4åˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if not self.use_full_metrics:
            # ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            return self.calculate_word_overlap(reference, candidate)
        
        try:
            ref_tokens = nltk.word_tokenize(reference.lower())
            cand_tokens = nltk.word_tokenize(candidate.lower())
            
            if len(cand_tokens) == 0:
                return 0.0
            
            weights = (0.25, 0.25, 0.25, 0.25)
            score = sentence_bleu([ref_tokens], cand_tokens, weights=weights, smoothing_function=self.smoothing)
            return score
        except:
            return self.calculate_word_overlap(reference, candidate)
    
    def calculate_rouge_l(self, reference: str, candidate: str) -> float:
        """ROUGE-Låˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if not self.use_full_metrics:
            return self.calculate_word_overlap(reference, candidate)
        
        try:
            scores = self.rouge_scorer.score(reference, candidate)
            return scores['rougeL'].fmeasure
        except:
            return self.calculate_word_overlap(reference, candidate)
    
    def evaluate_batch_with_types(self, predictions: List[str], references: List[str], 
                                  item_ids: List[str]) -> Dict[str, Any]:
        """
        æ‰¹é‡è¯„ä¼°å¹¶æŒ‰ç±»å‹åˆ†ç»„è¿”å›æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            references: å‚è€ƒç­”æ¡ˆåˆ—è¡¨
            item_ids: item IDåˆ—è¡¨
            
        Returns:
            åŒ…å«æ€»ä½“æŒ‡æ ‡å’Œåˆ†ç±»æŒ‡æ ‡çš„å­—å…¸
        """
        if len(predictions) != len(references) or len(predictions) != len(item_ids):
            raise ValueError("Predictions, references, and item_ids must have the same length")
        
        # æŒ‰ç±»å‹åˆ†ç»„æ•°æ®
        type_groups = defaultdict(list)
        for pred, ref, item_id in zip(predictions, references, item_ids):
            item_type = get_item_type(item_id)
            type_groups[item_type].append((pred, ref, item_id))
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = self._calculate_metrics_for_group(predictions, references, "overall")
        
        # è®¡ç®—å„ç±»å‹æŒ‡æ ‡
        type_metrics = {}
        for item_type, group_data in type_groups.items():
            if group_data:
                group_preds = [item[0] for item in group_data]
                group_refs = [item[1] for item in group_data]
                type_metrics[item_type] = self._calculate_metrics_for_group(
                    group_preds, group_refs, item_type
                )
        
        # ç»„è£…ç»“æœ
        results = {
            "overall": overall_metrics,
            "by_type": type_metrics,
            "sample_counts": {
                "total": len(predictions),
                **{f"{item_type}_count": len(group_data) for item_type, group_data in type_groups.items()}
            }
        }
        
        return results
    
    def _calculate_metrics_for_group(self, predictions: List[str], references: List[str], 
                                   group_name: str) -> Dict[str, float]:
        """ä¸ºä¸€ç»„æ•°æ®è®¡ç®—æŒ‡æ ‡"""
        if not predictions:
            return {}
        
        metrics = {
            'exact_match': [],
            'soft_match': [],
            'word_overlap': [],
            'bleu_4': [],
            'rouge_l': []
        }
        
        for pred, ref in zip(predictions, references):
            metrics['exact_match'].append(self.calculate_exact_match(ref, pred))
            metrics['soft_match'].append(self.calculate_soft_match(ref, pred))
            metrics['word_overlap'].append(self.calculate_word_overlap(ref, pred))
            metrics['bleu_4'].append(self.calculate_bleu_4(ref, pred))
            metrics['rouge_l'].append(self.calculate_rouge_l(ref, pred))
        
        # è¿”å›å¹³å‡å€¼ï¼Œæ·»åŠ å‰ç¼€
        avg_metrics = {}
        for key, values in metrics.items():
            avg_metrics[f'eval_{key}'] = np.mean(values) if values else 0.0
        
        avg_metrics['eval_samples'] = len(predictions)
        return avg_metrics
    
    def evaluate_batch(self, predictions: List[str], references: List[str]) -> Dict[str, float]:
        """å‘åå…¼å®¹çš„æ‰¹é‡è¯„ä¼°æ–¹æ³•"""
        return self._calculate_metrics_for_group(predictions, references, "overall")

    
    

class EvaluationCallback:
    """è¯„ä¼°å›è°ƒï¼Œç”¨äºè®°å½•éªŒè¯è¿‡ç¨‹ä¸­çš„æ ·ä¾‹å’ŒæŒ‡æ ‡"""
    
    def __init__(self, output_dir: str, save_examples: bool = True):
        self.output_dir = output_dir
        self.save_examples = save_examples
        self.eval_history = []
        
        os.makedirs(output_dir, exist_ok=True)
    
    def log_evaluation_results(self, step: int, results: Dict[str, Any], 
                             predictions: List[str] = None, references: List[str] = None,
                             item_ids: List[str] = None):
        """è®°å½•è¯„ä¼°ç»“æœ"""
        
        # ä¿å­˜è¯„ä¼°æŒ‡æ ‡å†å²
        eval_record = {
            "step": step,
            "timestamp": time.time(),
            "metrics": results
        }
        self.eval_history.append(eval_record)
        
        # ä¿å­˜è¯„ä¼°å†å²åˆ°æ–‡ä»¶
        history_file = os.path.join(self.output_dir, "evaluation_history.json")
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(self.eval_history, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å½“å‰è¯„ä¼°ç»“æœ
        current_result_file = os.path.join(self.output_dir, f"evaluation_step_{step}.json")
        with open(current_result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        self._print_evaluation_results(step, results)
        
        # ä¿å­˜æ ·ä¾‹ï¼ˆå¦‚æœæä¾›ï¼‰
        if self.save_examples and predictions and references and item_ids:
            self._save_examples(step, predictions, references, item_ids)
    
    def _print_evaluation_results(self, step: int, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š EVALUATION RESULTS - Step {step}")
        print(f"{'='*60}")
        
        # æ‰“å°æ€»ä½“æŒ‡æ ‡
        overall = results.get("overall", {})
        if overall:
            print("ğŸ¯ Overall Metrics:")
            for metric, value in overall.items():
                if metric.startswith('eval_'):
                    print(f"  {metric}: {value:.4f}")
        
        # æ‰“å°åˆ†ç±»æŒ‡æ ‡
        by_type = results.get("by_type", {})
        if by_type:
            print("\nğŸ“‹ Metrics by Type:")
            for item_type, metrics in by_type.items():
                if metrics:
                    print(f"\n  ğŸ“‚ {item_type.upper()}:")
                    for metric, value in metrics.items():
                        if metric.startswith('eval_'):
                            print(f"    {metric}: {value:.4f}")
        
        # æ‰“å°æ ·æœ¬ç»Ÿè®¡
        sample_counts = results.get("sample_counts", {})
        if sample_counts:
            print(f"\nğŸ“Š Sample Counts:")
            for count_type, count in sample_counts.items():
                print(f"  {count_type}: {count}")
        
        print(f"{'='*60}\n")
    
    def _save_examples(self, step: int, predictions: List[str], references: List[str], item_ids: List[str]):
        """ä¿å­˜è¯„ä¼°æ ·ä¾‹"""
        examples = []
        max_examples = min(10, len(predictions))  # æœ€å¤šä¿å­˜10ä¸ªæ ·ä¾‹
        
        for i in range(max_examples):
            item_type = get_item_type(item_ids[i])
            examples.append({
                'step': step,
                'item_id': item_ids[i],
                'item_type': item_type,
                'prediction': predictions[i],
                'reference': references[i]
            })
        
        # ä¿å­˜æ ·ä¾‹åˆ°æ–‡ä»¶
        examples_file = os.path.join(self.output_dir, f"examples_step_{step}.json")
        with open(examples_file, 'w', encoding='utf-8') as f:
            json.dump(examples, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Saved {len(examples)} evaluation examples to {examples_file}")



def create_compute_metrics_with_types(processor, output_dir: str = None, type_mapping_file: str = None, eval_dataset=None):   
    """
    åˆ›å»ºæ”¯æŒç±»å‹åˆ†ç»„çš„compute_metricså‡½æ•°
    
    Args:
        processor: Qwenå¤„ç†å™¨
        output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜ç»“æœå’Œæ ·ä¾‹ï¼‰
        type_mapping_file: ç±»å‹æ˜ å°„æ–‡ä»¶è·¯å¾„
        eval_dataset: è¯„ä¼°æ•°æ®é›†ï¼ˆç”¨äºè·å–item_idsï¼‰
    
    Returns:
        compute_metricså‡½æ•°
    """
    
    # åŠ è½½type_mapping
    if type_mapping_file:
        load_type_mapping(type_mapping_file)
    
    evaluator = SimpleTextEvaluator()
    callback = EvaluationCallback(output_dir) if output_dir else None
    
    # æ·»åŠ æ­¥æ•°è®¡æ•°å™¨å’Œç´¯ç§¯å˜é‡
    accumulated_predictions = []
    accumulated_labels = []
    accumulated_item_ids = []
    step_counter = [0]
    
    def compute_metrics(eval_preds, compute_result=None):
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            eval_preds: EvalPredictionå¯¹è±¡
            compute_result: æ˜¯å¦è®¡ç®—å¹¶è¿”å›æœ€ç»ˆç»“æœï¼ˆç”¨äºbatch_eval_metricsï¼‰
        """
        # ä»EvalPredictionå¯¹è±¡ä¸­æå–æ•°æ®
        predictions = eval_preds.predictions
        labels = eval_preds.label_ids
        inputs = eval_preds.inputs if hasattr(eval_preds, 'inputs') else None
        
        if isinstance(predictions, tuple):
            # Qwen2-VL è¿”å› tupleï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ language logits
            predictions = predictions[0]
            print(f"[Qwen2-VL] Processing tuple output, using first element with shape: {predictions.shape}")
        
        
        # å¤„ç† predictions å’Œ labels çš„ç»´åº¦
        # åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œpredictions å¯èƒ½æ˜¯ logitsï¼Œéœ€è¦è½¬æ¢ä¸º token ids
        if len(predictions.shape) == 3:  # [batch_size, sequence_length, vocab_size]
            # å– argmax è·å¾— token ids
            predictions = predictions.argmax(axis=-1)
        
        # è§£ç å½“å‰æ‰¹æ¬¡
        batch_decoded_preds = []
        batch_decoded_labels = []
        batch_item_ids = []
        
        for i, (pred_ids, label_ids) in enumerate(zip(predictions, labels)):
            # è¿‡æ»¤æ‰-100 (padding)
            valid_indices = label_ids != -100
            if not valid_indices.any():
                # å¦‚æœå…¨æ˜¯-100ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                continue
            
            # è·å–ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
            first_valid_idx = valid_indices.nonzero()[0].item() if valid_indices.any() else 0
            last_valid_idx = valid_indices.nonzero()[-1].item() + 1 if valid_indices.any() else len(label_ids)
            
            # åªä¿ç•™éœ€è¦è®¡ç®—lossçš„éƒ¨åˆ†ï¼ˆå¯¹åº”label_idsä¸­é-100çš„éƒ¨åˆ†ï¼‰
            pred_ids_valid = pred_ids[first_valid_idx:last_valid_idx]
            label_ids_valid = label_ids[first_valid_idx:last_valid_idx]
            
            # å†æ¬¡è¿‡æ»¤å¯èƒ½æ®‹ç•™çš„-100ï¼ˆè™½ç„¶ç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼‰
            mask = label_ids_valid != -100
            pred_ids_valid = pred_ids_valid[mask]
            label_ids_valid = label_ids_valid[mask]
            
            try:
                pred_text = processor.tokenizer.decode(pred_ids_valid, skip_special_tokens=True)
                label_text = processor.tokenizer.decode(label_ids_valid, skip_special_tokens=True)
                
                batch_decoded_preds.append(pred_text)
                batch_decoded_labels.append(label_text)
                
                # è·å–item_id
                if eval_dataset and hasattr(eval_dataset, 'get_item_id'):
                    # è®¡ç®—å…¨å±€ç´¢å¼•
                    global_idx = len(accumulated_predictions) + i
                    item_id = eval_dataset.get_item_id(global_idx)
                else:
                    item_id = f"eval_item_{len(accumulated_predictions) + i}"
                batch_item_ids.append(item_id)
                
            except Exception as e:
                print(f"âš ï¸ Decoding error: {e}")
                batch_decoded_preds.append("")
                batch_decoded_labels.append("")
                batch_item_ids.append(f"eval_item_{len(accumulated_predictions) + i}")
        
        # ç´¯ç§¯ç»“æœ
        accumulated_predictions.extend(batch_decoded_preds)
        accumulated_labels.extend(batch_decoded_labels)
        accumulated_item_ids.extend(batch_item_ids)
        
        # å¦‚æœä¸éœ€è¦è®¡ç®—æœ€ç»ˆç»“æœï¼Œè¿”å›ç©ºå­—å…¸
        # if compute_result is False:
        #     return {}
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        if compute_result is True or (compute_result is None and len(accumulated_predictions) > 0):
            # ä½¿ç”¨æ‰€æœ‰ç´¯ç§¯çš„æ•°æ®è®¡ç®—æŒ‡æ ‡
            if TYPE_MAPPING_LOADED:
                results = evaluator.evaluate_batch_with_types(
                    accumulated_predictions, 
                    accumulated_labels, 
                    accumulated_item_ids
                )
            else:
                overall_metrics = evaluator.evaluate_batch(
                    accumulated_predictions, 
                    accumulated_labels
                )
                results = {"overall": overall_metrics}
            
            # è®°å½•è¯„ä¼°ç»“æœ
            if callback:
                step_counter[0] += 1
                callback.log_evaluation_results(
                    step_counter[0], 
                    results, 
                    accumulated_predictions[:10],  # åªä¿å­˜å‰10ä¸ªæ ·ä¾‹
                    accumulated_labels[:10], 
                    accumulated_item_ids[:10]
                )
            
            # æ‰“å°æ ·ä¾‹
            print("\nğŸ“ Evaluation Examples:")
            for i in range(min(3, len(accumulated_predictions))):
                item_type = get_item_type(accumulated_item_ids[i]) if TYPE_MAPPING_LOADED else "unknown"
                print(f"Example {i+1} [{item_type}]:")
                print(f"  Prediction: {accumulated_predictions[i][:100]}...")
                print(f"  Reference:  {accumulated_labels[i][:100]}...")
                print()
            
            # æ¸…ç©ºç´¯ç§¯æ•°æ®ï¼ˆå‡†å¤‡ä¸‹ä¸€è½®è¯„ä¼°ï¼‰
            accumulated_predictions.clear()
            accumulated_labels.clear()
            accumulated_item_ids.clear()
            
            # è¿”å›æ‰å¹³åŒ–çš„æŒ‡æ ‡
            flattened_metrics = {}
            
            # æ·»åŠ æ€»ä½“æŒ‡æ ‡
            overall = results.get("overall", {})
            for key, value in overall.items():
                flattened_metrics[key] = value
            
            # æ·»åŠ åˆ†ç±»æŒ‡æ ‡
            by_type = results.get("by_type", {})
            for item_type, metrics in by_type.items():
                for key, value in metrics.items():
                    flattened_metrics[f"{item_type}_{key}"] = value
            
            return flattened_metrics
        
        # é»˜è®¤è¿”å›ç©ºå­—å…¸
        return {}
    
    return compute_metrics


# ç”¨äºåœ¨ Trainer ä¸­è®¾ç½® compute_metrics çš„è¾…åŠ©å‡½æ•°
def setup_trainer_compute_metrics(trainer, processor, output_dir=None, type_mapping_file=None):
    """
    ä¸º Trainer è®¾ç½®è‡ªå®šä¹‰çš„ compute_metrics å‡½æ•°
    
    Args:
        trainer: Hugging Face Trainer å®ä¾‹
        processor: Qwenå¤„ç†å™¨
        output_dir: è¾“å‡ºç›®å½•
        type_mapping_file: ç±»å‹æ˜ å°„æ–‡ä»¶è·¯å¾„
    
    Example:
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
        )
        
        # è®¾ç½®è‡ªå®šä¹‰çš„ compute_metrics
        setup_trainer_compute_metrics(
            trainer, 
            processor, 
            output_dir="./eval_results",
            type_mapping_file="type_mapping.json"
        )
    """
    eval_dataset = trainer.eval_dataset
    compute_metrics_fn = create_compute_metrics_with_types(
        processor=processor,
        output_dir=output_dir,
        type_mapping_file=type_mapping_file,
        eval_dataset=eval_dataset
    )
    
    # è®¾ç½® trainer çš„ compute_metrics
    trainer.compute_metrics = compute_metrics_fn
    
    return trainer