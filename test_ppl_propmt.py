"""
æ¨ç†å’Œå›°æƒ‘åº¦è®¡ç®—è„šæœ¬
ç”¨äºè¿è¡ŒLoRAå’ŒBaseæ¨¡å‹çš„æ¨ç†ï¼Œè®¡ç®—å›°æƒ‘åº¦ï¼Œå¹¶ä¿å­˜è¯¦ç»†ç»“æœ
å¢å¼ºç‰ˆæœ¬ï¼šæ”¯æŒè‡ªå®šä¹‰promptå’Œæ™ºèƒ½yes/noé—®é¢˜æ£€æµ‹
"""

import argparse
import os
import json
import torch
import numpy as np
from tqdm import tqdm
from typing import Dict, List, Any, Optional, Tuple
import time
from datetime import datetime
import math
import re

from transformers import (
    Qwen2VLForConditionalGeneration,
    Qwen2VLProcessor,
    AutoProcessor,
    BitsAndBytesConfig
)
from peft import PeftModel, LoraConfig, get_peft_model
import torch.nn.functional as F
from PIL import Image

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.dataset import SupervisedDataset
from src.params_eval import DataArguments


class PromptHandler:
    """Promptå¤„ç†å™¨ï¼Œè´Ÿè´£å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œæ™ºèƒ½é—®é¢˜æ£€æµ‹"""
    
    def __init__(self, system_prompt: str = None, language: str = "en"):
        """
        åˆå§‹åŒ–Promptå¤„ç†å™¨
        
        Args:
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            language: è¯­è¨€è®¾ç½® ("en" æˆ– "zh")
        """
        self.language = language
        self.system_prompt = system_prompt or self._get_default_system_prompt()
        
        # Yes/Noé—®é¢˜æ£€æµ‹çš„å…³é”®è¯
        self.yes_no_keywords = {
            "en": [
                "is", "are", "was", "were", "can", "could", "will", "would", 
                "should", "do", "does", "did", "have", "has", "had",
                "yes or no", "true or false", "correct or incorrect",
                "right or wrong", "possible or impossible"
            ],
            "zh": [
                "æ˜¯", "æ˜¯å¦", "èƒ½å¦", "å¯ä»¥", "å¯èƒ½", "ä¼šä¸ä¼š", "æœ‰æ²¡æœ‰",
                "å¯¹ä¸å¯¹", "æ­£ç¡®å—", "é”™è¯¯å—", "æ˜¯çœŸçš„å—", "æ˜¯å‡çš„å—"
            ]
        }
        
        # Yes/Noå›ç­”æŒ‡å¯¼è¯
        self.yes_no_instructions = {
            "en": "Please answer with 'yes' or 'no' first, then provide a brief explanation.",
            "zh": "è¯·å…ˆå›ç­”'æ˜¯'æˆ–'å¦'ï¼Œç„¶åæä¾›ç®€çŸ­è§£é‡Šã€‚"
        }
    
    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        if self.language == "zh":
            return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œå…·æœ‰ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚
    ä½ çš„ä»»åŠ¡æ˜¯ï¼š
    1. å‡†ç¡®å›ç­”åŒ»ç–—ç›¸å…³é—®é¢˜
    2. æä¾›åŸºäºè¯æ®çš„åŒ»ç–—å»ºè®®
    3. å¯¹äºè¯Šæ–­ç±»é—®é¢˜ï¼Œè¯·ä»”ç»†åˆ†æç—‡çŠ¶å’Œä½“å¾
    4. å§‹ç»ˆä»¥æ‚£è€…å®‰å…¨ä¸ºç¬¬ä¸€è€ƒè™‘
    5. å¦‚æœé—®é¢˜è¶…å‡ºä½ çš„ä¸“ä¸šèŒƒå›´ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶å»ºè®®å’¨è¯¢ç›¸å…³ä¸“å®¶

    è¯·ç”¨ä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"""
        else:
                return """You are a professional medical assistant with extensive medical knowledge and clinical experience.
    Your tasks include:
    1. Accurately answering medical-related questions
    2. Providing evidence-based medical advice
    3. Carefully analyzing symptoms and signs for diagnostic questions
    4. Always prioritizing patient safety
    5. If questions exceed your expertise, clearly state this and suggest consulting relevant specialists

    Please answer questions using professional, accurate, and easily understandable language."""
    
    def detect_yes_no_question(self, text: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºyes/noé—®é¢˜
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºyes/noé—®é¢˜
        """
        text_lower = text.lower()
        keywords = self.yes_no_keywords[self.language]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«yes/noå…³é”®è¯
        for keyword in keywords:
            if keyword in text_lower:
                return True
        
        # æ£€æŸ¥é—®å·ç»“å°¾çš„ç®€å•é—®å¥æ¨¡å¼
        if text.strip().endswith("?"):
            # è‹±æ–‡ï¼šä»¥åŠ©åŠ¨è¯æˆ–beåŠ¨è¯å¼€å¤´çš„é—®å¥
            if self.language == "en":
                question_starters = ["is", "are", "was", "were", "can", "could", "will", "would", "should", "do", "does", "did", "have", "has", "had"]
                first_word = text_lower.split()[0] if text_lower.split() else ""
                if first_word in question_starters:
                    return True
            
            # ä¸­æ–‡ï¼šåŒ…å«å…¸å‹ç–‘é—®è¯çš„é—®å¥
            elif self.language == "zh":
                question_patterns = ["å—ï¼Ÿ", "å‘¢ï¼Ÿ", "ä¸ï¼Ÿ", "å§ï¼Ÿ"]
                for pattern in question_patterns:
                    if text.endswith(pattern):
                        return True
        
        return False
    
    def format_prompt(self, user_input: str, is_first_message: bool = True) -> str:
        """
        æ ¼å¼åŒ–æç¤ºè¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            is_first_message: æ˜¯å¦ä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
            
        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºè¯
        """
        # æ£€æµ‹æ˜¯å¦ä¸ºyes/noé—®é¢˜
        # is_yes_no = self.detect_yes_no_question(user_input)
        is_yes_no = False
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        if is_first_message:
            # ç¬¬ä¸€æ¡æ¶ˆæ¯åŒ…å«ç³»ç»Ÿæç¤ºè¯
            prompt_parts = [self.system_prompt]
            
            if is_yes_no:
                prompt_parts.append(self.yes_no_instructions[self.language])
            
            prompt_parts.append(f"Question: {user_input}")
            
            return "\n\n".join(prompt_parts)
        else:
            # åç»­æ¶ˆæ¯åªæ·»åŠ yes/noæŒ‡å¯¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if is_yes_no:
                return f"{self.yes_no_instructions[self.language]}\n\nQuestion: {user_input}"
            else:
                return user_input
    
    def get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return self.system_prompt


class PerplexityCalculator:
    """å›°æƒ‘åº¦è®¡ç®—å™¨"""
    
    def __init__(self, model, processor, device):
        self.model = model
        self.processor = processor
        self.device = device
    
    def calculate_perplexity(
        self, 
        input_text: str, 
        target_text: str,
        images: Optional[List] = None
    ) -> Tuple[float, float]:
        """
        è®¡ç®—å›°æƒ‘åº¦
        
        Args:
            input_text: è¾“å…¥æ–‡æœ¬
            target_text: ç›®æ ‡æ–‡æœ¬
            images: å›¾åƒåˆ—è¡¨
            
        Returns:
            (å›°æƒ‘åº¦, è´Ÿå¯¹æ•°ä¼¼ç„¶)
        """
        try:
            # æ–¹æ³•1: ä½¿ç”¨chat templateæ ¼å¼
            if images is not None:
                # æ„å»ºå¸¦å›¾åƒçš„æ¶ˆæ¯æ ¼å¼
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": input_text}
                    ]
                }]
                
                # åº”ç”¨chat template
                input_formatted = self.processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # å®Œæ•´æ–‡æœ¬ï¼ˆè¾“å…¥+ç›®æ ‡ï¼‰
                full_text = input_formatted + target_text
                
                # å¤„ç†è¾“å…¥
                inputs = self.processor(
                    text=[full_text],
                    images=images,
                    padding=True,
                    return_tensors="pt"
                )
                inputs = inputs.to(self.device)
                
                # è®¡ç®—ä»…è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦
                input_only = self.processor(
                    text=[input_formatted],
                    images=images,
                    padding=True,
                    return_tensors="pt"
                )
                input_length = input_only.input_ids.shape[1]
                
            else:
                # æ— å›¾åƒæƒ…å†µ
                messages = [{
                    "role": "user", 
                    "content": input_text
                }]
                
                input_formatted = self.processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                full_text = input_formatted + target_text
                
                inputs = self.processor(
                    text=[full_text],
                    images=None,
                    padding=True,
                    return_tensors="pt"
                )
                inputs = inputs.to(self.device)
                
                input_only = self.processor(
                    text=[input_formatted],
                    images=None,
                    padding=True,
                    return_tensors="pt"
                )
                input_length = input_only.input_ids.shape[1]
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # è·å–ç›®æ ‡éƒ¨åˆ†çš„logitså’Œlabels
                if input_length >= inputs.input_ids.shape[1]:
                    # å¦‚æœè¾“å…¥é•¿åº¦è¿‡é•¿ï¼Œè¿”å›é»˜è®¤å€¼
                    return float('inf'), float('inf')
                
                target_logits = logits[0, input_length-1:-1, :]  # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
                target_ids = inputs.input_ids[0, input_length:]    # çœŸå®çš„ç›®æ ‡token
                
                if target_logits.shape[0] == 0 or target_ids.shape[0] == 0:
                    return float('inf'), float('inf')
                
                # è®¡ç®—äº¤å‰ç†µæŸå¤±
                loss = F.cross_entropy(target_logits, target_ids, reduction='mean')
                
                # è®¡ç®—å›°æƒ‘åº¦
                perplexity = torch.exp(loss).item()
                nll = loss.item()
                
                return perplexity, nll
                
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œå°è¯•æ–¹æ³•2
            print(f"âš ï¸ Method 1 failed, trying method 2: {e}")
            return self._calculate_perplexity_v2(input_text, target_text, images)
    
    def _calculate_perplexity_v2(
        self, 
        input_text: str, 
        target_text: str,
        images: Optional[List] = None
    ) -> Tuple[float, float]:
        """
        å›°æƒ‘åº¦è®¡ç®—å¤‡ç”¨æ–¹æ³•
        """
        try:
            # æ–¹æ³•2: ç›´æ¥æ‹¼æ¥æ–‡æœ¬
            if images is not None:
                input_formatted = f"<image>\n{input_text}"
                full_text = f"<image>\n{input_text}{target_text}"
            else:
                input_formatted = input_text
                full_text = f"{input_text}{target_text}"
            
            # å¤„ç†å®Œæ•´è¾“å…¥
            inputs = self.processor(
                text=[full_text],
                images=images,
                padding=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.device)
            
            # å¤„ç†ä»…è¾“å…¥éƒ¨åˆ†
            input_only = self.processor(
                text=[input_formatted],
                images=images,
                padding=True,
                return_tensors="pt"
            )
            input_length = input_only.input_ids.shape[1]
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # è·å–ç›®æ ‡éƒ¨åˆ†çš„logitså’Œlabels
                if input_length >= inputs.input_ids.shape[1]:
                    return float('inf'), float('inf')
                
                target_logits = logits[0, input_length-1:-1, :]
                target_ids = inputs.input_ids[0, input_length:]
                
                if target_logits.shape[0] == 0 or target_ids.shape[0] == 0:
                    return float('inf'), float('inf')
                
                # è®¡ç®—äº¤å‰ç†µæŸå¤±
                loss = F.cross_entropy(target_logits, target_ids, reduction='mean')
                
                # è®¡ç®—å›°æƒ‘åº¦
                perplexity = torch.exp(loss).item()
                nll = loss.item()
                
                return perplexity, nll
                
        except Exception as e:
            print(f"âš ï¸ Method 2 also failed: {e}")
            # è¿”å›é»˜è®¤å€¼
            return float('inf'), float('inf')


class ModelInference:
    """æ¨¡å‹æ¨ç†ç±»"""
    
    def __init__(
        self,
        model_name_or_path: str,
        lora_weights_path: Optional[str] = None,
        processor_path: Optional[str] = None,
        device: str = "cuda",
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        torch_dtype: str = "float16",
        load_base_model: bool = True,
        system_prompt: Optional[str] = None,
        language: str = "en"
    ):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_name_or_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            lora_weights_path: LoRAæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            processor_path: Processorè·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨model_name_or_pathï¼‰
            device: è®¾å¤‡
            load_in_8bit: æ˜¯å¦8bité‡åŒ–
            load_in_4bit: æ˜¯å¦4bité‡åŒ–
            torch_dtype: æ•°æ®ç±»å‹
            load_base_model: æ˜¯å¦åŠ è½½åŸå§‹æ¨¡å‹è¿›è¡Œå¯¹æ¯”
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            language: è¯­è¨€è®¾ç½®
        """
        self.device = device
        self.model_name = model_name_or_path
        self.lora_path = lora_weights_path
        self.load_base_model = load_base_model
        
        # åˆå§‹åŒ–Promptå¤„ç†å™¨
        self.prompt_handler = PromptHandler(system_prompt, language)
        
        # è®¾ç½®æ•°æ®ç±»å‹
        dtype_map = {
            "float16": torch.float16,
            "bfloat16": torch.bfloat16,
            "float32": torch.float32
        }
        self.torch_dtype = dtype_map.get(torch_dtype, torch.float16)
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self._load_models_and_processor(
            model_name_or_path,
            lora_weights_path,
            processor_path,
            load_in_8bit,
            load_in_4bit
        )
        
        # åˆå§‹åŒ–å›°æƒ‘åº¦è®¡ç®—å™¨
        self.lora_perplexity_calc = PerplexityCalculator(
            self.lora_model if hasattr(self, 'lora_model') else self.base_model,
            self.processor,
            self.device
        )
        
        if hasattr(self, 'base_model'):
            self.base_perplexity_calc = PerplexityCalculator(
                self.base_model,
                self.processor,
                self.device
            )
    
    def _load_models_and_processor(
        self,
        model_name_or_path: str,
        lora_weights_path: Optional[str],
        processor_path: Optional[str],
        load_in_8bit: bool,
        load_in_4bit: bool
    ):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        print(f"ğŸ”„ Loading base model from: {model_name_or_path}")
        
        # é‡åŒ–é…ç½®
        quantization_config = None
        if load_in_8bit:
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        elif load_in_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=self.torch_dtype,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model_kwargs = {
            "torch_dtype": self.torch_dtype,
            "device_map": "auto",
            "trust_remote_code": True
        }
        
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name_or_path,
            **model_kwargs
        )
        
        # å¦‚æœéœ€è¦å¯¹æ¯”ï¼Œä¿å­˜åŸå§‹æ¨¡å‹
        if self.load_base_model:
            self.base_model = base_model
            self.base_model.eval()
            print("âœ… Base model loaded for comparison")
        
        # åŠ è½½LoRAæ¨¡å‹
        if lora_weights_path:
            print(f"ğŸ”„ Loading LoRA weights from: {lora_weights_path}")
            
            # ä¸ºLoRAåˆ›å»ºå•ç‹¬çš„æ¨¡å‹å®ä¾‹
            if self.load_base_model:
                lora_model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_name_or_path,
                    **model_kwargs
                )
            else:
                lora_model = base_model
            
            self.lora_model = PeftModel.from_pretrained(
                lora_model,
                lora_weights_path,
                torch_dtype=self.torch_dtype
            )
            self.lora_model.eval()
            print("âœ… LoRA model loaded")
        else:
            # åªæœ‰åŸºç¡€æ¨¡å‹
            self.lora_model = None
            if not self.load_base_model:
                self.base_model = base_model
        
        # åŠ è½½å¤„ç†å™¨
        processor_path = processor_path or model_name_or_path
        print(f"ğŸ”„ Loading processor from: {processor_path}")
        self.processor = AutoProcessor.from_pretrained(
            processor_path,
            trust_remote_code=True
        )
        
        print("âœ… Models and processor loaded successfully")
    
    def generate_response(
        self,
        messages: List[Dict[str, str]],
        images: Optional[List] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.1,
        top_p: float = 0.9,
        repetition_penalty: float = 1.0,
        model_type: str = "lora",
        use_custom_prompt: bool = True
    ) -> str:
        """
        ç”Ÿæˆå“åº”
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            images: å›¾åƒåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·å‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½š
            model_type: æ¨¡å‹ç±»å‹ ("lora" æˆ– "base")
            use_custom_prompt: æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # é€‰æ‹©æ¨¡å‹
        if model_type == "base" and hasattr(self, 'base_model'):
            model = self.base_model
        else:
            model = self.lora_model if hasattr(self, 'lora_model') else self.base_model
        
        # å¤„ç†è‡ªå®šä¹‰æç¤ºè¯
        processed_messages = []
        if use_custom_prompt and messages:
            # ä¸ºç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æ·»åŠ ç³»ç»Ÿæç¤ºè¯å’Œæ™ºèƒ½æ£€æµ‹
            first_user_content = messages[0]["content"]
            formatted_content = self.prompt_handler.format_prompt(
                first_user_content, 
                is_first_message=True
            )
            
            processed_messages.append({
                "role": "user",
                "content": formatted_content
            })
            
            # æ·»åŠ å…¶ä½™æ¶ˆæ¯
            for msg in messages[1:]:
                if msg["role"] == "user":
                    formatted_content = self.prompt_handler.format_prompt(
                        msg["content"], 
                        is_first_message=False
                    )
                    processed_messages.append({
                        "role": "user",
                        "content": formatted_content
                    })
                else:
                    processed_messages.append(msg)
        else:
            processed_messages = messages
        
        # ä¿®å¤æ¶ˆæ¯æ ¼å¼ä»¥åŒ…å«å›¾åƒä¿¡æ¯
        if images is not None:
            formatted_messages = []
            for i, msg in enumerate(processed_messages):
                if msg["role"] == "user":
                    # ä¸ºç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯æ·»åŠ å›¾åƒ
                    if i == 0:
                        formatted_messages.append({
                            "role": "user",
                            "content": [
                                {"type": "image"},
                                {"type": "text", "text": msg["content"]}
                            ]
                        })
                    else:
                        formatted_messages.append({
                            "role": "user",
                            "content": msg["content"]
                        })
                else:
                    formatted_messages.append(msg)
            
            processed_messages = formatted_messages
        
        # å‡†å¤‡è¾“å…¥
        text = self.processor.apply_chat_template(
            processed_messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.processor(
            text=[text],
            images=images,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to(self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                do_sample=temperature > 0.0
            )
        
        # åªè·å–ç”Ÿæˆçš„éƒ¨åˆ†
        generated_ids = generated_ids[:, inputs.input_ids.shape[1]:]
        generated_text = self.processor.batch_decode(
            generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        return generated_text
    
    def run_inference(
        self,
        dataset: SupervisedDataset,
        output_dir: str,
        max_samples: Optional[int] = None,
        max_new_tokens: int = 512,
        calculate_perplexity: bool = True,
        run_lora: bool = True,
        run_base: bool = True,
        use_custom_prompt: bool = True
    ) -> None:
        """
        è¿è¡Œæ¨ç†å¹¶ä¿å­˜ç»“æœ
        
        Args:
            dataset: è¯„ä¼°æ•°æ®é›†
            output_dir: è¾“å‡ºç›®å½•
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            calculate_perplexity: æ˜¯å¦è®¡ç®—å›°æƒ‘åº¦
            run_lora: æ˜¯å¦è¿è¡ŒLoRAæ¨¡å‹
            run_base: æ˜¯å¦è¿è¡ŒåŸºç¡€æ¨¡å‹
            use_custom_prompt: æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡è¯„ä¼°
        total_samples = len(dataset)
        if max_samples:
            total_samples = min(total_samples, max_samples)
        
        print(f"ğŸ“Š Running inference on {total_samples} samples...")
        if use_custom_prompt:
            print(f"ğŸ¤– Using custom prompt: {self.prompt_handler.language} language")
            print(f"ğŸ“ System prompt preview: {self.prompt_handler.get_system_prompt()[:100]}...")
        
        # æ”¶é›†ç»“æœ
        inference_results = {
            "metadata": {
                "base_model": self.model_name,
                "lora_weights": self.lora_path,
                "inference_time": datetime.now().isoformat(),
                "total_samples": total_samples,
                "run_lora": run_lora,
                "run_base": run_base,
                "calculate_perplexity": calculate_perplexity,
                "use_custom_prompt": use_custom_prompt,
                "system_prompt": self.prompt_handler.get_system_prompt() if use_custom_prompt else None,
                "language": self.prompt_handler.language if use_custom_prompt else "default"
            },
            "results": []
        }
        
        # è¿›åº¦æ¡
        pbar = tqdm(range(total_samples), desc="Running inference")
        
        for idx in pbar:
            try:
                # è·å–æ•°æ®
                item = dataset.list_data_dict[idx]
                item_id = dataset.get_item_id(idx)
                
                # æå–å¯¹è¯å’Œå›¾åƒ
                conversations = item.get("conversations", [])
                if not conversations:
                    continue
                
                # æ„å»ºæ¶ˆæ¯
                messages = []
                reference = ""
                
                for i in range(0, len(conversations), 2):
                    if i < len(conversations) - 1:
                        user_msg = conversations[i]
                        assistant_msg = conversations[i + 1]
                        
                        # æ¸…ç†ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å›¾åƒtoken
                        user_content = user_msg["value"]
                        if user_content.startswith("<image>\n"):
                            user_content = user_content[8:]
                        
                        messages.append({
                            "role": "user",
                            "content": user_content
                        })
                        
                        if i == len(conversations) - 2:
                            reference = assistant_msg["value"]
                
                # å¤„ç†å›¾åƒ
                images = None
                if "image" in item:
                    image_path = item["image"]
                    if not os.path.isabs(image_path):
                        image_path = os.path.join(dataset.data_args.image_folder, image_path)
                    
                    if os.path.exists(image_path):
                        images = [Image.open(image_path)]
                
                # å‡†å¤‡ç»“æœè®°å½•
                result_record = {
                    "item_id": item_id,
                    "messages": messages,
                    "reference": reference,
                    "has_image": images is not None
                }
                
                # æ£€æµ‹æ˜¯å¦ä¸ºyes/noé—®é¢˜
                if use_custom_prompt and messages:
                    result_record["is_yes_no_question"] = self.prompt_handler.detect_yes_no_question(
                        messages[0]["content"]
                    )
                
                # æå–è¾“å…¥æ–‡æœ¬ç”¨äºå›°æƒ‘åº¦è®¡ç®—
                input_text = messages[0]["content"] if messages else ""
                
                # è¿è¡ŒLoRAæ¨¡å‹æ¨ç†
                if run_lora and (hasattr(self, 'lora_model') or not self.lora_path):
                    lora_prediction = self.generate_response(
                        messages=messages,
                        images=images,
                        max_new_tokens=max_new_tokens,
                        temperature=0.1,
                        model_type="lora",
                        use_custom_prompt=use_custom_prompt
                    )
                    result_record["lora_prediction"] = lora_prediction
                    
                    # è®¡ç®—LoRAå›°æƒ‘åº¦
                    if calculate_perplexity:
                        lora_ppl, lora_nll = self.lora_perplexity_calc.calculate_perplexity(
                            input_text, reference, images
                        )
                        result_record["lora_perplexity"] = lora_ppl
                        result_record["lora_nll"] = lora_nll
                
                # è¿è¡ŒåŸºç¡€æ¨¡å‹æ¨ç†
                if run_base and hasattr(self, 'base_model'):
                    base_prediction = self.generate_response(
                        messages=messages,
                        images=images,
                        max_new_tokens=max_new_tokens,
                        temperature=0.1,
                        model_type="base",
                        use_custom_prompt=use_custom_prompt
                    )
                    result_record["base_prediction"] = base_prediction
                    
                    # è®¡ç®—åŸºç¡€æ¨¡å‹å›°æƒ‘åº¦
                    if calculate_perplexity:
                        base_ppl, base_nll = self.base_perplexity_calc.calculate_perplexity(
                            input_text, reference, images
                        )
                        result_record["base_perplexity"] = base_ppl
                        result_record["base_nll"] = base_nll
                
                # æ·»åŠ åˆ°ç»“æœ
                inference_results["results"].append(result_record)
                
                # æ›´æ–°è¿›åº¦æ¡
                info = {"item_id": item_id}
                if "lora_prediction" in result_record:
                    info["lora_len"] = len(result_record["lora_prediction"].split())
                if "base_prediction" in result_record:
                    info["base_len"] = len(result_record["base_prediction"].split())
                if use_custom_prompt and "is_yes_no_question" in result_record:
                    info["yes_no"] = result_record["is_yes_no_question"]
                pbar.set_postfix(info)
                
            except Exception as e:
                print(f"\nâš ï¸ Error processing item {idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ä¿å­˜ç»“æœ
        output_file = os.path.join(output_dir, "inference_results.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(inference_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Saved inference results to: {output_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_inference_summary(inference_results)
    
    def _print_inference_summary(self, results: Dict[str, Any]):
        """æ‰“å°æ¨ç†æ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“Š INFERENCE SUMMARY")
        print("="*50)
        
        total_results = len(results["results"])
        print(f"Total samples processed: {total_results}")
        
        # ç»Ÿè®¡yes/noé—®é¢˜
        if results["metadata"]["use_custom_prompt"]:
            yes_no_questions = [r for r in results["results"] if r.get("is_yes_no_question", False)]
            print(f"Yes/No questions detected: {len(yes_no_questions)}")
        
        # ç»Ÿè®¡LoRAç»“æœ
        lora_results = [r for r in results["results"] if "lora_prediction" in r]
        if lora_results:
            print(f"\nLoRA model:")
            print(f"  - Predictions generated: {len(lora_results)}")
            
            if any("lora_perplexity" in r for r in lora_results):
                valid_ppls = [r["lora_perplexity"] for r in lora_results 
                             if "lora_perplexity" in r and not math.isinf(r["lora_perplexity"])]
                if valid_ppls:
                    print(f"  - Average perplexity: {np.mean(valid_ppls):.4f}")
                    print(f"  - Median perplexity: {np.median(valid_ppls):.4f}")
        
        # ç»Ÿè®¡åŸºç¡€æ¨¡å‹ç»“æœ
        base_results = [r for r in results["results"] if "base_prediction" in r]
        if base_results:
            print(f"\nBase model:")
            print(f"  - Predictions generated: {len(base_results)}")
            
            if any("base_perplexity" in r for r in base_results):
                valid_ppls = [r["base_perplexity"] for r in base_results 
                             if "base_perplexity" in r and not math.isinf(r["base_perplexity"])]
                if valid_ppls:
                    print(f"  - Average perplexity: {np.mean(valid_ppls):.4f}")
                    print(f"  - Median perplexity: {np.median(valid_ppls):.4f}")
        
        print("="*50)


def main():
    parser = argparse.ArgumentParser(description="Model inference and perplexity calculation with custom prompts")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_name_or_path", type=str, 
                        default=None,
                       help="Path to base model")
    parser.add_argument("--lora_weights", type=str, 
                        default=None
    parser.add_argument("--processor_path", type=str, default=None,
                       help="Path to processor (default: same as model)")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_path", type=str, 
                        default='data_path/test_data/slake_test.json',
                       help="Path to evaluation data JSON")
    parser.add_argument("--image_folder", type=str, default="",
                       help="Base folder for images")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--max_samples", type=int, default=None,
                        help="Maximum number of samples to process")
    parser.add_argument("--max_new_tokens", type=int, default=512,
                        help="Maximum number of tokens to generate")
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument("--calculate_perplexity", action="store_true", default=True,
                        help="Calculate perplexity for each sample")
    parser.add_argument("--no_perplexity", dest="calculate_perplexity", action="store_false",
                        help="Don't calculate perplexity")
    parser.add_argument("--run_lora", action="store_true", default=True,
                        help="Run LoRA model inference")
    parser.add_argument("--no_lora", dest="run_lora", action="store_false",
                        help="Don't run LoRA model")
    parser.add_argument("--run_base", action="store_true", default=True,
                        help="Run base model inference")
    parser.add_argument("--no_base", dest="run_base", action="store_false",
                        help="Don't run base model")
    
    # æ–°å¢ï¼šè‡ªå®šä¹‰æç¤ºè¯å‚æ•°
    parser.add_argument("--use_custom_prompt", action="store_true", default=True,
                        help="Use custom system prompt with intelligent yes/no detection")
    parser.add_argument("--no_custom_prompt", dest="use_custom_prompt", action="store_false",
                        help="Don't use custom prompt")
    parser.add_argument("--system_prompt", type=str, 
                        # default="You are a professional medical assistant with extensive knowledge...",
                        default = None,
                        help="Custom system prompt (if not provided, uses default medical assistant prompt)")
    parser.add_argument("--language", type=str, default="en", choices=["en", "zh"],
                        help="Language for prompts and yes/no detection (en/zh)")
    
    # æ¨¡å‹åŠ è½½å‚æ•°
    parser.add_argument("--load_in_8bit", action="store_true",
                        help="Load model in 8-bit quantization")
    parser.add_argument("--load_in_4bit", action="store_true",
                        help="Load model in 4-bit quantization")
    parser.add_argument("--torch_dtype", type=str, default="bfloat16",
                        choices=["float16", "bfloat16", "float32"],
                        help="Torch dtype for model")
    
    # å›¾åƒå‚æ•°
    parser.add_argument("--image_min_pixels", type=int, default=336*336,
                        help="Minimum pixels for images")
    parser.add_argument("--image_max_pixels", type=int, default=1024*1024,
                        help="Maximum pixels for images")
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not args.run_lora and not args.run_base:
        parser.error("At least one of --run_lora or --run_base must be enabled")
    
    if args.run_base and not args.lora_weights:
        args.run_base = False
        print("âš ï¸ No LoRA weights provided, will only run base model")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if args.lora_weights:
        args.output_dir = os.path.join(os.path.dirname(args.lora_weights)+'_result_ppl_propmt', args.lora_weights.split('/')[-1])
    else:
        args.output_dir = os.path.join(os.path.dirname(args.model_name_or_path), 'base_model_evaluation')
    
    # è¯»å–è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶ï¼ˆå¦‚æœæä¾›çš„æ˜¯æ–‡ä»¶è·¯å¾„ï¼‰
    if args.system_prompt and os.path.isfile(args.system_prompt):
        with open(args.system_prompt, 'r', encoding='utf-8') as f:
            args.system_prompt = f.read().strip()
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    print("ğŸš€ Initializing model inference...")
    inferencer = ModelInference(
        model_name_or_path=args.model_name_or_path,
        lora_weights_path=args.lora_weights,
        processor_path=args.processor_path,
        load_in_8bit=args.load_in_8bit,
        load_in_4bit=args.load_in_4bit,
        torch_dtype=args.torch_dtype,
        load_base_model=args.run_base,
        system_prompt=args.system_prompt,
        language=args.language
    )
    
    # å‡†å¤‡æ•°æ®å‚æ•°
    data_args = DataArguments(
        data_path=args.data_path,
        image_folder=args.image_folder,
        image_min_pixels=args.image_min_pixels,
        image_max_pixels=args.image_max_pixels,
        video_min_pixels=args.image_min_pixels,
        video_max_pixels=args.image_max_pixels,
        fps=8
    )
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“‚ Loading dataset...")
    dataset = SupervisedDataset(
        data_path=args.data_path,
        processor=inferencer.processor,
        data_args=data_args,
        model_id=args.model_name_or_path,
        padding=False
    )
    print(f"âœ… Loaded {len(dataset)} samples")
    
    # è¿è¡Œæ¨ç†
    print("ğŸƒ Starting inference...")
    start_time = time.time()
    
    inferencer.run_inference(
        dataset=dataset,
        output_dir=args.output_dir,
        max_samples=args.max_samples,
        max_new_tokens=args.max_new_tokens,
        calculate_perplexity=args.calculate_perplexity,
        run_lora=args.run_lora,
        run_base=args.run_base,
        use_custom_prompt=args.use_custom_prompt
    )
    
    elapsed_time = time.time() - start_time
    print(f"\nâ±ï¸ Inference completed in {elapsed_time:.2f} seconds")
    print(f"ğŸ“ Results saved to: {args.output_dir}")


if __name__ == "__main__":
    main()