# Qwen-VL-Med-SFT
![Qwen-VL-Med-SFT æµç¨‹](flowchart0.png)
ä¸€ä¸ªåŸºäº Qwen2-VL ç³»åˆ—æ¨¡å‹çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„å¤šæ¨¡æ€ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®åŸºäº [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) æ¶æ„ï¼Œä½¿ç”¨ HuggingFace Transformers å’Œ DeepSpeed è¿›è¡Œè®­ç»ƒã€‚åœ¨åŸæœ‰åŸºç¡€ä¸Šå¢å¼ºäº†ä»¥ä¸‹åŠŸèƒ½ï¼š

- âœ… è®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯æ¨¡å—
- âœ… æ•°æ®é¢„å¤„ç†æ£€æŸ¥
- âœ… æ¨¡å‹æ•ˆæœéªŒè¯
- âœ… ç»“æœç»Ÿè®¡å¯¹æ¯”åˆ†æ

é¡¹ç›®é‡‡ç”¨ [LLaVA-Med](https://github.com/microsoft/LLaVA-Med) æ•°æ®é›†ï¼Œå®ç°ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼š**æ¦‚å¿µå¯¹é½**ï¼ˆConcept Alignmentï¼‰å’Œ**æŒ‡ä»¤è·Ÿéš**ï¼ˆInstruction Followingï¼‰ã€‚

## ğŸ“Š æ•°æ®é¢„å¤„ç†æµç¨‹

### è®­ç»ƒæ•°æ®å¤„ç†

1. **æ•°æ®æ ¼å¼è½¬æ¢**
   - å‚è€ƒ [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) ç”Ÿæˆæ ‡å‡† JSON æ–‡ä»¶
   - æŒ‰ç…§ DataLoader çš„æ•°æ®å¤„ç†æ–¹å¼è¿›è¡Œè¿­ä»£éªŒè¯ï¼Œç­›é€‰æœ‰æ•ˆæ•°æ®

2. **éªŒè¯æ•°æ®å¤„ç†**
   - ä½¿ç”¨ `./data_process/data_trans.py` è„šæœ¬å¤„ç†
   - ç”Ÿæˆ `test.json` å’Œ `type_mapping.json` ç”¨äºæµ‹è¯•å’Œåˆ†ç±»

### æ•°æ®åˆ†ç±»ä½“ç³»

æ ¹æ®é—®é¢˜ç±»å‹ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºä»¥ä¸‹ç±»åˆ«ï¼š

#### å°é—­å¼é—®é¢˜ï¼ˆClose-setï¼‰

| ç±»å‹ | ä»»åŠ¡æè¿° |
|------|----------|
| **Yes/No åˆ¤æ–­** | å¯¹åŒ»å­¦å½±åƒä¸­ç‰¹å®šç‰¹å¾æˆ–å¼‚å¸¸çš„å­˜åœ¨æ€§è¿›è¡ŒäºŒå…ƒåˆ¤æ–­ |
| **æ¨¡æ€è¯†åˆ«** | è¯†åˆ«åŒ»å­¦å½±åƒçš„æˆåƒæ–¹å¼æˆ–æŠ€æœ¯ç±»å‹ |
| **é€šç”¨é—®é¢˜** | ä¸å±äºç‰¹å®šç±»åˆ«çš„å°é—­å¼é—®é¢˜ |
| **ä½ç½®å®šä½** | è¯¢é—®ç—…å˜æˆ–ç»“æ„çš„å…·ä½“ä½ç½®ï¼ˆé™å®šé€‰é¡¹ï¼‰ |

#### å¼€æ”¾å¼é—®é¢˜ï¼ˆOpen-endï¼‰

| ç±»å‹ | ä»»åŠ¡æè¿° |
|------|----------|
| **é€šç”¨æè¿°** | éœ€è¦ç»¼åˆæè¿°æˆ–è§£é‡Šçš„å¼€æ”¾æ€§é—®é¢˜ |
| **è§£å‰–è¯†åˆ«** | è¯†åˆ«å’Œæè¿°å½±åƒä¸­çš„è§£å‰–ç»“æ„æˆ–å™¨å®˜ |
| **ä½ç½®æè¿°** | è¯¦ç»†æè¿°ç—…å˜æˆ–ç»“æ„çš„ä½ç½® |
| **å¼‚å¸¸è¯†åˆ«** | è¯†åˆ«å’Œæè¿°ç—…ç†æ”¹å˜æˆ–å¼‚å¸¸å‘ç° |
| **è®¡æ•°ä»»åŠ¡** | è®¡ç®—å½±åƒä¸­ç‰¹å®šå¯¹è±¡çš„æ•°é‡ |
| **æ¯”è¾ƒåˆ†æ** | æ¯”è¾ƒä¸åŒç»“æ„æˆ–æ—¶é—´ç‚¹çš„å˜åŒ– |
| **å¤–è§‚æè¿°** | æè¿°ç—…å˜æˆ–ç»“æ„çš„è§†è§‰ç‰¹å¾ |
| **å½±å“è¯„ä¼°** | è¯„ä¼°ç—…å˜å¯¹å‘¨å›´ç»“æ„çš„å½±å“ |

## ğŸš€ æ¨¡å‹è®­ç»ƒ

### è®­ç»ƒæ¶æ„

åŸºäº HuggingFace Transformers çš„ `Trainer` ç±»è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡ `trainer.add_callback(callback)` æ–¹å¼é›†æˆï¼š
- éªŒè¯æµç¨‹å›è°ƒ
- TensorBoard æ—¥å¿—è®°å½•
- è®­ç»ƒè¿‡ç¨‹ç›‘æ§

### å¯åŠ¨è®­ç»ƒ

#### å•å¡è®­ç»ƒ/è°ƒè¯•
```bash
bash finetune_lora_single_gpu.sh
```

#### å¤šå¡è®­ç»ƒï¼ˆDeepSpeedï¼‰
```bash
bash finetune_lora_mult_gpu.sh
```

## ğŸ§ª æ¨¡å‹æµ‹è¯•ä¸è¯„ä¼°

### æµ‹è¯•é…ç½®

æ”¯æŒä¸¤ç§æµ‹è¯•æ¨¡å¼ï¼š
- **è‡ªå®šä¹‰ Prompt æµ‹è¯•**ï¼šä½¿ç”¨è‡ªè®¾å®šçš„æç¤ºè¯
- **åŸå§‹ System Prompt æµ‹è¯•**ï¼šä½¿ç”¨æ¨¡å‹é»˜è®¤æç¤ºè¯

æµ‹è¯•è¿‡ç¨‹ä¼šè®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼Œå¹¶å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œ LoRA æ¨¡å‹çš„ç»“æœã€‚

### å¯åŠ¨æµ‹è¯•
```bash
bash run_eval.sh
```

### ç»“æœç»Ÿè®¡åˆ†æ

ä½¿ç”¨ `result_statistic.py` è„šæœ¬è¿›è¡Œåˆ†ç±»åˆ«ç»Ÿè®¡åˆ†æã€‚

#### è¯„ä¼°æŒ‡æ ‡ä½“ç³»

| æŒ‡æ ‡åç§° | å€¼åŸŸèŒƒå›´ | åˆç†åˆ†æ•°åŒºé—´ | å«ä¹‰è§£é‡Š | è¯´æ˜ |
|----------|----------|--------------|----------|------|
| **BLEU-4** | 0 ~ 1 | 0.2 ~ 0.6 | n-gram ç²¾ç¡®åŒ¹é…ç‡ | è¡¡é‡å±€éƒ¨è¯­è¨€åŒ¹é…ï¼Œæƒ©ç½šé‡å¤ã€ç¼ºè¯ |
| **ROUGE-L** | 0 ~ 1 | 0.3 ~ 0.6 | æœ€é•¿å…¬å…±å­åºåˆ—çš„å¬å›ç‡ | é‡è§†ä¿¡æ¯è¦†ç›–ï¼Œåå‘å¬å›ç‡ |
| **METEOR** | 0 ~ 1 | 0.3 ~ 0.5 | ç²¾ç¡®åº¦ + å¬å› + è¯­ä¹‰åŒä¹‰è¯ + è¯åºæƒ©ç½šç»¼åˆæŒ‡æ ‡ | å®½å®¹è¡¨è¾¾å·®å¼‚ï¼Œé€‚åˆç”Ÿæˆå¼ä»»åŠ¡ |
| **CIDEr** | 0 ~ âˆ | 0.5 ~ 2.0 | åŸºäº TF-IDF çš„åŠ æƒ n-gram åŒ¹é…è¯„åˆ† | å¤šå‚è€ƒæ—¶é²æ£’ï¼Œå¸¸ç”¨äºå›¾æ–‡ä»»åŠ¡ |
| **BERTScore (F1)** | 0 ~ 1 | 0.85 ~ 0.95 | åŸºäº BERT çš„å¥å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦ | è¡¨è¾¾çµæ´»æ—¶ä¾æ—§æœ‰æ•ˆ |
| **Soft Matching** | 0 ~ 1 | 0.6 ~ 0.95 | å­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼ˆå¦‚ SequenceMatcher/LCS æ¯”ä¾‹ï¼‰ | æ•æ‰å­—ç¬¦ä¸²çš„éƒ¨åˆ†ç›¸ä¼¼ï¼Œå®¹é”™æ€§å¼º |
| **Substring Match** | 0 æˆ– 1 | äºŒå€¼å‹ï¼ˆ0/1ï¼‰ | æ˜¯å¦å®Œå…¨åŒ…å«ï¼ˆå‚è€ƒ âˆˆ é¢„æµ‹ æˆ–åä¹‹ï¼‰ | éå¸¸ä¸¥æ ¼ï¼Œé€‚ç”¨äºç­”æ¡ˆçŸ­æ˜ç¡®åœºæ™¯ |

### ğŸ“ˆ å®éªŒç»“æœ

#### Qwen2-VL ç”Ÿç‰©åŒ»ç–—é—®ç­”å¾®è°ƒä¸æ€§èƒ½åˆ†æ

*  å¾®è°ƒæ–¹æ¡ˆä¸å®éªŒè®¾ç½®

* æ¦‚å¿µå¯¹é½å¾®è°ƒï¼ˆStage 1ï¼‰
- åŸºäº Qwen2-VL-Instruct 2B ä¸ 7B æ¨¡å‹
- ä½¿ç”¨çº¦ 16 ä¸‡æ¡å¤šæ¨¡æ€å¯¹é½æ•°æ®ï¼Œä¸»è¦é‡‡ç”¨ LoRA æŠ€æœ¯å¾®è°ƒè§†è§‰-è¯­è¨€èåˆæ¨¡å—ï¼ˆvisual-merger-projï¼‰

* Instruct SFT å¾®è°ƒï¼ˆStage 2ï¼‰
- é‡‡ç”¨ LoRA å¾®è°ƒ attention å±‚ FNN éƒ¨åˆ†ï¼Œæå‡ä¿¡æ¯æ•´åˆèƒ½åŠ›
- **æ–¹æ¡ˆ 1**ï¼šåŸºäº LLaVA-Med Instruct æ•°æ®é›†ï¼ˆ16Kï¼‰ï¼Œ3 è½®è®­ç»ƒï¼Œæ˜¾è‘—æå‡å›å¤è´¨é‡ï¼Œå›°æƒ‘åº¦ä¸‹é™ï¼ŒWord F1 æé«˜
- **æ–¹æ¡ˆ 2**ï¼šåŸºäº Slake è®­ç»ƒé›†ï¼ˆçº¦ 5Kï¼‰ï¼Œ3 è½®è®­ç»ƒï¼Œå›å¤é£æ ¼æ›´ç®€æ´ï¼ŒWord F1 æ˜¾è‘—æå‡

- **æµ‹è¯•é›†**ï¼šSlake ç”Ÿç‰©åŒ»å­¦è§†è§‰é—®ç­”æ•°æ®é›†

#### ç»“æœæ€»ç»“

- **æ¦‚å¿µå¯¹é½å¾®è°ƒ**ï¼šæœ‰åŠ©äºè§†è§‰ä¸è¯­è¨€ä¿¡æ¯çš„ä¸“ä¸šé¢†åŸŸå¯¹é½ï¼Œå¢å¼ºæ¨¡å‹åœ¨åŒ»å­¦é—®ç­”ä¸­çš„è¡¨ç°
- **Instruct SFT**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹å›å¤é£æ ¼ä¸è¯­è¨€è¡¨è¾¾
- **æ¨¡å‹å±€é™**ï¼š
  - ç—…å˜ä½ç½®ç¡®è®¤ä¸ç›¸å¯¹ä½ç½®è¡¨è¿°èƒ½åŠ›ä»å¾…æå‡
  - å¯¹éƒ¨åˆ†ç—…ç—‡æœ‰â€œè½»åº¦â€æè¿°å€¾å‘
* è¡¨æ ¼è¯´æ˜ï¼š

| æ¨¡å‹ç®€ç§°                            | è®­ç»ƒæ–¹å¼åŠè¯´æ˜                                                |
| ----------------------------------- | ------------------------------------------------------------ |
| Stage1               | Qwen2-VL-2B instructï¼Œæ¦‚å¿µå¯¹é½å¾®è°ƒ        |
| Stage1CTL                | Qwen2-VL-2B instruct æ¦‚å¿µå¯¹é½å¾®è°ƒ  context_learning       |
| Stage2               | Qwen2-VL-2B instructï¼Œæ¦‚å¿µå¯¹é½å¾®è°ƒ+æŒ‡ä»¤å¾®è°ƒ       |
| 7BOri                       | Qwen2-VL-7B instructï¼Œæ— å¾®è°ƒ                   |
| Stage1Reasoning                           | Qwen2-VL-7B æ¦‚å¿µå¯¹é½å¾®è°ƒ  Reasoning_SFT                      |
| Stage1ReasoningCTL                | Qwen2-VL-7B æ¦‚å¿µå¯¹é½å¾®è°ƒ  Reasoning_SFT  context Learning             |
| Qwen25VL32BInst                     | Qwen2-VL-32B instruct æ— è®­ç»ƒ                   |

- **Reasoningè®­ç»ƒ**ï¼šé’ˆå¯¹æ¯ä¸ªQAå¯¹æ·»åŠ äº†Reasoning promptï¼Œä½¿ç”¨Qwen3æ ¹æ®LLaVA Med QAè®­ç»ƒé›†ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå°†é—®é¢˜çš„answerä½œä¸ºè¾“å…¥ï¼Œç”ŸæˆReasoningå›ç­”å’ŒDirectlyå›ç­”ï¼Œæ„å»ºæ ·æœ¬é›†ã€‚

#### æ€§èƒ½å¯¹æ¯”
* **æ•´ä½“æ€§èƒ½** 

è¯¦ç»†ç»“æœè§ [`result/model_comparison_tables.txt`](result/model_comparison_tables.txt)ï¼Œ  
ä¸ªä¾‹ç»Ÿè®¡è§ [`result/evaluation_metrics.json`](result/evaluation_metrics.json)

| Category | Metric | Stage1CTL | Stage1 | Stage2 | Stage1Reasoning | Stage1ReasoningCTL | 7BOriCTL | 7BOri | Qwen25VL32BInst |
|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| Text Matching | Exact Match | 0.4807 | **0.4816** | 0.4722 | 0.0452 | 0.2413 | 0.2705 | 0.0000 | 0.0000 |
| Text Matching | Soft Match | 0.5822 | **0.5835** | 0.5791 | 0.3156 | 0.4713 | 0.3630 | 0.0960 | 0.0037 |
| Text Matching | ROUGE-L | 0.5267 | **0.5285** | 0.5125 | 0.3200 | 0.4623 | 0.3331 | 0.0897 | 0.0156 |
| Text Matching | BLEU-4 | 0.0916 | **0.0920** | 0.0899 | 0.0432 | 0.0736 | 0.0540 | 0.0082 | 0.0009 |
| Text Matching | Word Overlap | 0.5077 | **0.5088** | 0.4954 | 0.2514 | 0.4189 | 0.3092 | 0.0565 | 0.0125 |
| Text Matching | BERTScore F1 | **0.0000** | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |
| Fine-grained | Word Contain | 0.5938 | 0.5957 | 0.5533 | 0.5749 | 0.6136 | 0.6051 | 0.5975 | **0.7135** |
| Fine-grained | Char F1 | 0.6554 | **0.6569** | 0.6247 | 0.4249 | 0.5550 | 0.5466 | 0.3525 | 0.3236 |
| Fine-grained | Word F1 | 0.5198 | **0.5211** | 0.5052 | 0.3088 | 0.4533 | 0.3362 | 0.0994 | 0.0244 |
| Fine-grained | Char Precision | 0.6541 | **0.6557** | 0.6370 | 0.3581 | 0.5132 | 0.4875 | 0.2581 | 0.2264 |
| Fine-grained | Char Recall | 0.6958 | 0.6992 | 0.6383 | 0.7208 | 0.7159 | 0.7640 | 0.7999 | **0.8596** |
| Fine-grained | Word Precision | 0.5176 | **0.5192** | 0.5095 | 0.2628 | 0.4314 | 0.3106 | 0.0574 | 0.0125 |
| Fine-grained | Word Recall | 0.5490 | 0.5497 | 0.5154 | 0.5200 | 0.5667 | 0.5457 | 0.5403 | **0.6631** |
| Perplexity | Mean Perplexity | 278.8062 | 278.8062 | **173.8327** | 598.1220 | 598.1220 | 240741.5970 | 240741.5970 | 694010380428.6918 |
| Perplexity | Median Perplexity | 3.8906 | 3.8906 | **2.8906** | 3.7969 | 3.7969 | 216.0000 | 216.0000 | 13376.0000 |
| Perplexity | Std Perplexity | 2424.2811 | 2424.2811 | **1582.0417** | 6534.2893 | 6534.2893 | 1523977.9827 | 1523977.9827 | 4472871553015.6406 |
| Perplexity | Min Perplexity | 1.0391 | 1.0391 | 1.0156 | 1.0156 | 1.0156 | **1.0000** | 1.0000 | 1.0000 |
| Perplexity | Max Perplexity | 52736.0000 | 52736.0000 | **32000.0000** | 119296.0000 | 119296.0000 | 21364736.0000 | 21364736.0000 | 61572651155456.0000 |

## Open-ended Questions

| Category | Metric | Stage1CTL | Stage1 | Stage2 | Stage1Reasoning | Stage1ReasoningCTL | 7BOriCTL | 7BOri | Qwen25VL32BInst |
|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| Text Matching | Exact Match | **0.3371** | 0.3371 | 0.3329 | 0.0609 | 0.2266 | 0.1601 | 0.0000 | 0.0000 |
| Text Matching | Soft Match | 0.4866 | 0.4872 | **0.4934** | 0.3782 | 0.4627 | 0.2859 | 0.1123 | 0.0029 |
| Text Matching | ROUGE-L | 0.4007 | 0.4019 | 0.3934 | 0.3520 | **0.4120** | 0.2351 | 0.0844 | 0.0167 |
| Text Matching | BLEU-4 | 0.0682 | **0.0685** | 0.0681 | 0.0491 | 0.0664 | 0.0355 | 0.0076 | 0.0009 |
| Text Matching | Word Overlap | 0.3734 | **0.3737** | 0.3677 | 0.2771 | 0.3616 | 0.2055 | 0.0531 | 0.0133 |
| Text Matching | BERTScore F1 | **0.0000** | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |
| Fine-grained | Word Contain | 0.4986 | 0.5000 | 0.4547 | 0.5297 | 0.5326 | 0.5368 | 0.5467 | **0.7394** |
| Fine-grained | Char F1 | 0.5964 | **0.5971** | 0.5626 | 0.4896 | 0.5609 | 0.5315 | 0.4205 | 0.3947 |
| Fine-grained | Word F1 | 0.3902 | 0.3908 | 0.3824 | 0.3319 | **0.3969** | 0.2381 | 0.0930 | 0.0259 |
| Fine-grained | Char Precision | 0.5957 | **0.5965** | 0.5812 | 0.4360 | 0.5317 | 0.4598 | 0.3234 | 0.2873 |
| Fine-grained | Char Recall | 0.6512 | 0.6549 | 0.5825 | 0.6708 | 0.6685 | 0.7501 | 0.7706 | **0.8794** |
| Fine-grained | Word Precision | 0.3883 | **0.3893** | 0.3889 | 0.2942 | 0.3802 | 0.2076 | 0.0546 | 0.0133 |
| Fine-grained | Word Recall | 0.4312 | 0.4310 | 0.3978 | 0.4472 | 0.4621 | 0.4475 | 0.4607 | **0.6637** |


* æ€»ä½“ç»“æœåˆ†æ
- **å¸¸è§„æµ‹è¯•**ï¼šQwenç³»åˆ—æ¨¡å‹å¯¹åŒ»ç–—çš„æ–‡å­—æœ‰è§£ç­”å’Œå›ç­”çš„èƒ½åŠ›ï¼Œä½†æ˜¯å¯¹äºåŒ»ç–—å›¾åƒè®¤çŸ¥ç¨æ˜¾ä¸è¶³ï¼Œå¯¹äºå¸¸è§„çš„MRIã€X-Rayç­‰æœ‰ä¸€å®šè®¤çŸ¥ï¼Œä½†æ˜¯å¯¹äºç—…ç¶ã€ç—…ä¾‹åˆ¤æ–­ä¸è¶³ï¼Œå¯¹äºç”Ÿç‰©ç»„ç»‡ã€ç»†èƒå›¾çš„è®¤è¯†ç¨å¾®å·®ä¸€äº›
- **Reasoning**ï¼šReasoningè™½ç„¶æ²¡æœ‰å®Œå…¨ç”Ÿæˆéµå¾ªReasoning Propmtçš„èƒ½åŠ›ï¼Œä½†æ˜¯æ•°æ®é›†ç›¸å¯¹åº”Stage2æ˜¯ä¸€ä¸ªæ›´é«˜è´¨é‡ï¼Œå›ç­”æ›´åŠ ç®€æ´ä¿¡æ¯çš„æ•°æ®é›†ï¼Œå› æ­¤åœ¨Open-endedæ•°æ®é›†çš„ROUGE-Lç»Ÿè®¡ä¸Šï¼Œæ•ˆæœæœ‰äº†ä¸€ä¸ªè¾ƒå¤§çš„æå‡ï¼Œè¯´æ˜é€šè¿‡æŒ‡ä»¤SFTå¯ä»¥æå‡æ¨¡å‹å¯¹äºä¸“ä¸šé¢†åŸŸä»»åŠ¡çš„ç†è§£
- **å°é—­å¼é—®é¢˜**ï¼šLoRA æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯ Word F1 ä» 0.1121 æå‡åˆ° 0.7493
- **å¼€æ”¾å¼é—®é¢˜**ï¼šWord F1 ä» 0.0930 æå‡åˆ° 0.3824
- **æ•´ä½“æ€§èƒ½**ï¼šWord F1 ä» 0.0994 æå‡åˆ° 0.5052ï¼ŒLoRA å¾®è°ƒåæ¨¡å‹åœ¨åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´å¥½çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›

* ç»“æœæƒ…å†µï¼šï¼ˆè¯·å¿½ç•¥ä¸å‡†ç¡®çš„æœºå™¨ç¿»è¯‘ï¼‰
![result compare 1](imgs/img1.jpg)
![result compare 2](imgs/img2.jpg)
![result compare 3](imgs/img3.jpg)

## ğŸ”§ ç¯å¢ƒè¦æ±‚

requirement.txt

## ğŸ“š å‚è€ƒèµ„æ–™

- [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune)
- [Microsoft LLaVA-Med](https://github.com/microsoft/LLaVA-Med)
- [Qwen2-VL å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen2-VL)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªç›¸åº”çš„å¼€æºè®¸å¯è¯ï¼Œè¯·å‚è€ƒå„ä¸ªä¾èµ–é¡¹ç›®çš„è®¸å¯è¯è¦æ±‚ã€‚



# Qwen-VL-Med-SFT

![Qwen-VL-Med-SFT Workflow](flowchart0.png)

A medical vision-language model fine-tuning framework based on the Qwen2-VL series, specifically optimized for biomedical multimodal tasks.

## ğŸ¯ Project Overview

This project is built upon the [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) architecture, utilizing HuggingFace Transformers and DeepSpeed for training. Enhanced features include:

- âœ… Validation module during training
- âœ… Data preprocessing validation
- âœ… Model effectiveness verification
- âœ… Statistical comparison analysis

The project uses the [LLaVA-Med](https://github.com/microsoft/LLaVA-Med) dataset and implements a two-stage fine-tuning strategy: **Concept Alignment** and **Instruction Following**.

## ğŸ“Š Data Preprocessing Pipeline

### Training Data Processing
1. **Data Format Conversion**
   - Generate standard JSON files following [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) format
   - Iterative validation according to DataLoader processing to filter valid data

2. **Validation Data Processing**
   - Use `./data_process/data_trans.py` script for processing
   - Generate `test.json` and `type_mapping.json` for testing and classification

### Data Classification System

Based on question types, we categorize data into the following classes:

#### Closed-set Questions
| Type | Task Description |
|------|------------------|
| **Yes/No Judgment** | Binary judgment on the presence of specific features or abnormalities in medical images |
| **Modality Recognition** | Identify imaging modalities or technical types of medical images |
| **General Questions** | Closed-set questions not belonging to specific categories |
| **Location Positioning** | Inquire about specific locations of lesions or structures (limited options) |

#### Open-end Questions
| Type | Task Description |
|------|------------------|
| **General Description** | Open-ended questions requiring comprehensive description or explanation |
| **Anatomical Identification** | Identify and describe anatomical structures or organs in images |
| **Location Description** | Detailed description of lesion or structure locations |
| **Abnormality Recognition** | Identify and describe pathological changes or abnormal findings |
| **Counting Tasks** | Count specific objects in images |
| **Comparative Analysis** | Compare changes across different structures or time points |
| **Appearance Description** | Describe visual characteristics of lesions or structures |
| **Impact Assessment** | Evaluate lesion impact on surrounding structures |

## ğŸš€ Model Training

### Training Architecture
Based on HuggingFace Transformers' `Trainer` class with integrated callbacks via `trainer.add_callback(callback)`:
- Validation workflow callbacks
- TensorBoard logging
- Training process monitoring

### Starting Training

#### Single GPU Training/Debugging
```bash
bash finetune_lora_single_gpu.sh
```

#### Multi-GPU Training (DeepSpeed)
```bash
bash finetune_lora_mult_gpu.sh
```

## ğŸ§ª Model Testing and Evaluation

### Test Configuration
Supports two testing modes:
- **Custom Prompt Testing**: Using custom prompts
- **Original System Prompt Testing**: Using model default prompts

The testing process calculates perplexity and compares results between base and LoRA models.

### Running Tests
```bash
bash run_eval.sh
```

### Statistical Result Analysis
Use the `result_statistic.py` script for category-wise statistical analysis.

#### Evaluation Metrics System
| Metric Name | Value Range | Reasonable Score Range | Meaning | Description |
|-------------|-------------|----------------------|---------|-------------|
| **BLEU-4** | 0 ~ 1 | 0.2 ~ 0.6 | n-gram precision matching rate | Measures local language matching, penalizes repetition and missing words |
| **ROUGE-L** | 0 ~ 1 | 0.3 ~ 0.6 | Longest common subsequence recall | Emphasizes information coverage, favors recall |
| **METEOR** | 0 ~ 1 | 0.3 ~ 0.5 | Comprehensive metric: precision + recall + semantic synonyms + word order penalty | Tolerant of expression differences, suitable for generative tasks |
| **CIDEr** | 0 ~ âˆ | 0.5 ~ 2.0 | TF-IDF weighted n-gram matching score | Robust with multiple references, common in vision-language tasks |
| **BERTScore (F1)** | 0 ~ 1 | 0.85 ~ 0.95 | BERT-based sentence vector semantic similarity | Effective even with flexible expressions |
| **Soft Matching** | 0 ~ 1 | 0.6 ~ 0.95 | Character-level similarity (e.g., SequenceMatcher/LCS ratio) | Captures partial string similarity, strong fault tolerance |
| **Substring Match** | 0 or 1 | Binary (0/1) | Whether completely contained (reference âˆˆ prediction or vice versa) | Very strict, suitable for short and clear answer scenarios |

## ğŸ“ˆ Experimental Results

Based on Qwen2-VL-Instruct with concept alignment fine-tuning, applying LoRA technique to the visual-merger-proj module. Comparison results available in: `result/evaluation_metrics.json`

### Performance Comparison
| Category | Metric | Qwen2VL2BInst_Stage1 | Qwen2VL7BInst | Qwen2VL7B | Qwen2VL7BBase_Stage1 | Qwen2VL7BInst_Stage1 | Qwen2VL7BInst_SlakeTrain_BaseOnStage1 | Qwen25VL32BInst |
|----------|--------|--------|--------|--------|--------|--------|--------|--------|
| Text Matching | Exact Match | 0.4873 | 0.0000 | 0.0000 | 0.2873 | 0.7437 | **0.7493** | 0.0000 |
| Text Matching | Soft Match | 0.5198 | 0.0634 | 0.0154 | 0.3570 | 0.7443 | **0.7495** | 0.0053 |
| Text Matching | ROUGE-L | 0.5366 | 0.1002 | 0.0258 | 0.3834 | 0.7450 | **0.7493** | 0.0134 |
| Text Matching | BLEU-4 | 0.0914 | 0.0093 | 0.0021 | 0.0607 | 0.1324 | **0.1332** | 0.0010 |
| Text Matching | Word Overlap | 0.5167 | 0.0631 | 0.0188 | 0.3467 | 0.7444 | **0.7493** | 0.0109 |
| Fine-grained | Char F1 | 0.5674 | 0.2173 | 0.1875 | 0.4293 | 0.7424 | **0.7481** | 0.1822 |
| Fine-grained | Word F1 | 0.5390 | 0.1121 | 0.0361 | 0.3886 | 0.7451 | **0.7493** | 0.0214 |
| Fine-grained | Char Precision | 0.5349 | 0.1283 | 0.1083 | 0.3722 | 0.7417 | **0.7480** | 0.1052 |
| Fine-grained | Char Recall | 0.7765 | **0.8582** | 0.7798 | 0.8061 | 0.7535 | 0.7493 | 0.8202 |
| Fine-grained | Word Precision | 0.5167 | 0.0631 | 0.0188 | 0.3467 | 0.7444 | **0.7493** | 0.0109 |
| Fine-grained | Word Recall | 0.7296 | 0.6986 | 0.5887 | 0.7014 | **0.7549** | 0.7493 | 0.6620 |

### Key Findings
- **Closed-set Questions**: LoRA model shows significant improvement across all metrics, particularly Exact Match improving from 0.000 to 0.341
- **Open-end Questions**: Although improvements are relatively smaller, notable enhancements in Soft Match and ROUGE-L metrics
- **Overall Performance**: LoRA fine-tuned model demonstrates better understanding and generation capabilities in medical visual question answering tasks

## ğŸ”§ Environment Requirements

See `requirements.txt` for detailed dependencies.

## ğŸ“š References

- [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune)
- [Microsoft LLaVA-Med](https://github.com/microsoft/LLaVA-Med)
- [Qwen2-VL Official Documentation](https://github.com/QwenLM/Qwen2-VL)

## ğŸ“„ License

This project follows the corresponding open-source licenses. Please refer to the license requirements of each dependency project.

---

## ğŸŒ Language Versions

- [English](README.md)
- [ä¸­æ–‡](README_zh.md)
