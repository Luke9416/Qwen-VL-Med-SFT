# Qwen-VL-Med-SFT
![Qwen-VL-Med-SFT æµç¨‹](flowchart0.png)
ä¸€ä¸ªåŸºäº Qwen2-VL ç³»åˆ—æ¨¡å‹çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„å¤šæ¨¡æ€ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®åŸºäº [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) æ¶æ„ï¼Œä½¿ç”¨ HuggingFace Transformers å’Œ DeepSpeed è¿›è¡Œè®­ç»ƒã€‚åœ¨åŸæœ‰åŸºç¡€ä¸Šå¢å¼ºäº†ä»¥ä¸‹åŠŸèƒ½ï¼š

- âœ… è®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯æ¨¡å—
- âœ… æ•°æ®é¢„å¤„ç†æ£€æŸ¥
- âœ… æ¨¡å‹æ•ˆæœéªŒè¯
- âœ… ç»“æœç»Ÿè®¡å¯¹æ¯”åˆ†æ

é¡¹ç›®é‡‡ç”¨ [LLaVA-Med](https://github.com/microsoft/LLaVA-Med) æ•°æ®é›†ï¼Œå®ç°ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼š**æ¦‚å¿µå¯¹é½**ï¼ˆConcept Alignmentï¼‰å’Œ**æŒ‡ä»¤è·Ÿéš**ï¼ˆInstruction Followingï¼‰ã€‚

## ğŸ“Š æ•°æ®é¢„å¤„ç†æµç¨‹

### è®­ç»ƒæ•°æ®å¤„ç†

1. **æ•°æ®æ ¼å¼è½¬æ¢**
   - å‚è€ƒ [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) ç”Ÿæˆæ ‡å‡† JSON æ–‡ä»¶
   - æŒ‰ç…§ DataLoader çš„æ•°æ®å¤„ç†æ–¹å¼è¿›è¡Œè¿­ä»£éªŒè¯ï¼Œç­›é€‰æœ‰æ•ˆæ•°æ®

2. **éªŒè¯æ•°æ®å¤„ç†**
   - ä½¿ç”¨ `./data_process/data_trans.py` è„šæœ¬å¤„ç†
   - ç”Ÿæˆ `test.json` å’Œ `type_mapping.json` ç”¨äºæµ‹è¯•å’Œåˆ†ç±»

### æ•°æ®åˆ†ç±»ä½“ç³»

æ ¹æ®é—®é¢˜ç±»å‹ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºä»¥ä¸‹ç±»åˆ«ï¼š

#### å°é—­å¼é—®é¢˜ï¼ˆClose-setï¼‰

| ç±»å‹ | ä»»åŠ¡æè¿° |
|------|----------|
| **Yes/No åˆ¤æ–­** | å¯¹åŒ»å­¦å½±åƒä¸­ç‰¹å®šç‰¹å¾æˆ–å¼‚å¸¸çš„å­˜åœ¨æ€§è¿›è¡ŒäºŒå…ƒåˆ¤æ–­ |
| **æ¨¡æ€è¯†åˆ«** | è¯†åˆ«åŒ»å­¦å½±åƒçš„æˆåƒæ–¹å¼æˆ–æŠ€æœ¯ç±»å‹ |
| **é€šç”¨é—®é¢˜** | ä¸å±äºç‰¹å®šç±»åˆ«çš„å°é—­å¼é—®é¢˜ |
| **ä½ç½®å®šä½** | è¯¢é—®ç—…å˜æˆ–ç»“æ„çš„å…·ä½“ä½ç½®ï¼ˆé™å®šé€‰é¡¹ï¼‰ |

#### å¼€æ”¾å¼é—®é¢˜ï¼ˆOpen-endï¼‰

| ç±»å‹ | ä»»åŠ¡æè¿° |
|------|----------|
| **é€šç”¨æè¿°** | éœ€è¦ç»¼åˆæè¿°æˆ–è§£é‡Šçš„å¼€æ”¾æ€§é—®é¢˜ |
| **è§£å‰–è¯†åˆ«** | è¯†åˆ«å’Œæè¿°å½±åƒä¸­çš„è§£å‰–ç»“æ„æˆ–å™¨å®˜ |
| **ä½ç½®æè¿°** | è¯¦ç»†æè¿°ç—…å˜æˆ–ç»“æ„çš„ä½ç½® |
| **å¼‚å¸¸è¯†åˆ«** | è¯†åˆ«å’Œæè¿°ç—…ç†æ”¹å˜æˆ–å¼‚å¸¸å‘ç° |
| **è®¡æ•°ä»»åŠ¡** | è®¡ç®—å½±åƒä¸­ç‰¹å®šå¯¹è±¡çš„æ•°é‡ |
| **æ¯”è¾ƒåˆ†æ** | æ¯”è¾ƒä¸åŒç»“æ„æˆ–æ—¶é—´ç‚¹çš„å˜åŒ– |
| **å¤–è§‚æè¿°** | æè¿°ç—…å˜æˆ–ç»“æ„çš„è§†è§‰ç‰¹å¾ |
| **å½±å“è¯„ä¼°** | è¯„ä¼°ç—…å˜å¯¹å‘¨å›´ç»“æ„çš„å½±å“ |

## ğŸš€ æ¨¡å‹è®­ç»ƒ

### è®­ç»ƒæ¶æ„

åŸºäº HuggingFace Transformers çš„ `Trainer` ç±»è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡ `trainer.add_callback(callback)` æ–¹å¼é›†æˆï¼š
- éªŒè¯æµç¨‹å›è°ƒ
- TensorBoard æ—¥å¿—è®°å½•
- è®­ç»ƒè¿‡ç¨‹ç›‘æ§

### å¯åŠ¨è®­ç»ƒ

#### å•å¡è®­ç»ƒ/è°ƒè¯•
```bash
bash finetune_lora_single_gpu.sh
```

#### å¤šå¡è®­ç»ƒï¼ˆDeepSpeedï¼‰
```bash
bash finetune_lora_mult_gpu.sh
```

## ğŸ§ª æ¨¡å‹æµ‹è¯•ä¸è¯„ä¼°

### æµ‹è¯•é…ç½®

æ”¯æŒä¸¤ç§æµ‹è¯•æ¨¡å¼ï¼š
- **è‡ªå®šä¹‰ Prompt æµ‹è¯•**ï¼šä½¿ç”¨è‡ªè®¾å®šçš„æç¤ºè¯
- **åŸå§‹ System Prompt æµ‹è¯•**ï¼šä½¿ç”¨æ¨¡å‹é»˜è®¤æç¤ºè¯

æµ‹è¯•è¿‡ç¨‹ä¼šè®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼Œå¹¶å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œ LoRA æ¨¡å‹çš„ç»“æœã€‚

### å¯åŠ¨æµ‹è¯•
```bash
bash run_eval.sh
```

### ç»“æœç»Ÿè®¡åˆ†æ

ä½¿ç”¨ `result_statistic.py` è„šæœ¬è¿›è¡Œåˆ†ç±»åˆ«ç»Ÿè®¡åˆ†æã€‚

#### è¯„ä¼°æŒ‡æ ‡ä½“ç³»

| æŒ‡æ ‡åç§° | å€¼åŸŸèŒƒå›´ | åˆç†åˆ†æ•°åŒºé—´ | å«ä¹‰è§£é‡Š | è¯´æ˜ |
|----------|----------|--------------|----------|------|
| **BLEU-4** | 0 ~ 1 | 0.2 ~ 0.6 | n-gram ç²¾ç¡®åŒ¹é…ç‡ | è¡¡é‡å±€éƒ¨è¯­è¨€åŒ¹é…ï¼Œæƒ©ç½šé‡å¤ã€ç¼ºè¯ |
| **ROUGE-L** | 0 ~ 1 | 0.3 ~ 0.6 | æœ€é•¿å…¬å…±å­åºåˆ—çš„å¬å›ç‡ | é‡è§†ä¿¡æ¯è¦†ç›–ï¼Œåå‘å¬å›ç‡ |
| **METEOR** | 0 ~ 1 | 0.3 ~ 0.5 | ç²¾ç¡®åº¦ + å¬å› + è¯­ä¹‰åŒä¹‰è¯ + è¯åºæƒ©ç½šç»¼åˆæŒ‡æ ‡ | å®½å®¹è¡¨è¾¾å·®å¼‚ï¼Œé€‚åˆç”Ÿæˆå¼ä»»åŠ¡ |
| **CIDEr** | 0 ~ âˆ | 0.5 ~ 2.0 | åŸºäº TF-IDF çš„åŠ æƒ n-gram åŒ¹é…è¯„åˆ† | å¤šå‚è€ƒæ—¶é²æ£’ï¼Œå¸¸ç”¨äºå›¾æ–‡ä»»åŠ¡ |
| **BERTScore (F1)** | 0 ~ 1 | 0.85 ~ 0.95 | åŸºäº BERT çš„å¥å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦ | è¡¨è¾¾çµæ´»æ—¶ä¾æ—§æœ‰æ•ˆ |
| **Soft Matching** | 0 ~ 1 | 0.6 ~ 0.95 | å­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼ˆå¦‚ SequenceMatcher/LCS æ¯”ä¾‹ï¼‰ | æ•æ‰å­—ç¬¦ä¸²çš„éƒ¨åˆ†ç›¸ä¼¼ï¼Œå®¹é”™æ€§å¼º |
| **Substring Match** | 0 æˆ– 1 | äºŒå€¼å‹ï¼ˆ0/1ï¼‰ | æ˜¯å¦å®Œå…¨åŒ…å«ï¼ˆå‚è€ƒ âˆˆ é¢„æµ‹ æˆ–åä¹‹ï¼‰ | éå¸¸ä¸¥æ ¼ï¼Œé€‚ç”¨äºç­”æ¡ˆçŸ­æ˜ç¡®åœºæ™¯ |

## ğŸ“ˆ å®éªŒç»“æœ

åŸºäº Qwen2-VL-Instruct è¿›è¡Œæ¦‚å¿µå¯¹é½å¾®è°ƒï¼Œå¯¹ visual-merger-proj æ¨¡å—åº”ç”¨ LoRA æŠ€æœ¯ã€‚å¯¹æ¯”ç»“æœjson:result/evaluation_metrics.json

### æ€§èƒ½å¯¹æ¯”

| æ•°æ®ç±»å‹ | æ¨¡å‹ | Exact Match | Soft Match | Word Overlap | BLEU-4 | ROUGE-L |
|----------|------|-------------|------------|--------------|--------|---------|
| **Overall** | Base | 0.000 | 0.056 | 0.034 | 0.004 | 0.051 |
|  | **LoRA** | **0.117** | **0.238** | **0.175** | **0.030** | **0.210** |
| **Open-end** | Base | 0.000 | 0.069 | 0.035 | 0.004 | 0.052 |
|  | **LoRA** | **0.004** | **0.163** | **0.071** | **0.012** | **0.109** |
| **Closed-set** | Base | 0.000 | 0.030 | 0.031 | 0.004 | 0.048 |
|  | **LoRA** | **0.341** | **0.388** | **0.384** | **0.067** | **0.411** |

### å…³é”®å‘ç°

- **å°é—­å¼é—®é¢˜**ï¼šLoRA æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯ Exact Match ä» 0.000 æå‡åˆ° 0.341
- **å¼€æ”¾å¼é—®é¢˜**ï¼šè™½ç„¶æå‡ç›¸å¯¹è¾ƒå°ï¼Œä½†åœ¨ Soft Match å’Œ ROUGE-L æŒ‡æ ‡ä¸Šä»æœ‰æ˜æ˜¾æ”¹å–„
- **æ•´ä½“æ€§èƒ½**ï¼šLoRA å¾®è°ƒåæ¨¡å‹åœ¨åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´å¥½çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›

## ğŸ”§ ç¯å¢ƒè¦æ±‚

requirement.txt

## ğŸ“š å‚è€ƒèµ„æ–™

- [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune)
- [Microsoft LLaVA-Med](https://github.com/microsoft/LLaVA-Med)
- [Qwen2-VL å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen2-VL)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªç›¸åº”çš„å¼€æºè®¸å¯è¯ï¼Œè¯·å‚è€ƒå„ä¸ªä¾èµ–é¡¹ç›®çš„è®¸å¯è¯è¦æ±‚ã€‚



# Qwen-VL-Med-SFT

![Qwen-VL-Med-SFT Workflow](flowchart0.png)

A medical vision-language model fine-tuning framework based on the Qwen2-VL series, specifically optimized for biomedical multimodal tasks.

## ğŸ¯ Project Overview

This project is built upon the [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) architecture, utilizing HuggingFace Transformers and DeepSpeed for training. Enhanced features include:

- âœ… Validation module during training
- âœ… Data preprocessing validation
- âœ… Model effectiveness verification
- âœ… Statistical comparison analysis

The project uses the [LLaVA-Med](https://github.com/microsoft/LLaVA-Med) dataset and implements a two-stage fine-tuning strategy: **Concept Alignment** and **Instruction Following**.

## ğŸ“Š Data Preprocessing Pipeline

### Training Data Processing
1. **Data Format Conversion**
   - Generate standard JSON files following [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune) format
   - Iterative validation according to DataLoader processing to filter valid data

2. **Validation Data Processing**
   - Use `./data_process/data_trans.py` script for processing
   - Generate `test.json` and `type_mapping.json` for testing and classification

### Data Classification System

Based on question types, we categorize data into the following classes:

#### Closed-set Questions
| Type | Task Description |
|------|------------------|
| **Yes/No Judgment** | Binary judgment on the presence of specific features or abnormalities in medical images |
| **Modality Recognition** | Identify imaging modalities or technical types of medical images |
| **General Questions** | Closed-set questions not belonging to specific categories |
| **Location Positioning** | Inquire about specific locations of lesions or structures (limited options) |

#### Open-end Questions
| Type | Task Description |
|------|------------------|
| **General Description** | Open-ended questions requiring comprehensive description or explanation |
| **Anatomical Identification** | Identify and describe anatomical structures or organs in images |
| **Location Description** | Detailed description of lesion or structure locations |
| **Abnormality Recognition** | Identify and describe pathological changes or abnormal findings |
| **Counting Tasks** | Count specific objects in images |
| **Comparative Analysis** | Compare changes across different structures or time points |
| **Appearance Description** | Describe visual characteristics of lesions or structures |
| **Impact Assessment** | Evaluate lesion impact on surrounding structures |

## ğŸš€ Model Training

### Training Architecture
Based on HuggingFace Transformers' `Trainer` class with integrated callbacks via `trainer.add_callback(callback)`:
- Validation workflow callbacks
- TensorBoard logging
- Training process monitoring

### Starting Training

#### Single GPU Training/Debugging
```bash
bash finetune_lora_single_gpu.sh
```

#### Multi-GPU Training (DeepSpeed)
```bash
bash finetune_lora_mult_gpu.sh
```

## ğŸ§ª Model Testing and Evaluation

### Test Configuration
Supports two testing modes:
- **Custom Prompt Testing**: Using custom prompts
- **Original System Prompt Testing**: Using model default prompts

The testing process calculates perplexity and compares results between base and LoRA models.

### Running Tests
```bash
bash run_eval.sh
```

### Statistical Result Analysis
Use the `result_statistic.py` script for category-wise statistical analysis.

#### Evaluation Metrics System
| Metric Name | Value Range | Reasonable Score Range | Meaning | Description |
|-------------|-------------|----------------------|---------|-------------|
| **BLEU-4** | 0 ~ 1 | 0.2 ~ 0.6 | n-gram precision matching rate | Measures local language matching, penalizes repetition and missing words |
| **ROUGE-L** | 0 ~ 1 | 0.3 ~ 0.6 | Longest common subsequence recall | Emphasizes information coverage, favors recall |
| **METEOR** | 0 ~ 1 | 0.3 ~ 0.5 | Comprehensive metric: precision + recall + semantic synonyms + word order penalty | Tolerant of expression differences, suitable for generative tasks |
| **CIDEr** | 0 ~ âˆ | 0.5 ~ 2.0 | TF-IDF weighted n-gram matching score | Robust with multiple references, common in vision-language tasks |
| **BERTScore (F1)** | 0 ~ 1 | 0.85 ~ 0.95 | BERT-based sentence vector semantic similarity | Effective even with flexible expressions |
| **Soft Matching** | 0 ~ 1 | 0.6 ~ 0.95 | Character-level similarity (e.g., SequenceMatcher/LCS ratio) | Captures partial string similarity, strong fault tolerance |
| **Substring Match** | 0 or 1 | Binary (0/1) | Whether completely contained (reference âˆˆ prediction or vice versa) | Very strict, suitable for short and clear answer scenarios |

## ğŸ“ˆ Experimental Results

Based on Qwen2-VL-Instruct with concept alignment fine-tuning, applying LoRA technique to the visual-merger-proj module. Comparison results available in: `result/evaluation_metrics.json`

### Performance Comparison
| Data Type | Model | Exact Match | Soft Match | Word Overlap | BLEU-4 | ROUGE-L |
|-----------|-------|-------------|------------|--------------|--------|---------|
| **Overall** | Base | 0.000 | 0.056 | 0.034 | 0.004 | 0.051 |
|  | **LoRA** | **0.117** | **0.238** | **0.175** | **0.030** | **0.210** |
| **Open-end** | Base | 0.000 | 0.069 | 0.035 | 0.004 | 0.052 |
|  | **LoRA** | **0.004** | **0.163** | **0.071** | **0.012** | **0.109** |
| **Closed-set** | Base | 0.000 | 0.030 | 0.031 | 0.004 | 0.048 |
|  | **LoRA** | **0.341** | **0.388** | **0.384** | **0.067** | **0.411** |

### Key Findings
- **Closed-set Questions**: LoRA model shows significant improvement across all metrics, particularly Exact Match improving from 0.000 to 0.341
- **Open-end Questions**: Although improvements are relatively smaller, notable enhancements in Soft Match and ROUGE-L metrics
- **Overall Performance**: LoRA fine-tuned model demonstrates better understanding and generation capabilities in medical visual question answering tasks

## ğŸ”§ Environment Requirements

See `requirements.txt` for detailed dependencies.

## ğŸ“š References

- [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune)
- [Microsoft LLaVA-Med](https://github.com/microsoft/LLaVA-Med)
- [Qwen2-VL Official Documentation](https://github.com/QwenLM/Qwen2-VL)

## ğŸ“„ License

This project follows the corresponding open-source licenses. Please refer to the license requirements of each dependency project.

---

## ğŸŒ Language Versions

- [English](README.md)
- [ä¸­æ–‡](README_zh.md)
