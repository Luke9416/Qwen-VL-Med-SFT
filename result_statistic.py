"""
å®Œæ•´ç‰ˆè¯„ä¼°æŒ‡æ ‡è®¡ç®—è„šæœ¬
ç”¨äºè¯»å–æ¨ç†ç»“æœï¼Œè®¡ç®—å„ç§æ–‡æœ¬åŒ¹é…æŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œæ¨¡å‹å¯¹æ¯”
æ”¯æŒæŒ‰original_answer_typeå’Œquestion_typeåˆ†ç±»ç»Ÿè®¡
"""

import argparse
import os
import sys
import json
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from datetime import datetime
import math
import csv
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš ï¸ pandas not installed. Pivot tables will not be generated.")

# å¯¼å…¥è¯„ä¼°å™¨
from src.eval.evaluation_0702 import (
    SimpleTextEvaluator,
    # load_type_mapping,
    # get_item_type,
    # TYPE_MAPPING_LOADED
)


TYPE_MAPPING_LOADED = True
TYPE_MAPPING = {}
def load_type_mapping(mapping_file_path: str) -> bool:
    """åŠ è½½type_mappingæ–‡ä»¶ä½œä¸ºå…¨å±€å˜é‡"""
    global TYPE_MAPPING, TYPE_MAPPING_LOADED
    
    if not os.path.exists(mapping_file_path):
        print(f"âš ï¸ Type mapping file not found: {mapping_file_path}")
        return False
    
    try:
        with open(mapping_file_path, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # æå–type_mappingéƒ¨åˆ†
        TYPE_MAPPING = {item['id']: item['original_answer_type'] for item in mapping_data["entries"]}
        
        TYPE_MAPPING_LOADED = True
        
        # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
        type_stats = {}
        for item_type in TYPE_MAPPING.values():
            type_stats[item_type] = type_stats.get(item_type, 0) + 1
        
        print(f"âœ… Type mapping loaded: {len(TYPE_MAPPING)} items")
        print(f"ğŸ“Š Type distribution: {type_stats}")
        return True
        
    except Exception as e:
        print(f"âŒ Failed to load type mapping: {e}")
        return False

def get_item_type(item_id: str) -> str:
    """è·å–itemçš„ç±»å‹"""
    # if not TYPE_MAPPING_LOADED:
    #     return "unknown"
    return TYPE_MAPPING.get(item_id, "unknown")

class ExtendedTextEvaluator(SimpleTextEvaluator):
    """æ‰©å±•çš„æ–‡æœ¬è¯„ä¼°å™¨ï¼Œæ·»åŠ äº†evaluate_pairæ–¹æ³•"""
    
    def evaluate_pair(self, prediction: str, reference: str) -> Dict[str, float]:
        """
        è¯„ä¼°å•ä¸ªé¢„æµ‹-å‚è€ƒå¯¹
        
        Args:
            prediction: é¢„æµ‹æ–‡æœ¬
            reference: å‚è€ƒæ–‡æœ¬
            
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        # ä½¿ç”¨çˆ¶ç±»çš„æ–¹æ³•è®¡ç®—å„é¡¹æŒ‡æ ‡
        metrics = {
            "exact_match": self.calculate_exact_match(reference, prediction),
            "soft_match": self.calculate_soft_match(reference, prediction),
            "word_overlap": self.calculate_word_overlap(reference, prediction),
            "bleu_4": self.calculate_bleu_4(reference, prediction),
            "rouge_l": self.calculate_rouge_l(reference, prediction)
        }
        
        # è®¡ç®—é¢å¤–çš„æŒ‡æ ‡
        pred_clean = self.preprocess_text(prediction)
        ref_clean = self.preprocess_text(reference)
        
        # å­—ç¬¦çº§åˆ«çš„æŒ‡æ ‡
        if len(ref_clean) > 0:
            char_precision = sum(c in ref_clean for c in pred_clean) / max(len(pred_clean), 1)
            char_recall = sum(c in pred_clean for c in ref_clean) / len(ref_clean)
            char_f1 = 2 * char_precision * char_recall / max(char_precision + char_recall, 1e-8)
            
            metrics["char_precision"] = char_precision
            metrics["char_recall"] = char_recall
            metrics["char_f1"] = char_f1
        else:
            metrics["char_precision"] = 0.0
            metrics["char_recall"] = 0.0
            metrics["char_f1"] = 0.0
        
        # è¯çº§åˆ«çš„æŒ‡æ ‡
        pred_words = self.simple_tokenize(prediction)
        ref_words = self.simple_tokenize(reference)
        
        if len(ref_words) > 0:
            pred_set = set(pred_words)
            ref_set = set(ref_words)
            
            if len(pred_set) > 0:
                word_precision = len(pred_set & ref_set) / len(pred_set)
            else:
                word_precision = 0.0
                
            word_recall = len(pred_set & ref_set) / len(ref_set)
            word_f1 = 2 * word_precision * word_recall / max(word_precision + word_recall, 1e-8)
            
            metrics["word_precision"] = word_precision
            metrics["word_recall"] = word_recall
            metrics["word_f1"] = word_f1
        else:
            metrics["word_precision"] = 0.0
            metrics["word_recall"] = 0.0
            metrics["word_f1"] = 0.0
        
        return metrics

    def evaluate_batch_with_types(self, predictions: List[str], references: List[str], 
                                  item_ids: List[str]) -> Dict[str, Any]:
        """
        æ‰¹é‡è¯„ä¼°å¹¶æŒ‰ç±»å‹åˆ†ç»„è¿”å›æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            references: å‚è€ƒç­”æ¡ˆåˆ—è¡¨
            item_ids: item IDåˆ—è¡¨
            
        Returns:
            åŒ…å«æ€»ä½“æŒ‡æ ‡å’Œåˆ†ç±»æŒ‡æ ‡çš„å­—å…¸
        """
        if len(predictions) != len(references) or len(predictions) != len(item_ids):
            raise ValueError("Predictions, references, and item_ids must have the same length")
        
        # æŒ‰ç±»å‹åˆ†ç»„æ•°æ®
        type_groups = defaultdict(list)
        for pred, ref, item_id in zip(predictions, references, item_ids):
            item_type = get_item_type(item_id)
            type_groups[item_type].append((pred, ref, item_id))
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = self._calculate_metrics_for_group(predictions, references, "overall")
        
        # è®¡ç®—å„ç±»å‹æŒ‡æ ‡
        type_metrics = {}
        for item_type, group_data in type_groups.items():
            if group_data:
                group_preds = [item[0] for item in group_data]
                group_refs = [item[1] for item in group_data]
                type_metrics[item_type] = self._calculate_metrics_for_group(
                    group_preds, group_refs, item_type
                )
        
        # ç»„è£…ç»“æœ
        results = {
            "overall": overall_metrics,
            "by_type": type_metrics,
            "sample_counts": {
                "total": len(predictions),
                **{f"{item_type}_count": len(group_data) for item_type, group_data in type_groups.items()}
            }
        }
        
        return results





def load_enhanced_type_mapping(type_mapping_file: str) -> Dict[str, Dict[str, Any]]:
    """
    åŠ è½½å¢å¼ºçš„ç±»å‹æ˜ å°„æ–‡ä»¶
    
    Args:
        type_mapping_file: ç±»å‹æ˜ å°„æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ˜ å°„å­—å…¸ {item_id: {question_type, answer_category, original_answer_type, ...}}
    """
    with open(type_mapping_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    type_mapping = {}
    for entry in data.get("entries", []):
        item_id = entry["id"]
        type_mapping[item_id] = {
            "question_type": entry.get("question_type", "unknown"),
            "answer_category": entry.get("answer_category", "unknown"),
            "original_answer_type": entry.get("original_answer_type", "unknown"),
            "domain": entry.get("domain", {})
        }
    
    return type_mapping


def generate_detailed_metrics_csv(
    evaluation_results: Dict[str, Any],
    type_mapping: Dict[str, Dict[str, Any]],
    output_dir: str
) -> None:
    """
    ç”ŸæˆæŒ‰original_answer_typeå’Œquestion_typeåˆ†ç±»çš„è¯¦ç»†æŒ‡æ ‡CSV
    
    Args:
        evaluation_results: è¯„ä¼°ç»“æœå­—å…¸
        type_mapping: ç±»å‹æ˜ å°„å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
    """
    # å‡†å¤‡æ•°æ®ç»“æ„å­˜å‚¨åˆ†ç±»æŒ‡æ ‡
    metrics_by_category = {
        "lora": defaultdict(lambda: defaultdict(lambda: defaultdict(list))),
        "base": defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
    }
    
    # ä»è¯¦ç»†ç»“æœä¸­æå–æŒ‡æ ‡
    for model_type in ["lora", "base"]:
        if model_type not in evaluation_results.get("detailed_metrics", {}):
            continue
            
        detailed_results = evaluation_results["detailed_metrics"][model_type]
        
        for result in detailed_results:
            item_id = result["item_id"]
            
            # è·å–åˆ†ç±»ä¿¡æ¯
            if item_id in type_mapping:
                mapping_info = type_mapping[item_id]
                original_answer_type = mapping_info["original_answer_type"]
                question_type = mapping_info["question_type"]
                
                # æ”¶é›†å„é¡¹æŒ‡æ ‡
                metrics = result.get("metrics", {})
                for metric_name, metric_value in metrics.items():
                    if isinstance(metric_value, (int, float)):
                        metrics_by_category[model_type][original_answer_type][question_type][metric_name].append(metric_value)
    
    # ç”ŸæˆCSVæ–‡ä»¶
    for model_type in ["lora", "base"]:
        if not metrics_by_category[model_type]:
            continue
            
        csv_file = os.path.join(output_dir, f"{model_type}_metrics_by_category.csv")
        
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        headers = ["original_answer_type", "question_type", "sample_count"]
        metric_names = set()
        
        # è®¡ç®—å¹³å‡å€¼å¹¶å‡†å¤‡æ•°æ®
        for answer_type, question_types in metrics_by_category[model_type].items():
            for q_type, metrics in question_types.items():
                row = {
                    "original_answer_type": answer_type,
                    "question_type": q_type,
                    "sample_count": 0
                }
                
                # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å¹³å‡å€¼
                for metric_name, values in metrics.items():
                    if values:
                        avg_value = sum(values) / len(values)
                        row[f"avg_{metric_name}"] = round(avg_value, 4)
                        metric_names.add(f"avg_{metric_name}")
                        row["sample_count"] = len(values)
                
                if row["sample_count"] > 0:
                    csv_data.append(row)
        
        # æ›´æ–°headers
        headers.extend(sorted(metric_names))
        
        # å†™å…¥CSV
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            
            # æŒ‰original_answer_typeå’Œquestion_typeæ’åº
            csv_data.sort(key=lambda x: (x["original_answer_type"], x["question_type"]))
            writer.writerows(csv_data)
        
        print(f"ğŸ’¾ Saved {model_type} metrics by category to: {csv_file}")
        
        # ä½¿ç”¨pandasç”Ÿæˆæ›´ç¾è§‚çš„æ±‡æ€»è¡¨
        if PANDAS_AVAILABLE:
            try:
                if csv_data:
                    df = pd.DataFrame(csv_data)
                    pivot_file = os.path.join(output_dir, f"{model_type}_metrics_pivot.csv")
                    
                    # ä¸ºæ¯ä¸ªä¸»è¦æŒ‡æ ‡åˆ›å»ºé€è§†è¡¨
                    main_metrics = ["avg_exact_match", "avg_soft_match", "avg_rouge_l", "avg_bleu_4"]
                    existing_metrics = [m for m in main_metrics if m in df.columns]
                    
                    if existing_metrics:
                        # åˆ›å»ºä¸€ä¸ªå¤šçº§ç´¢å¼•çš„DataFrame
                        pivot_dfs = []
                        
                        for metric in existing_metrics:
                            pivot = df.pivot_table(
                                values=metric,
                                index="original_answer_type",
                                columns="question_type",
                                aggfunc='first'
                            ).round(4)
                            
                            # æ·»åŠ è¡Œå’Œåˆ—çš„å¹³å‡å€¼
                            pivot['Average'] = pivot.mean(axis=1).round(4)
                            pivot.loc['Average'] = pivot.mean(axis=0).round(4)
                            
                            # æ·»åŠ æŒ‡æ ‡åç§°ä½œä¸ºæ ‡è¯†
                            pivot.index = [f"{metric}_{idx}" for idx in pivot.index]
                            pivot_dfs.append(pivot)
                        
                        # åˆå¹¶æ‰€æœ‰é€è§†è¡¨
                        combined_pivot = pd.concat(pivot_dfs)
                        combined_pivot.to_csv(pivot_file)
                        print(f"ğŸ’¾ Saved {model_type} pivot table to: {pivot_file}")
            except Exception as e:
                print(f"âš ï¸ Could not create pivot table: {e}")
    
    # ç”Ÿæˆå¯¹æ¯”CSVï¼ˆå¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½æœ‰ç»“æœï¼‰
    if "lora" in metrics_by_category and "base" in metrics_by_category:
        comparison_file = os.path.join(output_dir, "model_comparison_by_category.csv")
        comparison_data = []
        
        # è·å–æ‰€æœ‰çš„answer_typeå’Œquestion_typeç»„åˆ
        all_combinations = set()
        for model_metrics in metrics_by_category.values():
            for answer_type, question_types in model_metrics.items():
                for q_type in question_types.keys():
                    all_combinations.add((answer_type, q_type))
        
        # å¯¹æ¯ä¸ªç»„åˆè®¡ç®—å¯¹æ¯”
        for answer_type, q_type in sorted(all_combinations):
            row = {
                "original_answer_type": answer_type,
                "question_type": q_type
            }
            
            # è·å–ä¸»è¦æŒ‡æ ‡çš„å¯¹æ¯”
            main_metrics = ["exact_match", "soft_match", "rouge_l", "bleu_4"]
            
            for metric in main_metrics:
                lora_values = metrics_by_category["lora"][answer_type][q_type].get(metric, [])
                base_values = metrics_by_category["base"][answer_type][q_type].get(metric, [])
                
                if lora_values and base_values:
                    lora_avg = sum(lora_values) / len(lora_values)
                    base_avg = sum(base_values) / len(base_values)
                    improvement = ((lora_avg - base_avg) / base_avg * 100) if base_avg != 0 else 0
                    
                    row[f"lora_{metric}"] = round(lora_avg, 4)
                    row[f"base_{metric}"] = round(base_avg, 4)
                    row[f"{metric}_improvement_%"] = round(improvement, 2)
            
            row["lora_samples"] = len(metrics_by_category["lora"][answer_type][q_type].get("exact_match", []))
            row["base_samples"] = len(metrics_by_category["base"][answer_type][q_type].get("exact_match", []))
            
            if row["lora_samples"] > 0 or row["base_samples"] > 0:
                comparison_data.append(row)
        
        # å†™å…¥å¯¹æ¯”CSV
        if comparison_data:
            comparison_headers = ["original_answer_type", "question_type", "lora_samples", "base_samples"]
            for metric in ["exact_match", "soft_match", "rouge_l", "bleu_4"]:
                comparison_headers.extend([f"lora_{metric}", f"base_{metric}", f"{metric}_improvement_%"])
            
            with open(comparison_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=comparison_headers)
                writer.writeheader()
                writer.writerows(comparison_data)
            
            print(f"ğŸ’¾ Saved model comparison by category to: {comparison_file}")


def print_category_summary(
    evaluation_results: Dict[str, Any],
    type_mapping: Dict[str, Dict[str, Any]]
) -> None:
    """
    æ‰“å°æŒ‰ç±»åˆ«åˆ†ç»„çš„æŒ‡æ ‡æ‘˜è¦
    
    Args:
        evaluation_results: è¯„ä¼°ç»“æœå­—å…¸
        type_mapping: ç±»å‹æ˜ å°„å­—å…¸
    """
    print("\n" + "="*80)
    print("ğŸ“Š METRICS BY CATEGORY")
    print("="*80)
    
    # ç»Ÿè®¡å„ç±»åˆ«çš„æŒ‡æ ‡
    for model_type in ["lora", "base"]:
        if model_type not in evaluation_results.get("detailed_metrics", {}):
            continue
            
        print(f"\nğŸ¤– {model_type.upper()} MODEL - Metrics by Category:")
        print("-" * 70)
        
        # æ”¶é›†æŒ‰ç±»åˆ«åˆ†ç»„çš„æŒ‡æ ‡
        category_metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
        
        detailed_results = evaluation_results["detailed_metrics"][model_type]
        
        for result in detailed_results:
            item_id = result["item_id"]
            
            if item_id in type_mapping:
                mapping_info = type_mapping[item_id]
                original_answer_type = mapping_info["original_answer_type"]
                question_type = mapping_info["question_type"]
                
                metrics = result.get("metrics", {})
                for metric_name, metric_value in metrics.items():
                    if isinstance(metric_value, (int, float)):
                        category_metrics[original_answer_type][question_type][metric_name].append(metric_value)
        
        # æ‰“å°ç»“æœ
        for answer_type in sorted(category_metrics.keys()):
            print(f"\nğŸ“ Original Answer Type: {answer_type}")
            
            for q_type in sorted(category_metrics[answer_type].keys()):
                metrics = category_metrics[answer_type][q_type]
                sample_count = len(next(iter(metrics.values()), []))
                
                print(f"  ğŸ“‹ Question Type: {q_type} (n={sample_count})")
                
                # æ‰“å°ä¸»è¦æŒ‡æ ‡
                main_metrics = ["exact_match", "soft_match", "rouge_l", "bleu_4"]
                for metric_name in main_metrics:
                    if metric_name in metrics and metrics[metric_name]:
                        avg_value = sum(metrics[metric_name]) / len(metrics[metric_name])
                        print(f"    - {metric_name}: {avg_value:.4f}")
    
    print("\n" + "="*80)


class MetricsEvaluator:
    """æŒ‡æ ‡è¯„ä¼°å™¨"""
    
    def __init__(self, type_mapping_file: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            type_mapping_file: ç±»å‹æ˜ å°„æ–‡ä»¶è·¯å¾„
        """
        self.text_evaluator = ExtendedTextEvaluator()
        self.type_mapping = {}  # å­˜å‚¨å¢å¼ºçš„ç±»å‹æ˜ å°„
        
        # åŠ è½½ç±»å‹æ˜ å°„ï¼ˆå¦‚æœæä¾›ï¼‰
        # if type_mapping_file and os.path.exists(type_mapping_file):
        #     load_enhanced_type_mapping(type_mapping_file)
            # åŠ è½½å¢å¼ºçš„ç±»å‹æ˜ å°„
        self.type_mapping = load_enhanced_type_mapping(type_mapping_file)
        print(f"âœ… Loaded enhanced type mapping from: {type_mapping_file}")
    
    def evaluate_from_results(
        self,
        results_file: str,
        output_dir: str,
        evaluate_lora: bool = True,
        evaluate_base: bool = True,
        compare_models: bool = True
    ):
        """
        ä»æ¨ç†ç»“æœæ–‡ä»¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            results_file: æ¨ç†ç»“æœJSONæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            evaluate_lora: æ˜¯å¦è¯„ä¼°LoRAæ¨¡å‹
            evaluate_base: æ˜¯å¦è¯„ä¼°åŸºç¡€æ¨¡å‹
            compare_models: æ˜¯å¦è¿›è¡Œæ¨¡å‹å¯¹æ¯”
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ¨ç†ç»“æœ
        print(f"ğŸ“‚ Loading inference results from: {results_file}")
        with open(results_file, 'r', encoding='utf-8') as f:
            inference_data = json.load(f)
        
        results = inference_data["results"]
        metadata = inference_data["metadata"]
        
        print(f"âœ… Loaded {len(results)} samples")
        
        # å‡†å¤‡è¯„ä¼°ç»“æœ
        evaluation_results = {
            "metadata": {
                **metadata,
                "evaluation_time": datetime.now().isoformat(),
                "evaluate_lora": evaluate_lora,
                "evaluate_base": evaluate_base,
                "compare_models": compare_models
            },
            "metrics": {},
            "detailed_metrics": {}
        }
        
        # è¯„ä¼°LoRAæ¨¡å‹
        if evaluate_lora:
            lora_metrics = self._evaluate_model_results(
                results, 
                model_type="lora",
                output_dir=output_dir
            )
            if lora_metrics:
                evaluation_results["metrics"]["lora"] = lora_metrics["metrics"]
                evaluation_results["detailed_metrics"]["lora"] = lora_metrics["detailed"]
        
        # è¯„ä¼°åŸºç¡€æ¨¡å‹
        if evaluate_base:
            base_metrics = self._evaluate_model_results(
                results,
                model_type="base",
                output_dir=output_dir
            )
            if base_metrics:
                evaluation_results["metrics"]["base"] = base_metrics["metrics"]
                evaluation_results["detailed_metrics"]["base"] = base_metrics["detailed"]
        
        # æ¨¡å‹å¯¹æ¯”
        if compare_models and "lora" in evaluation_results["metrics"] and "base" in evaluation_results["metrics"]:
            comparison = self._compare_models(
                evaluation_results["metrics"]["lora"],
                evaluation_results["metrics"]["base"],
                results
            )
            evaluation_results["comparison"] = comparison
        
        # ç”ŸæˆæŒ‰ç±»åˆ«åˆ†ç»„çš„CSVæ–‡ä»¶
        if self.type_mapping:
            generate_detailed_metrics_csv(evaluation_results, self.type_mapping, output_dir)
            print_category_summary(evaluation_results, self.type_mapping)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        self._save_evaluation_results(evaluation_results, output_dir)
        
        return evaluation_results
    
    def _evaluate_model_results(
        self,
        results: List[Dict[str, Any]],
        model_type: str,
        output_dir: str
    ):
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹çš„ç»“æœ
        
        Args:
            results: æ¨ç†ç»“æœåˆ—è¡¨
            model_type: æ¨¡å‹ç±»å‹ ("lora" æˆ– "base")
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print(f"\nğŸ“Š Evaluating {model_type} model...")
        
        # æ”¶é›†é¢„æµ‹å’Œå‚è€ƒ
        predictions = []
        references = []
        item_ids = []
        perplexities = []
        nlls = []
        
        prediction_key = f"{model_type}_prediction"
        perplexity_key = f"{model_type}_perplexity"
        nll_key = f"{model_type}_nll"
        
        for result in results:
            if prediction_key in result:
                predictions.append(result[prediction_key])
                references.append(result["reference"])
                item_ids.append(result["item_id"])
                
                # æ”¶é›†å›°æƒ‘åº¦
                if perplexity_key in result and not math.isinf(result[perplexity_key]):
                    perplexities.append(result[perplexity_key])
                    if nll_key in result:
                        nlls.append(result[nll_key])
        
        if not predictions:
            print(f"âš ï¸ No {model_type} predictions found")
            return None
        
        print(f"ğŸ“Š Evaluating {len(predictions)} {model_type} predictions")
        
        # è®¡ç®—æ–‡æœ¬æŒ‡æ ‡
        if TYPE_MAPPING_LOADED:
            metrics = self.text_evaluator.evaluate_batch_with_types(
                predictions,
                references,
                item_ids
            )
        else:
            overall_metrics = self.text_evaluator.evaluate_batch(
                predictions,
                references
            )
            metrics = {"overall": overall_metrics}
        
        # æ·»åŠ å›°æƒ‘åº¦ç»Ÿè®¡
        if perplexities:
            perplexity_stats = {
                "mean_perplexity": np.mean(perplexities),
                "std_perplexity": np.std(perplexities),
                "median_perplexity": np.median(perplexities),
                "min_perplexity": np.min(perplexities),
                "max_perplexity": np.max(perplexities),
                "num_valid_samples": len(perplexities),
                "valid_ratio": len(perplexities) / len(predictions)
            }
            
            if nlls:
                perplexity_stats.update({
                    "mean_nll": np.mean(nlls),
                    "std_nll": np.std(nlls),
                    "median_nll": np.median(nlls)
                })
            
            metrics["perplexity"] = perplexity_stats
        
        # å‡†å¤‡è¯¦ç»†ç»“æœ
        detailed_results = []
        for i, (pred, ref, item_id) in enumerate(zip(predictions, references, item_ids)):
            detail = {
                "item_id": item_id,
                "prediction": pred,
                "reference": ref,
                "metrics": self.text_evaluator.evaluate_pair(pred, ref)
            }
            
            # æ·»åŠ å›°æƒ‘åº¦ä¿¡æ¯
            if i < len(results) and perplexity_key in results[i]:
                detail["perplexity"] = results[i][perplexity_key]
                if nll_key in results[i]:
                    detail["nll"] = results[i][nll_key]
            
            detailed_results.append(detail)
        
        return {
            "metrics": metrics,
            "detailed": detailed_results
        }
    
    def _compare_models(
        self,
        lora_metrics: Dict[str, Any],
        base_metrics: Dict[str, Any],
        results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            lora_metrics: LoRAæ¨¡å‹æŒ‡æ ‡
            base_metrics: åŸºç¡€æ¨¡å‹æŒ‡æ ‡
            results: åŸå§‹æ¨ç†ç»“æœ
            
        Returns:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        print("\nğŸ“Š Comparing models...")
        
        comparison = {
            "overall": {},
            "by_type": {},
            "perplexity": {},
            "sample_level": []
        }
        
        # æ¯”è¾ƒæ€»ä½“æŒ‡æ ‡
        if "overall" in lora_metrics and "overall" in base_metrics:
            for metric_name in lora_metrics["overall"]:
                if metric_name.startswith("eval_") and metric_name in base_metrics["overall"]:
                    lora_val = lora_metrics["overall"][metric_name]
                    base_val = base_metrics["overall"][metric_name]
                    
                    if isinstance(lora_val, (int, float)) and isinstance(base_val, (int, float)):
                        improvement = ((lora_val - base_val) / base_val * 100) if base_val != 0 else 0
                        
                        comparison["overall"][metric_name] = {
                            "base": base_val,
                            "lora": lora_val,
                            "improvement_%": improvement,
                            "better_model": "lora" if lora_val > base_val else "base"
                        }
        
        # æ¯”è¾ƒç±»å‹çº§åˆ«çš„æŒ‡æ ‡
        if "by_type" in lora_metrics and "by_type" in base_metrics:
            for type_name in lora_metrics["by_type"]:
                if type_name in base_metrics["by_type"]:
                    comparison["by_type"][type_name] = {}
                    
                    for metric_name in lora_metrics["by_type"][type_name]:
                        if metric_name.startswith("eval_") and metric_name in base_metrics["by_type"][type_name]:
                            lora_val = lora_metrics["by_type"][type_name][metric_name]
                            base_val = base_metrics["by_type"][type_name][metric_name]
                            
                            if isinstance(lora_val, (int, float)) and isinstance(base_val, (int, float)):
                                improvement = ((lora_val - base_val) / base_val * 100) if base_val != 0 else 0
                                
                                comparison["by_type"][type_name][metric_name] = {
                                    "base": base_val,
                                    "lora": lora_val,
                                    "improvement_%": improvement,
                                    "better_model": "lora" if lora_val > base_val else "base"
                                }
        
        # æ¯”è¾ƒå›°æƒ‘åº¦
        if "perplexity" in lora_metrics and "perplexity" in base_metrics:
            for ppl_metric in ["mean_perplexity", "median_perplexity", "mean_nll"]:
                if ppl_metric in lora_metrics["perplexity"] and ppl_metric in base_metrics["perplexity"]:
                    lora_val = lora_metrics["perplexity"][ppl_metric]
                    base_val = base_metrics["perplexity"][ppl_metric]
                    
                    # å›°æƒ‘åº¦è¶Šä½è¶Šå¥½
                    improvement = ((base_val - lora_val) / base_val * 100) if base_val != 0 else 0
                    
                    comparison["perplexity"][ppl_metric] = {
                        "base": base_val,
                        "lora": lora_val,
                        "improvement_%": improvement,
                        "better_model": "lora" if lora_val < base_val else "base"
                    }
        
        # æ ·æœ¬çº§åˆ«çš„æ¯”è¾ƒ
        for result in results:
            if "lora_prediction" in result and "base_prediction" in result:
                lora_pred = result["lora_prediction"]
                base_pred = result["base_prediction"]
                reference = result["reference"]
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                lora_sample_metrics = self.text_evaluator.evaluate_pair(lora_pred, reference)
                base_sample_metrics = self.text_evaluator.evaluate_pair(base_pred, reference)
                
                sample_comparison = {
                    "item_id": result["item_id"],
                    "lora_metrics": lora_sample_metrics,
                    "base_metrics": base_sample_metrics,
                    "better_model": self._determine_better_model(lora_sample_metrics, base_sample_metrics)
                }
                
                # æ·»åŠ å›°æƒ‘åº¦æ¯”è¾ƒ
                if "lora_perplexity" in result and "base_perplexity" in result:
                    sample_comparison["lora_perplexity"] = result["lora_perplexity"]
                    sample_comparison["base_perplexity"] = result["base_perplexity"]
                    
                    if not math.isinf(result["lora_perplexity"]) and not math.isinf(result["base_perplexity"]):
                        sample_comparison["perplexity_improvement_%"] = (
                            (result["base_perplexity"] - result["lora_perplexity"]) / 
                            result["base_perplexity"] * 100
                        )
                
                comparison["sample_level"].append(sample_comparison)
        
        # ç»Ÿè®¡è·èƒœæƒ…å†µ
        if comparison["sample_level"]:
            wins = {"lora": 0, "base": 0, "tie": 0}
            for sample in comparison["sample_level"]:
                wins[sample["better_model"]] += 1
            
            comparison["win_statistics"] = {
                "total_samples": len(comparison["sample_level"]),
                "lora_wins": wins["lora"],
                "base_wins": wins["base"],
                "ties": wins["tie"],
                "lora_win_rate": wins["lora"] / len(comparison["sample_level"]),
                "base_win_rate": wins["base"] / len(comparison["sample_level"])
            }
        
        return comparison
    
    def _determine_better_model(
        self,
        lora_metrics: Dict[str, float],
        base_metrics: Dict[str, float]
    ) -> str:
        """
        æ ¹æ®æŒ‡æ ‡ç¡®å®šå“ªä¸ªæ¨¡å‹æ›´å¥½
        
        Args:
            lora_metrics: LoRAæ¨¡å‹æŒ‡æ ‡
            base_metrics: åŸºç¡€æ¨¡å‹æŒ‡æ ‡
            
        Returns:
            "lora", "base" æˆ– "tie"
        """
        # å®šä¹‰é‡è¦æŒ‡æ ‡åŠå…¶æƒé‡
        metric_weights = {
            "exact_match": 3.0,
            "soft_match": 2.0,
            "rouge_l": 1.5,
            "bleu_4": 1.0
        }
        
        lora_score = 0
        base_score = 0
        
        for metric, weight in metric_weights.items():
            if metric in lora_metrics and metric in base_metrics:
                if lora_metrics[metric] > base_metrics[metric]:
                    lora_score += weight
                elif base_metrics[metric] > lora_metrics[metric]:
                    base_score += weight
        
        if lora_score > base_score:
            return "lora"
        elif base_score > lora_score:
            return "base"
        else:
            return "tie"
    
    def _save_evaluation_results(self, results: Dict[str, Any], output_dir: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = os.path.join(output_dir, "evaluation_metrics.json")
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Saved full evaluation results to: {full_results_file}")
        
        # ä¿å­˜ç®€åŒ–çš„æŒ‡æ ‡æ‘˜è¦
        summary = {
            "metadata": results["metadata"],
            "metrics": results["metrics"]
        }
        
        if "comparison" in results and "overall" in results["comparison"]:
            summary["comparison_summary"] = {
                "overall": results["comparison"]["overall"],
                "perplexity": results["comparison"].get("perplexity", {}),
                "win_statistics": results["comparison"].get("win_statistics", {})
            }
        
        summary_file = os.path.join(output_dir, "metrics_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Saved metrics summary to: {summary_file}")
        
        # ä¿å­˜è¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
        if "comparison" in results:
            comparison_file = os.path.join(output_dir, "model_comparison.json")
            with open(comparison_file, 'w', encoding='utf-8') as f:
                json.dump(results["comparison"], f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Saved model comparison to: {comparison_file}")
        
        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_evaluation_summary(results)
    
    def _print_evaluation_summary(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š EVALUATION SUMMARY")
        print("="*80)
        
        metrics = results["metrics"]
        
        # æ‰“å°æ¯ä¸ªæ¨¡å‹çš„æŒ‡æ ‡
        for model_type, model_metrics in metrics.items():
            print(f"\nğŸ¤– {model_type.upper()} MODEL METRICS:")
            print("-" * 50)
            
            # æ€»ä½“æŒ‡æ ‡
            if "overall" in model_metrics:
                print("\nğŸ¯ Overall Metrics:")
                overall = model_metrics["overall"]
                for key, value in overall.items():
                    if key.startswith('eval_') and isinstance(value, (int, float)):
                        print(f"  {key}: {value:.4f}")
            
            # å›°æƒ‘åº¦æŒ‡æ ‡
            if "perplexity" in model_metrics:
                print("\nğŸ§  Perplexity Metrics:")
                perplexity = model_metrics["perplexity"]
                for key, value in perplexity.items():
                    if isinstance(value, (int, float)):
                        print(f"  {key}: {value:.4f}")
            
            # åˆ†ç±»æŒ‡æ ‡
            if "by_type" in model_metrics:
                print("\nğŸ“‹ Metrics by Type:")
                by_type = model_metrics["by_type"]
                for item_type, type_metrics in by_type.items():
                    print(f"\n  ğŸ“‚ {item_type.upper()}:")
                    for key, value in type_metrics.items():
                        if key.startswith('eval_') and isinstance(value, (int, float)):
                            print(f"    {key}: {value:.4f}")
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        if "comparison" in results:
            print("\n" + "="*80)
            print("ğŸ“ˆ MODEL COMPARISON")
            print("="*80)
            
            comparison = results["comparison"]
            
            # æ€»ä½“å¯¹æ¯”
            if "overall" in comparison:
                print("\nğŸ¯ Overall Comparison:")
                for metric_name, comp in comparison["overall"].items():
                    print(f"\n  {metric_name}:")
                    print(f"    Base:  {comp['base']:.4f}")
                    print(f"    LoRA:  {comp['lora']:.4f}")
                    print(f"    Improvement: {comp['improvement_%']:+.2f}%")
                    print(f"    Better: {comp['better_model'].upper()}")
            
            # å›°æƒ‘åº¦å¯¹æ¯”
            if "perplexity" in comparison:
                print("\nğŸ§  Perplexity Comparison:")
                for metric_name, comp in comparison["perplexity"].items():
                    print(f"\n  {metric_name}:")
                    print(f"    Base:  {comp['base']:.4f}")
                    print(f"    LoRA:  {comp['lora']:.4f}")
                    print(f"    Improvement: {comp['improvement_%']:+.2f}%")
                    print(f"    Better: {comp['better_model'].upper()}")
            
            # è·èƒœç»Ÿè®¡
            if "win_statistics" in comparison:
                print("\nğŸ† Win Statistics:")
                stats = comparison["win_statistics"]
                print(f"  Total samples: {stats['total_samples']}")
                print(f"  LoRA wins: {stats['lora_wins']} ({stats['lora_win_rate']*100:.1f}%)")
                print(f"  Base wins: {stats['base_wins']} ({stats['base_win_rate']*100:.1f}%)")
                print(f"  Ties: {stats['ties']}")
        
        print("\n" + "="*80)


def main():
    parser = argparse.ArgumentParser(description="Evaluate model metrics from inference results")
    
    # è¾“å…¥è¾“å‡ºå‚æ•°
    parser.add_argument("--results_file", type=str, 
                        default=None)
    parser.add_argument("--type_mapping_file", type=str, 
                        default=None,
                        help="Path to type mapping JSON file for categorized evaluation")
    # è¯„ä¼°é€‰é¡¹
    parser.add_argument("--evaluate_lora", action="store_true", default=True,
                        help="Evaluate LoRA model results")
    parser.add_argument("--no_lora", dest="evaluate_lora", action="store_false",
                        help="Don't evaluate LoRA model")
    parser.add_argument("--evaluate_base", action="store_true", default=True,
                        help="Evaluate base model results")
    parser.add_argument("--no_base", dest="evaluate_base", action="store_false",
                        help="Don't evaluate base model")
    parser.add_argument("--compare_models", action="store_true", default=True,
                        help="Compare LoRA and base models")
    parser.add_argument("--no_compare", dest="compare_models", action="store_false",
                        help="Don't compare models")
    
    args = parser.parse_args()
    

    args.output_dir = os.path.join(os.path.dirname(args.results_file), 'analysis')

    if args.type_mapping_file:
        load_type_mapping(args.type_mapping_file)
    
    
    # éªŒè¯å‚æ•°
    if not args.evaluate_lora and not args.evaluate_base:
        parser.error("At least one of --evaluate_lora or --evaluate_base must be enabled")
    
    if args.compare_models and not (args.evaluate_lora and args.evaluate_base):
        print("âš ï¸ Model comparison requires both models to be evaluated")
        args.compare_models = False
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.results_file):
        parser.error(f"Results file not found: {args.results_file}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    print("ğŸš€ Initializing metrics evaluator...")
    evaluator = MetricsEvaluator(type_mapping_file=args.type_mapping_file)
    
    # è¿è¡Œè¯„ä¼°
    print("ğŸ“Š Starting evaluation...")
    results = evaluator.evaluate_from_results(
        results_file=args.results_file,
        output_dir=args.output_dir,
        evaluate_lora=args.evaluate_lora,
        evaluate_base=args.evaluate_base,
        compare_models=args.compare_models
    )
    
    print(f"\nâœ… Evaluation completed!")
    print(f"ğŸ“ Results saved to: {args.output_dir}")
    
    # æ‰“å°è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
    print("\nğŸ“„ Generated files:")
    output_files = os.listdir(args.output_dir)
    for file in sorted(output_files):
        print(f"  - {file}")


if __name__ == "__main__":
    main()