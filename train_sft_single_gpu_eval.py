import os
import torch
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
import ast
from transformers import AutoProcessor, BitsAndBytesConfig, Qwen2VLForConditionalGeneration, HfArgumentParser, Qwen2_5_VLForConditionalGeneration
from transformers import TrainerCallback
from transformers.integrations import TensorBoardCallback as HFTensorBoardCallback
from src.trainer import QwenSFTTrainer
from src.dataset import make_supervised_eval_data_module
from src.params_eval import DataArguments, ModelArguments, TrainingArguments
from src.train.train_utils import get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3, safe_save_model_for_hf_trainer
from src.eval.evaluation_0702 import create_compute_metrics_with_types, load_type_mapping

import pathlib
from liger_kernel.transformers import apply_liger_kernel_to_qwen2_vl, apply_liger_kernel_to_qwen2_5_vl
from src.train.monkey_patch_forward import replace_qwen2_5_with_mixed_modality_forward, replace_qwen_2_with_mixed_modality_forward
from src.utils import inspect_lora_model
from src.logger_utils import init_tensorboard, TensorBoardCallback

import logging
import sys
from datetime import datetime
import json
import numpy as np




# è®¾ç½®å•å¡è°ƒè¯•ç¯å¢ƒ
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
local_rank = 0

# å…¨å±€loggerå˜é‡
logger = None


class EvaluationCallback(TrainerCallback):
    """è‡ªå®šä¹‰è¯„ä¼°å›è°ƒï¼Œç¡®ä¿è¯„ä¼°è¢«æ‰§è¡Œå¹¶è®°å½•ç»“æœ"""
    
    def __init__(self, trainer, eval_dataset, compute_metrics, output_dir, logger):
        self.trainer = trainer
        self.eval_dataset = eval_dataset
        self.compute_metrics = compute_metrics
        self.output_dir = output_dir
        self.logger = logger
        self.eval_history = []
        
    def on_step_end(self, args, state, control, **kwargs):
        """åœ¨æ¯ä¸ªæ­¥éª¤ç»“æŸæ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦è¯„ä¼°"""
        # æ£€æŸ¥æ˜¯å¦åˆ°äº†è¯„ä¼°æ­¥éª¤
        if args.evaluation_strategy == "steps" and state.global_step % args.eval_steps == 0:
            self.logger.info(f"ğŸ” Step {state.global_step}: Running evaluation...")
            
            # å¼ºåˆ¶æ‰§è¡Œè¯„ä¼°
            metrics = self.trainer.evaluate(
                eval_dataset=self.eval_dataset,
                metric_key_prefix="eval"
            )
            
            # è®°å½•è¯„ä¼°ç»“æœ
            eval_result = {
                "step": state.global_step,
                "metrics": metrics,
                "timestamp": datetime.now().isoformat()
            }
            self.eval_history.append(eval_result)
            
            # ä¿å­˜è¯„ä¼°å†å²
            with open(os.path.join(self.output_dir, "eval_history.json"), "w") as f:
                json.dump(self.eval_history, f, indent=2)
            
            # æ‰“å°å…³é”®æŒ‡æ ‡
            self.logger.info(f"ğŸ“Š Evaluation Results at Step {state.global_step}:")
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    self.logger.info(f"  {key}: {value:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            if args.metric_for_best_model in metrics:
                current_metric = metrics[args.metric_for_best_model]
                if not hasattr(state, 'best_metric') or state.best_metric is None:
                    state.best_metric = current_metric
                    state.best_model_checkpoint = state.global_step
                    self.logger.info(f"ğŸ† New best model at step {state.global_step}!")
                elif (args.greater_is_better and current_metric > state.best_metric) or \
                     (not args.greater_is_better and current_metric < state.best_metric):
                    state.best_metric = current_metric
                    state.best_model_checkpoint = state.global_step
                    self.logger.info(f"ğŸ† New best model at step {state.global_step}!")
        
        return control


def setup_logger(output_dir: str, log_file: str = "training.log"):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
    global logger
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºlogger
    logger = logging.getLogger("QwenTraining")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤å·²æœ‰çš„handlers
    logger.handlers = []
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(
        os.path.join(output_dir, log_file),
        mode='w',
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # è®¾ç½®æ ¼å¼
    formatter = logging.Formatter(
        '[%(asctime)s] [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # æ·»åŠ handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def setup_logger_with_tensorboard_compatibility(output_dir, log_file="training.log"):
    """åˆ›å»ºä¸ TensorBoard å…¼å®¹çš„ logger"""
    # è·å–å½“å‰æ´»è·ƒçš„ loggers
    root_logger = logging.getLogger()
    
    # åˆ›å»ºæ–°çš„ logger
    new_logger = logging.getLogger('QwenTraining')
    new_logger.setLevel(logging.INFO)
    new_logger.propagate = False  # ä¸ä¼ æ’­åˆ°æ ¹ logger
    new_logger.handlers.clear()
    
    # æ£€æŸ¥æ ¹ logger æ˜¯å¦æœ‰ handlersï¼ˆTensorBoard å¯èƒ½æ·»åŠ çš„ï¼‰
    if root_logger.handlers:
        # å¤ç”¨æ ¹ logger çš„æ§åˆ¶å° handler æ ¼å¼
        for handler in root_logger.handlers:
            if isinstance(handler, logging.StreamHandler) and handler.stream == sys.stdout:
                # åˆ›å»ºç›¸åŒæ ¼å¼çš„ handler
                console_handler = logging.StreamHandler(sys.stdout)
                console_handler.setLevel(handler.level)
                console_handler.setFormatter(handler.formatter)
                new_logger.addHandler(console_handler)
                break
    else:
        # åˆ›å»ºé»˜è®¤çš„æ§åˆ¶å° handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s')
        console_handler.setFormatter(formatter)
        new_logger.addHandler(console_handler)
    
    # æ·»åŠ æ–‡ä»¶ handler
    file_handler = logging.FileHandler(
        os.path.join(output_dir, log_file),
        mode='a',
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s')
    file_handler.setFormatter(formatter)
    new_logger.addHandler(file_handler)
    
    return new_logger


def rank0_print(*args):
    """ä½¿ç”¨loggerè¾“å‡ºä¿¡æ¯"""
    global logger
    message = " ".join(str(arg) for arg in args)
    if logger:
        logger.info(message)
    else:
        print(message)

def verify_lora_weights_loaded(model, expected_modules=None):
    """éªŒè¯ LoRA æƒé‡æ˜¯å¦æˆåŠŸåŠ è½½"""
    rank0_print("\n" + "="*60)
    rank0_print("ğŸ” éªŒè¯ LoRA åŠ è½½æƒ…å†µ")
    rank0_print("="*60)
    
    lora_modules_found = {}
    total_lora_params = 0
    
    # æ£€æŸ¥æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        if "lora_" in name:
            # æå–æ¨¡å—è·¯å¾„
            module_path = name.split(".lora_")[0]
            base_module = module_path.split(".")[-1]
            
            if module_path not in lora_modules_found:
                lora_modules_found[module_path] = {
                    'base_module': base_module,
                    'lora_A': False,
                    'lora_B': False,
                    'params': 0
                }
            
            if "lora_A" in name:
                lora_modules_found[module_path]['lora_A'] = True
            elif "lora_B" in name:
                lora_modules_found[module_path]['lora_B'] = True
            
            lora_modules_found[module_path]['params'] += param.numel()
            total_lora_params += param.numel()
    
    # æ‰“å°æ‰¾åˆ°çš„ LoRA æ¨¡å—
    if lora_modules_found:
        rank0_print(f"\nâœ… æ‰¾åˆ° {len(lora_modules_found)} ä¸ª LoRA æ¨¡å—:")
        for path, info in sorted(lora_modules_found.items()):
            status = "âœ“" if (info['lora_A'] and info['lora_B']) else "âœ—"
            rank0_print(f"  {status} {path}")
            rank0_print(f"     - åŸºç¡€æ¨¡å—: {info['base_module']}")
            rank0_print(f"     - LoRA A: {'âœ“' if info['lora_A'] else 'âœ—'}")
            rank0_print(f"     - LoRA B: {'âœ“' if info['lora_B'] else 'âœ—'}")
            rank0_print(f"     - å‚æ•°æ•°: {info['params']:,}")
    else:
        rank0_print("âŒ æœªæ‰¾åˆ°ä»»ä½• LoRA æ¨¡å—")
        return False
    
    rank0_print(f"\nğŸ“Š æ€» LoRA å‚æ•°æ•°: {total_lora_params:,}")
    
    # æ£€æŸ¥é¢„æœŸæ¨¡å—
    if expected_modules:
        rank0_print(f"\nğŸ¯ æ£€æŸ¥é¢„æœŸæ¨¡å—:")
        all_found = True
        for expected in expected_modules:
            found = any(expected in path for path in lora_modules_found.keys())
            if found:
                rank0_print(f"  âœ… {expected}")
            else:
                rank0_print(f"  âŒ {expected}")
                all_found = False
        
        return all_found and len(lora_modules_found) > 0
    
    return len(lora_modules_found) > 0

def load_and_merge_existing_lora(model, lora_model_id, device, compute_dtype):
    """åŠ è½½å·²æœ‰çš„ LoRA æƒé‡å¹¶åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹"""
    rank0_print("\n" + "="*60)
    rank0_print("ğŸ”„ åŠ è½½å¹¶åˆå¹¶å·²æœ‰ LoRA æƒé‡")
    rank0_print("="*60)
    
    try:
        # å…ˆæ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(lora_model_id):
            rank0_print(f"âŒ LoRA è·¯å¾„ä¸å­˜åœ¨: {lora_model_id}")
            return model, False
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        adapter_config_path = os.path.join(lora_model_id, "adapter_config.json")
        adapter_model_path = os.path.join(lora_model_id, "adapter_model.bin")
        
        if not os.path.exists(adapter_config_path):
            adapter_model_path = os.path.join(lora_model_id, "adapter_model.safetensors")
            if not os.path.exists(adapter_model_path):
                rank0_print("âŒ æœªæ‰¾åˆ° adapter_model.bin æˆ– adapter_model.safetensors")
                return model, False
        
        # è¯»å–é€‚é…å™¨é…ç½®
        with open(adapter_config_path, 'r') as f:
            adapter_config = json.load(f)
            
        rank0_print(f"ğŸ“‹ åŠ è½½çš„ LoRA é…ç½®:")
        rank0_print(f"  - rank: {adapter_config.get('r', 'N/A')}")
        rank0_print(f"  - alpha: {adapter_config.get('lora_alpha', 'N/A')}")
        rank0_print(f"  - target_modules: {adapter_config.get('target_modules', 'N/A')}")
        
        # è·å–åŸå§‹çš„ target_modules
        original_target_modules = adapter_config.get('target_modules', [])
        
        # ä½¿ç”¨ PeftModel åŠ è½½
        rank0_print(f"\nğŸ“¥ ä» {lora_model_id} åŠ è½½ LoRA æƒé‡...")
        peft_model = PeftModel.from_pretrained(
            model, 
            lora_model_id,
            torch_dtype=compute_dtype,
            device_map={"": device}
        )
        
        # éªŒè¯åŠ è½½æ˜¯å¦æˆåŠŸ
        loaded_successfully = verify_lora_weights_loaded(peft_model, original_target_modules)
        
        if loaded_successfully:
            rank0_print("\nâœ… LoRA æƒé‡åŠ è½½æˆåŠŸ!")
            
            # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹
            rank0_print("\nğŸ”€ åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
            merged_model = peft_model.merge_and_unload()
            rank0_print("âœ… åˆå¹¶å®Œæˆ!")
            
            # éªŒè¯åˆå¹¶åçš„æ¨¡å‹
            rank0_print("\nğŸ” éªŒè¯åˆå¹¶åçš„æ¨¡å‹:")
            total_params = sum(p.numel() for p in merged_model.parameters())
            rank0_print(f"  - æ€»å‚æ•°æ•°: {total_params:,}")
            
            # æ£€æŸ¥åˆå¹¶åä¸åº”è¯¥æœ‰ LoRA å±‚
            lora_layers_after_merge = sum(1 for n, _ in merged_model.named_parameters() if "lora_" in n)
            if lora_layers_after_merge == 0:
                rank0_print("  - âœ… åˆå¹¶æˆåŠŸï¼šæ²¡æœ‰æ®‹ç•™çš„ LoRA å±‚")
            else:
                rank0_print(f"  - âš ï¸ è­¦å‘Šï¼šå‘ç° {lora_layers_after_merge} ä¸ªæ®‹ç•™çš„ LoRA å±‚")
            
            return merged_model, True
        else:
            rank0_print("âŒ LoRA æƒé‡åŠ è½½å¤±è´¥")
            return model, False
            
    except Exception as e:
        rank0_print(f"âŒ åŠ è½½ LoRA æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return model, False


def find_target_linear_names(model, num_lora_modules=-1, lora_namespan_exclude=[], verbose=True):
    linear_cls = torch.nn.modules.Linear
    embedding_cls = torch.nn.modules.Embedding
    lora_module_names = []

    for name, module in model.named_modules():
        if any(ex_keyword in name for ex_keyword in lora_namespan_exclude):
            continue
        if isinstance(module, (linear_cls, embedding_cls)):
            lora_module_names.append(name)
    
    if num_lora_modules > 0:
        lora_module_names = lora_module_names[-num_lora_modules:]
    if verbose:
        rank0_print(f"Found {len(lora_module_names)} lora modules: {lora_module_names}")
    return lora_module_names


def set_requires_grad(parameters, requires_grad):
    for p in parameters:
        p.requires_grad = requires_grad


def configure_vision_tower(model, training_args, compute_dtype, device):
    vision_tower = model.visual
    vision_tower.to(dtype=compute_dtype, device=device)

    vision_model_params = model.visual.parameters()
    set_requires_grad(vision_model_params, not training_args.freeze_vision_tower)
    
    # Handle merger specifically
    merger_params = model.visual.merger.parameters()
    set_requires_grad(merger_params, not training_args.freeze_merger)


def configure_llm(model, training_args):
    lm_head = model.lm_head.parameters()
    set_requires_grad(lm_head, not training_args.freeze_llm)

    llm_params = model.model.parameters()
    set_requires_grad(llm_params, not training_args.freeze_llm)


def print_model_info(model):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯ç”¨äºè°ƒè¯•"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    rank0_print(f"Model loaded successfully!")
    rank0_print(f"Total parameters: {total_params:,}")
    rank0_print(f"Trainable parameters: {trainable_params:,}")
    rank0_print(f"Trainable ratio: {trainable_params/total_params:.2%}")
    
    # æ‰“å°ä¸€äº›å¯è®­ç»ƒå‚æ•°çš„åç§°
    rank0_print("\nSample trainable parameters:")
    count = 0
    for name, param in model.named_parameters():
        if param.requires_grad and count < 10:
            rank0_print(f"  {name}: {param.shape}")
            count += 1


def setup_evaluation(data_args, training_args):
    """è®¾ç½®éªŒè¯é…ç½®"""
    # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯æ•°æ®
    eval_data_path = getattr(data_args, 'eval_data_path', None)
    
    if eval_data_path and os.path.exists(eval_data_path):
        rank0_print(f"ğŸ“Š Found evaluation data: {eval_data_path}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„type_mappingæ–‡ä»¶
        type_mapping_file = None
        possible_mapping_files = [
            eval_data_path.replace('.json', '_mapping.json'),
            eval_data_path.replace('.json', '_type_mapping.json'),
            os.path.join(os.path.dirname(eval_data_path), 'type_mapping.json'),
            os.path.join(os.path.dirname(eval_data_path), 'validation_mapping.json')
        ]
        
        for mapping_file in possible_mapping_files:
            if os.path.exists(mapping_file):
                type_mapping_file = mapping_file
                rank0_print(f"ğŸ“‹ Found type mapping: {mapping_file}")
                break
        
        if not type_mapping_file:
            rank0_print("âš ï¸ No type mapping file found, using standard evaluation")
        
        # å¼ºåˆ¶å¯ç”¨éªŒè¯ç›¸å…³å‚æ•°
        training_args.evaluation_strategy = "steps"
        training_args.eval_steps = max(1, training_args.logging_steps)  # ç¡®ä¿eval_stepsæœ‰å€¼
        training_args.metric_for_best_model = "eval_soft_match" if training_args.metric_for_best_model is None else training_args.metric_for_best_model
        training_args.greater_is_better = True if training_args.greater_is_better is None else training_args.greater_is_better
        training_args.load_best_model_at_end = True
        training_args.save_total_limit = 3 if training_args.save_total_limit is None else training_args.save_total_limit
        
        # ç¡®ä¿do_evalä¸ºTrue
        training_args.do_eval = True
        
        rank0_print(f"âœ… Evaluation enabled:")
        rank0_print(f"   - Strategy: {training_args.evaluation_strategy}")
        rank0_print(f"   - Eval steps: {training_args.eval_steps}")
        rank0_print(f"   - Metric: {training_args.metric_for_best_model}")
        rank0_print(f"   - Do eval: {training_args.do_eval}")
        
        return True, type_mapping_file
    else:
        rank0_print("âš ï¸ No evaluation data found, skipping validation")
        training_args.evaluation_strategy = "no"
        training_args.do_eval = False
        return False, None


def train():
    global local_rank, logger
    
    # lora_target_modules = ["visual.merger.mlp.0", "visual.merger.mlp.2"]
    
    # åœ¨è§£æå‚æ•°ä¹‹å‰å°±ç¦ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    os.environ['LOCAL_RANK'] = '-1'
    os.environ['WORLD_SIZE'] = '1' 
    os.environ['RANK'] = '0'
    # æ¸…é™¤å¯èƒ½å¯¼è‡´åˆ†å¸ƒå¼åˆå§‹åŒ–çš„ç¯å¢ƒå˜é‡
    for key in ['MASTER_ADDR', 'MASTER_PORT', 'NODE_RANK']:
        os.environ.pop(key, None)

    parser = HfArgumentParser(
        (ModelArguments, DataArguments, TrainingArguments))
    
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    training_args.label_names = ['labels']
    training_args.include_for_metrics = ['inputs']
    training_args.batch_eval_metrics = True
    
    
    
    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    # logger = setup_logger(training_args.output_dir)
    # original_logger = logger
    # original_handlers = logger.handlers[:] if logger else []
    
    # ä¿å­˜é…ç½®ä¿¡æ¯åˆ°æ–‡ä»¶
    config_info = {
        "model_args": vars(model_args),
        "data_args": vars(data_args),
        "training_args": training_args.to_dict(),
        "timestamp": datetime.now().isoformat(),
        "command": " ".join(sys.argv)
    }
    with open(os.path.join(training_args.output_dir, "training_config.json"), "w", encoding="utf-8") as f:
        json.dump(config_info, f, indent=2, ensure_ascii=False)
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # è®¾ç½®éªŒè¯é…ç½® - åœ¨åŠ è½½æ¨¡å‹ä¹‹å‰
    has_eval_data, type_mapping_file = setup_evaluation(data_args, training_args)
    # åˆå§‹åŒ–TensorBoard
    tb_writer = init_tensorboard(training_args, model_args, data_args)
    progress_callback = TensorBoardCallback(tb_writer, output_dir=training_args.output_dir)

    logger = setup_logger_with_tensorboard_compatibility(training_args.output_dir)
    
    # è°ƒè¯•ä¿¡æ¯
    rank0_print("=" * 60)
    rank0_print("QWEN2.5-VL TRAINING WITH EVALUATION")
    rank0_print("=" * 60)
    rank0_print(f"ğŸ”§ Device: {device}")
    rank0_print(f"ğŸ¤– Model: {model_args.model_id}")
    rank0_print(f"ğŸ“ Output dir: {training_args.output_dir}")
    rank0_print(f"ğŸ¯ LoRA enabled: {training_args.lora_enable}")
    rank0_print(f"ğŸ“Š Evaluation: {'âœ… Enabled' if has_eval_data else 'âŒ Disabled'}")
    if type_mapping_file:
        rank0_print(f"ğŸ“‹ Type-based evaluation: âœ… Enabled")
    elif has_eval_data:
        rank0_print(f"ğŸ“‹ Type-based evaluation: âš ï¸ No mapping file")
    rank0_print(f"âš¡ Max steps: {training_args.max_steps}")
    rank0_print(f"ğŸ“¦ Batch size: {training_args.per_device_train_batch_size}")
    rank0_print(f"ğŸ”„ Gradient accumulation: {training_args.gradient_accumulation_steps}")
    rank0_print(f"ğŸ“ˆ Learning rate: {training_args.learning_rate}")
    if has_eval_data:
        rank0_print(f"ğŸ” Eval frequency: every {training_args.eval_steps} steps")
        rank0_print(f"ğŸ† Best model metric: {training_args.metric_for_best_model}")
    rank0_print("=" * 60)
    


    

    
    # åº”ç”¨ä¼˜åŒ–
    use_liger = training_args.use_liger
    if "Qwen2.5" in model_args.model_id:
        replace_qwen2_5_with_mixed_modality_forward(use_liger=use_liger)
        if use_liger:
            apply_liger_kernel_to_qwen2_5_vl(fused_linear_cross_entropy=False)
    else:
        replace_qwen_2_with_mixed_modality_forward(use_liger=use_liger)
        if use_liger:
            apply_liger_kernel_to_qwen2_vl(fused_linear_cross_entropy=False)

    # éªŒè¯å‚æ•°é…ç½®
    if training_args.lora_enable and not training_args.freeze_llm:
        raise ValueError("If `lora_enable` is True, `freeze_llm` must also be True.")

    if not training_args.lora_enable:
        assert not training_args.vision_lora, \
            "Error: training_args.lora_enable is not enabled, but training_args.vision_lora is enabled."
        
    if training_args.vision_lora and not training_args.freeze_vision_tower:
        raise ValueError("If `vision_lora` is True, `freeze_vision_tower` must also be True.")

    # å¤„ç†LoRAæ’é™¤åˆ—è¡¨
    if training_args.lora_namespan_exclude is not None:
        training_args.lora_namespan_exclude = ast.literal_eval(training_args.lora_namespan_exclude)
    else:
        training_args.lora_namespan_exclude = []
    if training_args.lora_target_modules is not None:
        training_args.lora_target_modules = ast.literal_eval(training_args.lora_target_modules)
    else:
        training_args.lora_target_modules = []
    lora_target_modules = training_args.lora_target_modules
    
    if not training_args.vision_lora:
        training_args.lora_namespan_exclude += ["visual"]

    local_rank = training_args.local_rank
    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))

    # é‡åŒ–é…ç½®
    bnb_model_from_pretrained_args = {}
    if training_args.bits in [4,8]:
        bnb_model_from_pretrained_args.update(dict(
            device_map={"": device},
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=training_args.bits==4,
                load_in_8bit=training_args.bits==8,
                llm_int8_skip_modules=["visual", "lm_head"],
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=training_args.double_quant,
                bnb_4bit_quant_type=training_args.quant_type,
            )
        ))

    # åŠ è½½æ¨¡å‹
    rank0_print("ğŸ¤– Loading model...")
    # if model_args.lora_model_id is None:
    if "Qwen2.5" in model_args.model_id:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_args.model_id,
            torch_dtype=compute_dtype,
            attn_implementation="flash_attention_2" if not training_args.disable_flash_attn2 else "sdpa", 
            **bnb_model_from_pretrained_args
        )
    else:
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_args.model_id,
            torch_dtype=compute_dtype,
            attn_implementation="flash_attention_2" if not training_args.disable_flash_attn2 else "sdpa", 
            **bnb_model_from_pretrained_args
            )


    model.config.use_cache = False
    model_to_configure = model
    
    # é…ç½®æ¨¡å‹ç»„ä»¶
    rank0_print("âš™ï¸ Configuring model components...")
    configure_llm(model_to_configure, training_args)
    configure_vision_tower(model_to_configure, training_args, compute_dtype, training_args.device)

    # é‡åŒ–è®­ç»ƒå‡†å¤‡
    if training_args.bits in [4,8]:
        model.config.torch_dtype = (torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))
        from peft import prepare_model_for_kbit_training
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing, gradient_checkpointing_kwargs={"use_reentrant": True})
    
    if training_args.gradient_checkpointing:
        model.enable_input_require_grads()
        training_args.gradient_checkpointing_kwargs = {"use_reentrant": True}

    if model_args.lora_model_id is not None:
        model, merge_success = load_and_merge_existing_lora(
            model, 
            model_args.lora_model_id, 
            device, 
            compute_dtype
        )
        if not merge_success:
            rank0_print("âš ï¸ ç»§ç»­ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œè®­ç»ƒ...")

    # è®¾ç½®LoRA
    if training_args.lora_enable:
        rank0_print("ğŸ¯ Setting up LoRA...")
        # lora_namespan_exclude = training_args.lora_namespan_exclude
        peft_config = LoraConfig(
            r=training_args.lora_rank,
            lora_alpha=training_args.lora_alpha,
            target_modules=lora_target_modules,
            lora_dropout=training_args.lora_dropout,
            bias=training_args.lora_bias
        )
        if training_args.bits == 16:
            if training_args.bf16:
                model.to(torch.bfloat16)
            if training_args.fp16:
                model.to(torch.float16)
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        rank0_print("\nğŸ“Š æ–° LoRA é…ç½®ä¿¡æ¯:")
        rank0_print(f"  - ç›®æ ‡æ¨¡å—: {lora_target_modules}")
        rank0_print(f"  - Rank: {training_args.lora_rank}")
        rank0_print(f"  - Alpha: {training_args.lora_alpha}")
        rank0_print(f"  - Dropout: {training_args.lora_dropout}")

        verify_lora_weights_loaded(model, lora_target_modules)
        model.print_trainable_parameters()
        
        # ç¡®ä¿visionç›¸å…³å‚æ•°çš„è®¾ç½®
        if not training_args.freeze_vision_tower:
            for name, param in model.named_parameters():
                if "visual" in name:
                    param.requires_grad = True

        if not training_args.freeze_merger:
            for name, param in model.named_parameters():
                if "merger" in name:
                    param.requires_grad = True
                    

    # æ£€æŸ¥LoRAé…ç½®
    inspect_lora_model(model)
    print_model_info(model)

    # åŠ è½½å¤„ç†å™¨
    rank0_print("ğŸ”§ Loading processor...")
    processor = AutoProcessor.from_pretrained(model_args.model_id)

    # é‡åŒ–è®­ç»ƒçš„é¢å¤–è®¾ç½®
    if training_args.bits in [4, 8]:
        from peft.tuners.lora import LoraLayer
        for name, module in model.named_modules():
            if isinstance(module, LoraLayer):
                if training_args.bf16:
                    module = module.to(torch.bfloat16)
            if 'norm' in name:
                module = module.to(torch.float32)
            
            if 'lm_head' in name or 'embed_token' in name:
                if hasattr(module, 'weight'):
                    if training_args.bf16 and module.weight.dtype == torch.float32:
                        module = module.to(torch.bfloat16)

    # åˆ›å»ºæ•°æ®æ¨¡å—
    rank0_print("ğŸ“Š Creating data module...")
    data_module = make_supervised_eval_data_module(model_id=model_args.model_id,
                                              processor=processor,
                                              data_args=data_args)
    
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŠ è½½äº†éªŒè¯æ•°æ®é›†
    eval_dataset = data_module.get('eval_dataset', None)
    if has_eval_data and eval_dataset is None:
        rank0_print("âš ï¸ Warning: Evaluation requested but no eval dataset created!")
        has_eval_data = False
        training_args.evaluation_strategy = "no"
        training_args.do_eval = False
    elif eval_dataset is not None:
        rank0_print(f"âœ… Evaluation dataset loaded: {len(eval_dataset)} samples")

    # è®¾ç½®è¯„ä¼°å‡½æ•°
    compute_metrics = None
    if has_eval_data and eval_dataset is not None:
        rank0_print("ğŸ” Setting up evaluation metrics...")
        
        if type_mapping_file:
            rank0_print(f"ğŸ“‹ Using type-based evaluation with: {type_mapping_file}")
            # é¢„åŠ è½½type_mapping
            load_type_mapping(type_mapping_file)
            compute_metrics = create_compute_metrics_with_types(
                processor, 
                training_args.output_dir, 
                type_mapping_file,
                eval_dataset=eval_dataset
            )
        else:
            rank0_print("ğŸ“Š Using standard evaluation")
            compute_metrics = create_compute_metrics_with_types(
                processor, 
                training_args.output_dir,
                eval_dataset=eval_dataset
            )


    # åˆ›å»ºè®­ç»ƒå™¨
    rank0_print("ğŸš€ Creating trainer...")
    trainer = QwenSFTTrainer(
        model=model,
        processing_class=processor,
        args=training_args,
        compute_metrics=compute_metrics,
        **data_module
    )
    
    # æ·»åŠ å›è°ƒ
    callbacks_to_add = []
    
    # TensorBoardå›è°ƒ
    if hasattr(trainer, 'add_callback'):
        callbacks_to_add.append(progress_callback)
    
    # æ·»åŠ è¯„ä¼°å›è°ƒ
    if has_eval_data and eval_dataset is not None:
        eval_callback = EvaluationCallback(
            trainer=trainer,
            eval_dataset=eval_dataset,
            compute_metrics=compute_metrics,
            output_dir=training_args.output_dir,
            logger=logger
        )
        callbacks_to_add.append(eval_callback)
        rank0_print("âœ… Added evaluation callback")
    


    # æ·»åŠ æ‰€æœ‰å›è°ƒ
    for callback in callbacks_to_add:
        if hasattr(trainer, 'add_callback'):
            trainer.add_callback(callback)
    
    rank0_print(f"ğŸ“ Added {len(callbacks_to_add)} callbacks to trainer")
    
    # å¼€å§‹è®­ç»ƒå‰ï¼Œå…ˆè¿è¡Œä¸€æ¬¡è¯„ä¼°ä½œä¸ºbaseline
    
    
    if has_eval_data and eval_dataset is not None:
        rank0_print("ğŸ“Š Running initial evaluation as baseline...")
        try:
            initial_metrics = trainer.evaluate(eval_dataset=eval_dataset)
            rank0_print("ğŸ“Š Initial Evaluation Results:")
            for key, value in initial_metrics.items():
                if isinstance(value, (int, float)) and key.startswith('eval_'):
                    rank0_print(f"  {key}: {value:.4f}")
        except Exception as e:
            rank0_print(f"âš ï¸ Initial evaluation failed: {e}")
    
    # å¼€å§‹è®­ç»ƒ
    rank0_print("ğŸš€ Starting training...")
    try:
        if list(pathlib.Path(training_args.output_dir).glob("checkpoint-*")):
            rank0_print("ğŸ”„ Found existing checkpoint, resuming training...")
            trainer.train(resume_from_checkpoint=True)
        else:
            trainer.train()
    except Exception as e:
        logger.error(f"âŒ Training failed with error: {str(e)}")
        raise

    # ä¿å­˜æ¨¡å‹
    rank0_print("ğŸ’¾ Saving final model...")
    trainer.save_state()

    model.config.use_cache = True
    
    if training_args.lora_enable:
        state_dict = get_peft_state_maybe_zero_3(
            model.named_parameters(), training_args.lora_bias
        )

        non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(
            model.named_parameters(), require_grad_only=True
        )
        # ä¿å­˜LoRAæ¨¡å‹
        model.config.save_pretrained(training_args.output_dir)
        model.save_pretrained(training_args.output_dir, state_dict=state_dict)
        processor.save_pretrained(training_args.output_dir)
        torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, "non_lora_state_dict.bin"))
        rank0_print(f"âœ… LoRA model saved to {training_args.output_dir}")
    else:
        safe_save_model_for_hf_trainer(trainer, output_dir=training_args.output_dir)
        rank0_print(f"âœ… Full model saved to {training_args.output_dir}")

    # æœ€ç»ˆéªŒè¯
    if has_eval_data and eval_dataset is not None:
        rank0_print("ğŸ” Running final evaluation...")
        try:
            final_metrics = trainer.evaluate(eval_dataset=eval_dataset)
            rank0_print("ğŸ“Š Final Evaluation Results:")
            for key, value in final_metrics.items():
                if isinstance(value, (int, float)) and key.startswith('eval_'):
                    rank0_print(f"  {key}: {value:.4f}")
            
            # è®¡ç®—æ”¹è¿›
            if 'initial_metrics' in locals():
                rank0_print("\nğŸ“ˆ Improvements:")
                for key in final_metrics:
                    if key in initial_metrics and isinstance(final_metrics[key], (int, float)):
                        improvement = final_metrics[key] - initial_metrics[key]
                        rank0_print(f"  {key}: {improvement:+.4f}")
            
            # ä¿å­˜æœ€ç»ˆè¯„ä¼°ç»“æœ
            with open(os.path.join(training_args.output_dir, "final_evaluation.json"), "w") as f:
                json.dump(final_metrics, f, indent=2)
                
        except Exception as e:
            rank0_print(f"âš ï¸ Final evaluation failed: {e}")

       # è®°å½•è®­ç»ƒå®Œæˆä¿¡æ¯
    training_summary = {
        "status": "completed",
        "completion_time": datetime.now().isoformat(),
        "total_steps": trainer.state.global_step,
        "best_metric": trainer.state.best_metric if hasattr(trainer.state, 'best_metric') else None,
        "best_model_checkpoint": trainer.state.best_model_checkpoint if hasattr(trainer.state, 'best_model_checkpoint') else None
    }
    
    with open(os.path.join(training_args.output_dir, "training_summary.json"), "w") as f:
        json.dump(training_summary, f, indent=2)

    rank0_print("ğŸ‰" + "=" * 58 + "ğŸ‰")
    rank0_print("ğŸ‰ TRAINING COMPLETED SUCCESSFULLY!")
    rank0_print("ğŸ‰" + "=" * 58 + "ğŸ‰")
    rank0_print(f"ğŸ“ All results saved in: {training_args.output_dir}")
    rank0_print(f"ğŸ“„ Check training.log for complete logs")
    rank0_print(f"ğŸ“Š Check evaluation_history.json for eval metrics")
    rank0_print(f"ğŸ“ˆ Use TensorBoard to visualize: tensorboard --logdir {training_args.output_dir}/logs")



if __name__ == "__main__":
    train()